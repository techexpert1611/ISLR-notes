{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Chapter 2: Statastical Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 What Is Statistical Learning?\n",
    "\t\t\n",
    "    ->In order to motivate our study of statistical learning, we begin with a simple example. \n",
    ">For Example, \n",
    "<br>Suppose that we are `statistical consultants` hired by a `client` to provide advice on __how to improve sales of a particular product__.\n",
    "<br>The Advertising data set consists of the `sales` of that `product` in `200 different markets`, along with `advertising budgets` for the product in each of those `markets` for `three different media`: __TV , radio , and newspaper__. \n",
    "<br>The `data` are displayed in `Figure`. It is `not` possible for our `client` to directly `increase sales` of the `product`. \n",
    "<br>On the other hand, they can `control` the `advertising expenditure` in each of the `three media`. \n",
    "Therefore, if we determine that there is an `association` between `advertising and sales`, then we can `instruct` our `client` to adjust `advertising budgets`, __thereby indirectly increasing sales__.\n",
    ">\n",
    "<a id='Figure2.1'></a>\n",
    "![image.png](Figures/Figure2.1.png)\n",
    "\n",
    ">The Advertising data set.\n",
    "\n",
    ">The plot displays `sales`, in `thousands of units`, as a function of TV, radio, and newspaper budgets, in` thousands of dollars`, for `200 different markets`. \n",
    "\n",
    ">In `each plot` we show the simple `least squares` fit of `sales` to that variable, as described in Chapter 3. \n",
    "\n",
    ">In other words, each blue line represents a simple model that can be used to predict sales using TV, radio, and newspaper, respectively.\n",
    "\n",
    "__-> In other words, our goal is to develop an accurate model that can be used to predict SALES on the basis of the THREE MEDIA BUDGETS.__\n",
    "\n",
    "suppose that we observe a quantitative response $Y$ and $p$ different predictors, \n",
    "$x_{1}, x_{2}, \\dots, x_{p}$.\n",
    "\n",
    "We assume that there is some relationship between $Y$ and $X = (x_{1}, x_{2}, \\dots, x_{p})$, \n",
    "which can be written in the very general form \n",
    "               \n",
    "$Y = f (X) + \\epsilon $ \n",
    ">Where,\n",
    "><br>$f$ is some fixed but unknown function of $x_{1}, x_{2}, \\dots, x_{p}$. \n",
    "> <br>$\\epsilon $ is a random error term, \n",
    "><br>__[Note: $\\epsilon $ is independent of X and has mean Zero $ \\sim $ 0]__\n",
    "><br>In this formulation, $f $ represents the systematic information that $X $ provides about $Y$ .\n",
    "\n",
    "Which is Simillar to Our Linear Regression Equation, $[ Y = m(X) + b]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">As another example, \n",
    "><br>consider the `left-hand` panel of Below Figure, a plot of `income versus years of education` for 30 individuals in the `Income data set`.\n",
    "><br>The plot suggests that one might be able to `predict income using years of education`. \n",
    "><br>However, the function $f $ that connects the `input variable` to the\n",
    "`output variable` is in `general unknown`. \n",
    "><br>In this situation one must estimate $f $ based on the `observed points`. \n",
    "><br>Since Income is a `simulated data` set, $f$ is `known` and is shown by the `blue curve` in the `right-hand` panel of Below Figure.\n",
    "><br>The vertical lines represent the `error terms` $\\epsilon$.\n",
    ">We note that some of the 30 observations lie above the `blue curve` and some `lie below` it; \n",
    ">overall, the `errors` have approximately `mean zero`.\n",
    "\n",
    "![image.png](Figures/Figure2.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.1.1 Why Estimate f ?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main reasons that we may wish to estimate $f$:\n",
    ">__prediction__ & __inference__\n",
    "\n",
    "Let's understand both one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many situations $X$ will be readily available, but Output $Y$ can not be easily Obtained. \n",
    "We can Predict $Y$ using:\n",
    "\n",
    "$ Y' = f'(X)$\n",
    "\n",
    ">Where\n",
    "><br> $f'$ is the estimate for $f$.\n",
    "><br> $Y'$ is the prediced result of $Y$\n",
    "\n",
    "In this `Setting` $f'$ often treated like `black-box`, in the scense that no one is typically concerned wiht the exact form(Formula) of $f'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Figure2.3'></a>\n",
    "![image.png](Figures/Figure2.3.png)\n",
    "\n",
    ">The plot displays `income` as a function of `years of education`\n",
    "and `seniority` in the `Income data set`. \n",
    "><br>The blue surface represents the true underlying relationship between `income` and `years of education` and `seniority`, which is known since the data are simulated. \n",
    "><br>The `red dots` indicate the `observed values` of these `quantities` for 30 individuals.\n",
    "\n",
    "\n",
    "The Accuracy of $Y'$ as a prediction of $Y$ depends on two main quantities:\n",
    "1. __Reducible Error:__\n",
    "\n",
    "> Errors that can be Reduced/Decreased are called Reducible Errors.\n",
    "\n",
    ">Ex, $f(X)$<br>\n",
    "> Where we can choose __`best Form(ula)`__ of $f()$ using `Statastical Learning`,  <br>\n",
    "> so It can Work(Predict) well and well.<br>\n",
    "\n",
    "\n",
    "2. __Ir-reducible Error:__\n",
    "\n",
    "> Errors that can not be Reduced/Decreased are called Ir-reducible Errors.\n",
    ">Ex, $ \\epsilon $ because,<br>\n",
    ">The $Y$ is the Function of $f(X)$ and $\\epsilon$ both, and variablity associated with $\\epsilon$ also affects the accuracy of our predicion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Why is the ir-reducible error larger than zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$E(y-y')^2 = E [ f(x) + \\epsilon - f'(x) ]^2$\n",
    "        >> =>$ [ f(x)-f'(x) ]^2 + Var( \\epsilon ) $\n",
    "        >>><br> \n",
    "        Where $E(y-y')^2$ Representes the __Average__, or __Expected value__, or __Squared Difference__ between $y-y'$ <br>\n",
    "        >>>$ Var( \\epsilon ) $ Varience is associated with Error Term.\n",
    "        >>><br><br> __[ NOTE, $ [ f(x)-f'(x) ]^2$ is `reducible`, $Var( \\epsilon ) $ is `ir-reducible`].__\n",
    "        \n",
    "$E(y-y')^2 = E [ f(x) + \\epsilon - f'(x) ]^2$ - [Formula in detail Explained](https://www.youtube.com/watch?v=A83LNKCoB0U)\n",
    "\n",
    "[Squared Difference Understanding - For More Detailed Idea.](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/more-on-regression/v/squared-error-of-regression-line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are often interested in understanding the way that $Y$ is affected as $x_{1}, x_{2}, \\dots, x_{p}$ change.\n",
    "\n",
    "__Prediction  of $Y$ is not always our Goal.__\n",
    "\n",
    "__Understand the Relationship Between $ X \\& Y $__\n",
    "\n",
    "In simple words, __Why/How the $Y$ changes with the Change in $X$?__\n",
    "\n",
    "> To Understnad this Question properly.\n",
    ">1. we need the $f()$ as White Box,\n",
    ">2. we want the Exact Form(ula) of $f()$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this `setting`, one may be interested in answering the following questions:\n",
    "\n",
    "1. __Which predictors($X$) are associated with the response($Y$)?__\n",
    "\n",
    "    -> Identification of few Important Features amongst all can help in work.\n",
    "    \n",
    "2. __What is the relationship between the response($Y$) and each predictor($X_{1}, X_{2}, \\dots, X_{p}$)?__\n",
    "\n",
    "    -> You need to understand the relationship between the Response and Predictor properly, confused how? Don't worry let me give you few scenarios.\n",
    "    \n",
    "    -> if $ X_{1} $ increases $ Y $ increases.\n",
    "    \n",
    "    -> if $ X_{2} $ decreases $ Y $ decreases.\n",
    "    \n",
    "    -> Sometimes it becomes more often harder like\n",
    "    \n",
    "    -> if $ X_{3} $ decreases $ X_{1} $ increases and Therefore $ Y $ increases.\n",
    "\n",
    "\n",
    "3. __Can the relationship between $Y$ and each predictor($X_{1}, X_{2}, \\dots, X_{p}$) be adequately summarized using a linear equation, or is the relationship more complicated?__\n",
    "\n",
    "    -> Mostly $ f $ will be Linear-Equation but,\n",
    "    \n",
    "    -> In some case when Trye Relationship between Input & Output Varies.\n",
    "    \n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Importatant:</b> <br>We will see the <br>1) Prediction$f()$ Settings,<br> 2) Inference $ \\epsilon $ Settings or, <br> 3)Combination of both.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.1.2 How Do We Estimate f ? ?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So till now, we explored many `linear` and `non-linear` approaches for `estimating` $f$.\n",
    "\n",
    "These mathods `shares` some `characteristics`.\n",
    "\n",
    "<a id=\"Figure2.2\"></a>\n",
    "![image.png](Figures/Figure2.2.png)\n",
    "\n",
    "In the above figure we observed that we have $ n = 30 $ `data points`.\n",
    "These `datapoints` are called `Training Data`, because we will use it to __train__, or __teach__, our mehtod __`how to estimate $f$`__?\n",
    "\n",
    ">Let $x_{ij}$ represent the `value` of the $jth$ `predictor`, or `input`, for `observation` $ i $\n",
    ">>where<br>\n",
    ">>$i = 1, 2, \\dots , n$<br> \n",
    ">>$j = 1, 2, . . . , p$. \n",
    "\n",
    ">Correspondingly, let $y_{i}$ represent the `response variable` for the $ith$ observation.<br>\n",
    ">Then our training data consist of ${(x_{1}, y_{1}), (x_{2} , y{2}), \\dots , (x_{n} , y_{n})}$ \n",
    ">>where<br>\n",
    ">>$x_{i} = (x_{i1} , x_{i2},\\dots, x_{ip})^T$ .\n",
    "\n",
    "__Our goal is to apply a statistical learning method to the training data\n",
    "in order to estimate the unknown function $f$.__\n",
    "\n",
    "In other words, \n",
    "\n",
    "__we want to find a function $f'$ such that $Y \\approx f'(X)$ for any observation $(X, Y)$.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&bullet; Most __`Statistical Learning Mehtods`__ an be `characterized` as either,\n",
    "\n",
    ">__1. Parametric.__<br>\n",
    ">__2. Non-Parametric.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Parametric Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__-> involves a `two-step` `model-based` approach.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, we make an assumption about the functional form, or shape, of $f$.\n",
    "    \n",
    "    For example, one very simple assumption is that $f$ is linear in <br>$X$:\n",
    "    \n",
    "    $ f(X) = β_{0} + β_{1} X_{1} + β_{2} X_{2} + \\dots+ β_{p} X_{p}.$\n",
    "    \n",
    "    This is one Linear Equation which will be discussed in Chapter3.\n",
    "    \n",
    ">Once we have assumed that $f$ is `linear`, \n",
    "<br>the problem of `estimating` $f$ is `greatly simplified`. \n",
    "<br>Instead of having to estimate an entirely arbitrary(random/Personal Choice) $p$-dimensional function $f(X)$,\n",
    "<br>one only needs to estimate the $p + 1$ __`coefficients`__ $β_{0}, β_{1}, β_{2}, \\dots, β_{p} $ .\n",
    "<br><br> __$p$ = Number Of Features/Predictors__.\n",
    "\n",
    "2. After a model has been selected,<br>\n",
    "    we need a __`procedure`__ that uses the __`training data`__ to __`fit`__ or __`train`__ the model.<br>\n",
    "    In the case of the `linear model`, <br>\n",
    "    we need to estimate the parameters $β_{0}, β_{1}, β_{2}, \\dots, β_{p}$.<br>\n",
    "    That is, we want to find values of these parameters such that,<br>\n",
    "    \n",
    "    $ Y \\approx β_{0} + β_{1} X_{1} + β_{2} X_{2} + \\dots+ β_{p} X_{p}.$\n",
    "        \n",
    ">One of the Common approach to fit the `Linear Model` is [__Least Squares__](https://www.khanacademy.org/math/linear-algebra/alternate-bases/orthogonal-projections/v/linear-algebra-another-least-squares-example).\n",
    "<br>But `least Square` is not the only way to fit `Linear Model`, it is One of Many Possible Ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Figure2.4\"></a>\n",
    "![image.png](Figures/Figure2.4.png)\n",
    "\n",
    "> A `linear model` fit by `least squares` to the `Income data` from <a href=\"#Figure2.3\">Figure 2.3</a>. \n",
    "<br>The observations are shown in __`red`__, and the __`yellow plane`__ indicates the\n",
    "__`least squares`__ fit to the data.\n",
    "\n",
    "-> The __Parametric Approach__ reduces the problem of estimating $f$ down to one of the estimating set of __parameters.__\n",
    "\n",
    "-> Assuming a `parametric form` for $f$ simplifies the problem of `estimating` $f$ because it is generally much `easier` to estimate a `set of parameters`, such as $β_{0}, β_{1}, β_{2}, \\dots, β_{p}$ in the `linear model`, than it is to fit an entirely `arbitrary function` $f$ .\n",
    "\n",
    "-> The Potential __Disadvantage__ of `Parametric Approach` is that the Model we choose will not match the Original Unknown Form of $f$. \n",
    "\n",
    "-> We can `solve` this `problem` by `choosing` the `flexible Model` that can `fit` many `different possible Forms` for $f$.\n",
    "\n",
    ">But in general, fitting more flexible Models `requires` estimating `big` number of `Features/Parameters`.\n",
    "<br> These more complex models can lead to a important topic called [_Overfitting_](https://www.youtube.com/watch?v=j9_yzC-x-js)\n",
    "<br>which essentially means they follow the __errors, or noise__, too closely\n",
    "\n",
    "<a href='#Figure2.4'>Figure 2.4</a> shows an example of the `parametric approach` applied to the\n",
    "`Income data` from <a href='#Figure2.3'>Figure 2.3</a>.\n",
    "\n",
    "We have `fit` a `linear model` of the form:\n",
    "\n",
    "$ income \\approx β_{0} + β_{1} × education + β_{2} × seniority.$\n",
    "\n",
    "Since we have assumed a __`linear relationship`__ between the `response`$y$ and the `two predictors`, the entire fitting problem reduces to estimating $β_{0} , β_{1} , and β_{2} $, which we do using __`least squares linear regression`__.\n",
    "\n",
    "Comparing <a href='#Figure2.3'>Figure 2.3</a> to <a href='#Figure2.4'>Figure 2.4</a>, \n",
    "\n",
    "we can see that the linear fit given in <a href='#Figure2.4'>Figure 2.4</a> is not quite\n",
    "right: \n",
    "\n",
    "the true $f$ has some `curvature` that is `not captured` in the `linear fit`.\n",
    "\n",
    "However, the `linear fit` still appears to do a `reasonable job` of capturing the\n",
    "`positive relationship` between `years of education and income` , as well as the slightly `less positive relationship` between `seniority and income` . \n",
    "\n",
    "It may be that with such a small number of observations, this is the best we can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Non-Parametric Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Non-Parametric Methods` don't take explicit assumption about the Functional Form of $f$.\n",
    "\n",
    "Instead they `seek` an estimate of $f$ that gets as `close` to the `data points` as possible without being `too rough` or `wiggly`. \n",
    "\n",
    "Such approaches can have a `major advantage` over `parametric approaches`: by avoiding the `assumption` of a `particular functional form` for $f$ , they have the `potential` to accurately fit a `wider range` of `possible shapes` for $f$.\n",
    "\n",
    "Any `parametric approach` brings with it the `possibility` that the `functional form` used to `estimate` $f$ is  `very different` from the `true` $f$ , in which case the `resulting model` will not fit the data well.\n",
    "\n",
    "In contrast, `non-parametric` approaches __completely avoid this danger__, since essentially `no assumption` about the `form` of $f$ is made.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Disadvantage:</b><br>since they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for $f$ .\n",
    "</div>\n",
    "\n",
    "<a id=\"Figure2.5\"></a>\n",
    "![image.png](Figures/Figure2.5.png)\n",
    "\n",
    "An `example` of a `non-parametric approach` to `fitting the Income data` is\n",
    "shown in <a href=\"#Figure2.5\">Figure 2.5</a>.\n",
    "\n",
    "A `thin-plate spline` is used to estimate $f$. \n",
    "\n",
    "This `approach` does `not impose` any `pre-specified model` on $f$. \n",
    "\n",
    "It `instead` attempts to `produce` an `estimate` for $f$ that is as `close as possible to the observed\n",
    "data`, subject to the fit—that is, the `yellow surface` in <a href=\"#Figure2.5\">Figure 2.5</a>—_being Smooth_\n",
    "\n",
    "In this case, the `non-parametric fit` has `produced` a `remarkably accurate estimate` of the `true` $f$ shown in <a href=\"#Figure2.3\">Figure 2.3</a>. \n",
    "\n",
    "In order to `fit` a `thin-plate spline`, the `data analyst` must `select` a `level of smoothness`. \n",
    "\n",
    "<a id=\"Figure2.6\"></a>\n",
    "![image.png](Figures/Figure2.6.png)\n",
    "\n",
    ">A rough thin-plate spline fit to the Income data from <a href=\"#Figure2.3\">Figure 2.3</a>.\n",
    "<br>This fit makes zero errors on the training data.\n",
    "\n",
    "<a href=\"#Figure2.6\">Figure 2.6</a> shows the same `thin-plate spline fit` using a `lower level of smoothness`, allowing for a `rougher fit`. \n",
    "\n",
    "The resulting estimate __`fits the observed data perfectly`!__\n",
    "\n",
    "However, the `spline fit` shown in <a href=\"#Figure2.6\">Figure 2.6</a> is `far more variable` than the true function $f$ ,from <a href=\"#Figure2.3\">Figure 2.3</a>. \n",
    "\n",
    "This is an `example` of `overfitting the data`, which we discussed previously. \n",
    "\n",
    "It is an `undesirable situation` because the fit obtained will `not yield accurate estimates` of the `response` on __new observations that were not part of the original training data set__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability ?\n",
    "\n",
    "(__Trade Off__ - a balance achieved between two desirable but incompatible features; a compromise.)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Methods are Less Flexible, or we can say more restrictive, in the `sense` that they `can produce` just a `relatively small` range of shapes to `estimate` $f$.\n",
    "\n",
    ">For Example,<br>\n",
    "`linear regression` is a relatively `inflexible approach`, because it can only `generate linear functions`\n",
    "such as the `lines` shown in <a href=\"#Figure2.1\">Figure 2.1</a> or the `plane` shown in <a href=\"#Figure2.4\">Figure 2.4</a>.\n",
    "\n",
    "<a id=\"#Figure2.7.png\"></a>\n",
    "![image.png](Figures/Figure2.7.png)\n",
    "\n",
    ">A representation of the `tradeoff` between `flexibility and interpretability`, using different statistical learning methods. \n",
    "<br>__In general, as the `flexibility of a method increases`, `its interpretability decreases`.__\n",
    "\n",
    "Other methods, such as the `thin plate splines` shown in <a href=\"#Figures2.5\">Figures 2.5</a> and <a href=\"#Figures2.6\">2.6</a>, are considerably `more flexible` because `they can generate a much wider range of possible shapes to estimate` $f$.\n",
    "\n",
    "Any one can reasonably some questions like:\n",
    "\n",
    "- Why would we ever choose to use a more restrictive method instead of a very flexible approach?\n",
    "    There are several reasons why we might prefer more `restrictive Model` \n",
    "    \n",
    "    If we are mainly interested in <a href=\"#Inference.\">Inference</a>, then restrictive models are much more interpretable.\n",
    ">For Example, when `inference` is the goal, the `linear model` may be a `good choice` since it will be `quite easy to understand` the `relationship` between $Y$ and $X_{1} , X_{2} , \\dots , X_{p}$ . \n",
    "<br>In contrast, `very flexible approaches`, such as the `splines` displayed in <a href=\"#Figures2.5\">Figures 2.5</a> and <a href=\"#Figures2.6\">2.6</a>, and the `boosting methods`, can `lead` to such `complicated estimates` of $f$ that it is `difficult to understand` __`how any individual predictor is associated with the response`__.\n",
    "\n",
    "<a href=\"#Figure2.7\">Figure 2.7</a> provides an illustration of the `trade-off` between `flexibility` and `interpretability` for some of the methods. \n",
    "\n",
    "`Least squares linear regression`, is relatively `inflexible` but is quite `interpretable`. \n",
    "\n",
    "The `lasso`, relies upon the `linear model` but uses an `alternative fitting procedure` for `estimating\n",
    "the coefficients` $ β_{0} , β_{1} ,\\dots, β_{p}$ .\n",
    "\n",
    "&bullet; ___Generalized Addictive Models_(GAMs)__, instead extend the linear model to allow for certain `non-linear relationships`.\n",
    "\n",
    "They are also somewhat `less interpretable` than `linear regression`, because the `relationship between each predictor` and the `response` is now `modeled using a curve`. \n",
    "\n",
    "Finally, `fully non-linear methods` such as __bagging, boosting, and support vector machines with non-linear kernels__, are `highly flexible approaches` that are harder to interpret.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Supervised Versus Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Statistical Learning falls into one of the two categories:\n",
    "\n",
    ">__1. Supervised Learning__\n",
    ">>-> For each observation of the `predictor measurement(s)` $x_{i}, i = 1, \\dots , n$ there is an `associated response measurement` $y_{i}$ . \n",
    "<br>-> We wish to fit a model that `relates the response to the predictors`, with the `aim of accurately predicting the response` for `future observations (prediction)` or `better understanding the relationship between the response and the predictors (inference)`. \n",
    "<br>-> Many `classical` `statistical learning methods` such as `linear regression` and __`logistic regression`__ as well as more modern approaches such as `GAM`, `boosting`, and `support vec-\n",
    "tor machines`, operate in the __`supervised learning domain`__.\n",
    "\n",
    ">__2. Unsupervised Learning.__\n",
    ">>->`unsupervised learning` describes the somewhat `more challenging situation` in which for `every observation` $i = 1, \\dots , n$, we observe a `vector of measurements` $x_{i}$ but `no associated response` $y_{i}$. \n",
    "<br>->It is `not possible` to `fit a linear regression model`, since there is `no response variable`\n",
    "to `predict`. \n",
    "<br>->In this `setting`, we are in `some sense working blind`; the `situation` is referred to as __`unsupervised`__ because we `lack a response variable` that can `supervise our analysis`.\n",
    "\n",
    "\n",
    "<a id=\"Figure2.8\"></a>\n",
    "![image.png](Figures/Figure2.8.png)\n",
    ">A `clustering data set` involving `three groups`. \n",
    "<br>Each `group` is shown using a `different colored symbol`. \n",
    "\n",
    ">__Left:__ The `three groups are well-separated`. In\n",
    "this `setting`, a `clustering approach` should `successfully identify` the `three groups`.\n",
    "\n",
    ">__Right:__ There is some `overlap` among the `groups`. Now the `clustering task is more challenging`.\n",
    "\n",
    "__What sort of statistical analysis is possible?__\n",
    ">We can seek to understand the `relationships` between the `variables` or `between` the `observations`. \n",
    "<br>`One statistical learning` tool that we may use in this `setting` is __`cluster analysis`__, or __`clustering`__. \n",
    "<br>The goal of `cluster analysis` is to ascertain, on the basis of $x_{1} , \\dots , x_{n}$ , whether the `observations fall` into `relatively distinct groups`. \n",
    ">>For example, <br>In a market `segmentation study` we\n",
    "might observe `multiple characteristics` (variables) for `potential customers`,\n",
    "such as `zip code`, `family income`, and `shopping habits`.\n",
    "<br>We might believe that the `customers fall` into `different groups`, such as __`big spenders versus\n",
    "low spenders`__.\n",
    "\n",
    "A __`semi-supervised learning problem`__. \n",
    ">In this `setting`, we wish to use a `statistical learning` method that can `incorporate` the $m$ `observations` for which `response measurements are available` as well as the $n − m$ `observations for which they are not`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Regression Versus Classification Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">__1. Quantitative__\n",
    ">>-> `Quantitative variables` take on `numerical values`.\n",
    ">>>Examples: <br> A `person’s age`, `height`, or `income`, the `value of a house`, and the `price of a stock`.\n",
    "\n",
    ">__2. Qualitative(aka. categorical)__\n",
    ">>-> `qualitative variables` take on `values` in one of $K$ `different classes`, or __categories__. \n",
    ">>>Examples <br>include a `person’s gender` (male or female), the `brand of prod-\n",
    "uct purchased` (brand A, B, or C), whether a person `defaults on a debt`\n",
    "(yes or no), or a `cancer diagnosis` (Acute Myelogenous Leukemia, Acute\n",
    "Lymphoblastic Leukemia, or No Leukemia).\n",
    "\n",
    "-> We tend to refer to problems with a `quantitative response` as `regression problems`, while those involving a `qualitative response` are often referred to as __`classification problems`__.\n",
    "\n",
    "-> However, the `distinction` is not always that `crisp`. \n",
    "\n",
    "-> `Least squares linear regression` is used with a `quantitative response`, where as `logistic regression` is typically used with a `qualitative` (two-class, or __`binary`__) response.  As such it is often used as a __classification__ method.\n",
    "\n",
    "-> But since it `estimates class probabilities`, it can be thought of as a `regression method` as well. \n",
    "\n",
    "-> __Some statistical methods, such as `K-nearest neighbors`and `boosting`, can be used in the case of\n",
    "either `quantitative` or `qualitative responses`.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Assessing Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to wide range of `statistical learning methods` that extend far beyond the `standard linear\n",
    "regression` approach. \n",
    "\n",
    "- __Why is it necessary to introduce so many different statistical learning approaches, rather than just a single best method?__ \n",
    "    \n",
    "   _There is no `free lunch` in `statistics`_: no one method dominates all others over all `possible data sets`. \n",
    "    \n",
    "    On a particular data set, one `specific method` may `work best`, but `some other method` may `work better` on `a similar but different data set`. \n",
    "    \n",
    "    Hence it is `an important task` to `decide` for `any given set of data` `which method produces` the __`best results`__. \n",
    "    \n",
    "    `Selecting the best approach` can be `one of the most challenging parts` of performing `statistical learning` in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Measuring the Quality of Fit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the `performance` of a `statistical learning method` on a `given data set`, we need some way to `measure` __`how well its predictions actually match the observed data`__. \n",
    "\n",
    "That is, we need to `quantify` the extent to which the `predicted response value` for a `given observation` is `close to the true response value` for that `observation`. \n",
    "\n",
    "In the `regression setting`, the `most commonly-used measure` is the __mean squared error (MSE)__, given by:\n",
    "\n",
    "<a id=\"Formula2.5\"></a>\n",
    "$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} ((y_{i} - f'(x_{i}))^2$\n",
    ">Where<br>\n",
    "$f'(x_{i})$ is the Prediction that $f'$ gives for the $ith$ observation.\n",
    ">>MSE will be `Low` if the `predicted response` are `very close` to `true ones`,\n",
    "<br> otherwise, it'll be `very large` if the `predicition responses` are `way far` from `true values`.\n",
    "\n",
    "\n",
    "The MSE computed using Training Data that was used to fit the model, and referred as _training MSE_.\n",
    "\n",
    "But in general we don't really care how well our method works on training data.\n",
    "\n",
    "Rather, we are interested in the accuracy of the _previously unseen test data_\n",
    "\n",
    "__Why we don't care about the training accuracy?__\n",
    ">Example,<br>Suppose that we are `interested` in `developing` an `algorithm` to `predict a stock’s price` based on `previousstock returns`. \n",
    "<br>We can `train` the method using `stock returns` from the `past 6 months`. \n",
    "<br>But we `don’t really care` how `well our method predicts last week’s stock price`. \n",
    "\n",
    ">We instead `care about` __`how well it will predict tomorrow’s price or next month’s price.`__\n",
    "\n",
    "we are `really not interested` in whether $f'(x_{i}) \\approx y_{i};$ \n",
    "\n",
    "instead, `we want` to know whether $f'(x_{0})$ is `approximately equal` to $y_{0}$ ,\n",
    "where $(x_{0} , y_{0})$ is a `previously unseen test observation` __not used to train\n",
    "the statistical learning method.__ \n",
    "\n",
    "We want to `choose the method` that gives the __`lowest test MSE`__, as `opposed` to the `lowest training MSE`. \n",
    "\n",
    "In other words, if we had a `large number` of `test observations`, we could `compute` the `average squared prediction error` for these `test observations` $(x_{0} , y_{0} )$.\n",
    "\n",
    "<a id=\"Formula2.6\"></a>\n",
    ">$Ave (y_{0} − f' (x_{0}))^2 $\n",
    "\n",
    "___How can we go about trying to select a method that minimizes the test\n",
    "MSE?___\n",
    "\n",
    ">In some `settings`, we `may` have a `test data set available` — that is, we `may have access` to a `set of observations` that `were not` used to `train the statistical learning method`. \n",
    "<br>We can then `simply evaluate` <a href=\"#Formula2.6\">2.6</a> on the `test observations`, and `select the learning method` for which the `test MSE` is smallest.\n",
    "\n",
    "<a id=\"Figure2.9\"></a>\n",
    "![image.png](Figures/Figure2.9.png)\n",
    ">__Left:__ Data simulated from $f$ , shown in `black`. \n",
    "<br>Three estimates of $f$ are shown: `the linear regression line (orange curve)`, and __`two smoothing spline\n",
    "fits (blue and green curves)`__. \n",
    "\n",
    ">__Right:__ `Training MSE (grey curve)`, `test MSE (red\n",
    "curve)`, and `minimum possible test MSE over all methods (dashed line)`. \n",
    "<br>`Squares` represent the `training and test MSEs` for the `three fits` shown in the `left-hand panel`.\n",
    "\n",
    "__what if no test observations are available?__\n",
    "\n",
    "In that case, one might imagine simply `selecting` a `statistical learning method` that `minimizes the training MSE` <a href=\"#Formula2.5\">(2.5)</a>. \n",
    "\n",
    "This seems like it `might` be a `sensible approach`, since the `training MSE` and the `test MSE` appear to be `closely related`.\n",
    "\n",
    "Unfortunately, there is a __`fundamental problem with this strategy`:__ `there\n",
    "is no guarantee` that the `method` with the `lowest training MSE` will also have the `lowest test MSE`. \n",
    "\n",
    "Roughly speaking, `the problem` is that `many statistical methods` specifically `estimate coefficients` so as to `minimize the training set MSE`. \n",
    "\n",
    "For these methods, the `training set MSE` can be `quite small`, but the `test MSE` is `often much larger`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
