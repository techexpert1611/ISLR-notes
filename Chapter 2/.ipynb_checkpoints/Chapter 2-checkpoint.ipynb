{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Chapter 2: Statastical Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 What Is Statistical Learning?\n",
    "\t\t\n",
    "    ->In order to motivate our study of statistical learning, we begin with a simple example. \n",
    ">For Example, \n",
    "<br>Suppose that we are `statistical consultants` hired by a `client` to provide advice on __how to improve sales of a particular product__.\n",
    "<br>The Advertising data set consists of the `sales` of that `product` in `200 different markets`, along with `advertising budgets` for the product in each of those `markets` for `three different media`: __TV , radio , and newspaper__. \n",
    "<br>The `data` are displayed in `Figure`. It is `not` possible for our `client` to directly `increase sales` of the `product`. \n",
    "<br>On the other hand, they can `control` the `advertising expenditure` in each of the `three media`. \n",
    "Therefore, if we determine that there is an `association` between `advertising and sales`, then we can `instruct` our `client` to adjust `advertising budgets`, __thereby indirectly increasing sales__.\n",
    ">\n",
    "<a id='Figure2.4'></a>\n",
    "![image.png](Figures/Figure2.1.png)\n",
    "\n",
    ">The Advertising data set.\n",
    "\n",
    ">The plot displays `sales`, in `thousands of units`, as a function of TV, radio, and newspaper budgets, in` thousands of dollars`, for `200 different markets`. \n",
    "\n",
    ">In `each plot` we show the simple `least squares` fit of `sales` to that variable, as described in Chapter 3. \n",
    "\n",
    ">In other words, each blue line represents a simple model that can be used to predict sales using TV, radio, and newspaper, respectively.\n",
    "\n",
    "__-> In other words, our goal is to develop an accurate model that can be used to predict SALES on the basis of the THREE MEDIA BUDGETS.__\n",
    "\n",
    "suppose that we observe a quantitative response $Y$ and $p$ different predictors, \n",
    "$x_{1}, x_{2}, \\dots, x_{p}$.\n",
    "\n",
    "We assume that there is some relationship between $Y$ and $X = (x_{1}, x_{2}, \\dots, x_{p})$, \n",
    "which can be written in the very general form \n",
    "               \n",
    "$Y = f (X) + \\epsilon $ \n",
    ">Where,\n",
    "><br>$f$ is some fixed but unknown function of $x_{1}, x_{2}, \\dots, x_{p}$. \n",
    "> <br>$\\epsilon $ is a random error term, \n",
    "><br>__[Note: $\\epsilon $ is independent of X and has mean Zero $ \\sim $ 0]__\n",
    "><br>In this formulation, $f $ represents the systematic information that $X $ provides about $Y$ .\n",
    "\n",
    "Which is Simillar to Our Linear Regression Equation, $[ Y = m(X) + b]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">As another example, \n",
    "><br>consider the `left-hand` panel of Below Figure, a plot of `income versus years of education` for 30 individuals in the `Income data set`.\n",
    "><br>The plot suggests that one might be able to `predict income using years of education`. \n",
    "><br>However, the function $f $ that connects the `input variable` to the\n",
    "`output variable` is in `general unknown`. \n",
    "><br>In this situation one must estimate $f $ based on the `observed points`. \n",
    "><br>Since Income is a `simulated data` set, $f$ is `known` and is shown by the `blue curve` in the `right-hand` panel of Below Figure.\n",
    "><br>The vertical lines represent the `error terms` $\\epsilon$.\n",
    ">We note that some of the 30 observations lie above the `blue curve` and some `lie below` it; \n",
    ">overall, the `errors` have approximately `mean zero`.\n",
    "\n",
    "![image.png](Figures/Figure2.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Why Estimate f ?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main reasons that we may wish to estimate $f$:\n",
    ">__prediction__ & __inference__\n",
    "\n",
    "Let's understand both one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many situations $X$ will be readily available, but Output $Y$ can not be easily Obtained. \n",
    "We can Predict $Y$ using:\n",
    "\n",
    "$ Y' = f'(X)$\n",
    "\n",
    ">Where\n",
    "><br> $f'$ is the estimate for $f$.\n",
    "><br> $Y'$ is the prediced result of $Y$\n",
    "\n",
    "In this `Setting` $f'$ often treated like `black-box`, in the scense that no one is typically concerned wiht the exact form(Formula) of $f'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Figure2.3'></a>\n",
    "![image.png](Figures/Figure2.3.png)\n",
    "\n",
    ">The plot displays `income` as a function of `years of education`\n",
    "and `seniority` in the `Income data set`. \n",
    "><br>The blue surface represents the true underlying relationship between `income` and `years of education` and `seniority`, which is known since the data are simulated. \n",
    "><br>The `red dots` indicate the `observed values` of these `quantities` for 30 individuals.\n",
    "\n",
    "\n",
    "The Accuracy of $Y'$ as a prediction of $Y$ depends on two main quantities:\n",
    "1. __Reducible Error:__\n",
    "\n",
    "> Errors that can be Reduced/Decreased are called Reducible Errors.\n",
    "\n",
    ">Ex, $f(X)$<br>\n",
    "> Where we can choose __`best Form(ula)`__ of $f()$ using `Statastical Learning`,  <br>\n",
    "> so It can Work(Predict) well and well.<br>\n",
    "\n",
    "\n",
    "2. __Ir-reducible Error:__\n",
    "\n",
    "> Errors that can not be Reduced/Decreased are called Ir-reducible Errors.\n",
    ">Ex, $ \\epsilon $ because,<br>\n",
    ">The $Y$ is the Function of $f(X)$ and $\\epsilon$ both, and variablity associated with $\\epsilon$ also affects the accuracy of our predicion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Why is the ir-reducible error larger than zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$E(y-y')^2 = E [ f(x) + \\epsilon - f'(x) ]^2$\n",
    "        >> =>$ [ f(x)-f'(x) ]^2 + Var( \\epsilon ) $\n",
    "        >>><br> \n",
    "        Where $E(y-y')^2$ Representes the __Average__, or __Expected value__, or __Squared Difference__ between $y-y'$ <br>\n",
    "        >>>$ Var( \\epsilon ) $ Varience is associated with Error Term.\n",
    "        >>><br><br> __[ NOTE, $ [ f(x)-f'(x) ]^2$ is `reducible`, $Var( \\epsilon ) $ is `ir-reducible`].__\n",
    "        \n",
    "$E(y-y')^2 = E [ f(x) + \\epsilon - f'(x) ]^2$ - [Formula in detail Explained](https://www.youtube.com/watch?v=A83LNKCoB0U)\n",
    "\n",
    "[Squared Difference Understanding - For More Detailed Idea.](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/more-on-regression/v/squared-error-of-regression-line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are often interested in understanding the way that $Y$ is affected as $x_{1}, x_{2}, \\dots, x_{p}$ change.\n",
    "\n",
    "__Prediction  of $Y$ is not always our Goal.__\n",
    "\n",
    "__Understand the Relationship Between $ X \\& Y $__\n",
    "\n",
    "In simple words, __Why/How the $Y$ changes with the Change in $X$?__\n",
    "\n",
    "> To Understnad this Question properly.\n",
    ">1. we need the $f()$ as White Box,\n",
    ">2. we want the Exact Form(ula) of $f()$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this `setting`, one may be interested in answering the following questions:\n",
    "\n",
    "1. __Which predictors($X$) are associated with the response($Y$)?__\n",
    "\n",
    "    -> Identification of few Important Features amongst all can help in work.\n",
    "    \n",
    "2. __What is the relationship between the response($Y$) and each predictor($X_{1}, X_{2}, \\dots, X_{p}$)?__\n",
    "\n",
    "    -> You need to understand the relationship between the Response and Predictor properly, confused how? Don't worry let me give you few scenarios.\n",
    "    \n",
    "    -> if $ X_{1} $ increases $ Y $ increases.\n",
    "    \n",
    "    -> if $ X_{2} $ decreases $ Y $ decreases.\n",
    "    \n",
    "    -> Sometimes it becomes more often harder like\n",
    "    \n",
    "    -> if $ X_{3} $ decreases $ X_{1} $ increases and Therefore $ Y $ increases.\n",
    "\n",
    "\n",
    "3. __Can the relationship between $Y$ and each predictor($X_{1}, X_{2}, \\dots, X_{p}$) be adequately summarized using a linear equation, or is the relationship more complicated?__\n",
    "\n",
    "    -> Mostly $ f $ will be Linear-Equation but,\n",
    "    \n",
    "    -> In some case when Trye Relationship between Input & Output Varies.\n",
    "    \n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Importatant:</b> <br>We will see the <br>1) Prediction$f()$ Settings,<br> 2) Inference $ \\epsilon $ Settings or, <br> 3)Combination of both.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## How Do We Estimate f ? ?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So till now, we explored many `linear` and `non-linear` approaches for `estimating` $f$.\n",
    "\n",
    "These mathods `shares` some `characteristics`.\n",
    "\n",
    "<a id=\"Figure2.2\"></a>\n",
    "![image.png](Figures/Figure2.2.png)\n",
    "\n",
    "In the above figure we observed that we have $ n = 30 $ `data points`.\n",
    "These `datapoints` are called `Training Data`, because we will use it to __train__, or __teach__, our mehtod __`how to estimate $f$`__?\n",
    "\n",
    ">Let $x_{ij}$ represent the `value` of the $jth$ `predictor`, or `input`, for `observation` $ i $\n",
    ">>where<br>\n",
    ">>$i = 1, 2, \\dots , n$<br> \n",
    ">>$j = 1, 2, . . . , p$. \n",
    "\n",
    ">Correspondingly, let $y_{i}$ represent the `response variable` for the $ith$ observation.<br>\n",
    ">Then our training data consist of ${(x_{1}, y_{1}), (x_{2} , y{2}), \\dots , (x_{n} , y_{n})}$ \n",
    ">>where<br>\n",
    ">>$x_{i} = (x_{i1} , x_{i2},\\dots, x_{ip})^T$ .\n",
    "\n",
    "__Our goal is to apply a statistical learning method to the training data\n",
    "in order to estimate the unknown function $f$.__\n",
    "\n",
    "In other words, \n",
    "\n",
    "__we want to find a function $f'$ such that $Y \\approx f'(X)$ for any observation $(X, Y)$.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&bullet; Most __`Statistical Learning Mehtods`__ an be `characterized` as either,\n",
    "\n",
    ">__1. Parametric.__<br>\n",
    ">__2. Non-Parametric.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parametric Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__-> involves a `two-step` `model-based` approach.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, we make an assumption about the functional form, or shape, of $f$.\n",
    "    \n",
    "    For example, one very simple assumption is that $f$ is linear in <br>$X$:\n",
    "    \n",
    "    $ f(X) = β_{0} + β_{1} X_{1} + β_{2} X_{2} + \\dots+ β_{p} X_{p}.$\n",
    "    \n",
    "    This is one Linear Equation which will be discussed in Chapter3.\n",
    "    \n",
    ">Once we have assumed that $f$ is `linear`, \n",
    "<br>the problem of `estimating` $f$ is `greatly simplified`. \n",
    "<br>Instead of having to estimate an entirely arbitrary(random/Personal Choice) $p$-dimensional function $f(X)$,\n",
    "<br>one only needs to estimate the $p + 1$ __`coefficients`__ $β_{0}, β_{1}, β_{2}, \\dots, β_{p} $ .\n",
    "<br><br> __$p$ = Number Of Features/Predictors__.\n",
    "\n",
    "2. After a model has been selected,<br>\n",
    "    we need a __`procedure`__ that uses the __`training data`__ to __`fit`__ or __`train`__ the model.<br>\n",
    "    In the case of the `linear model`, <br>\n",
    "    we need to estimate the parameters $β_{0}, β_{1}, β_{2}, \\dots, β_{p}$.<br>\n",
    "    That is, we want to find values of these parameters such that,<br>\n",
    "    \n",
    "    $ Y \\approx β_{0} + β_{1} X_{1} + β_{2} X_{2} + \\dots+ β_{p} X_{p}.$\n",
    "        \n",
    ">One of the Common approach to fit the `Linear Model` is [__Least Squares__](https://www.khanacademy.org/math/linear-algebra/alternate-bases/orthogonal-projections/v/linear-algebra-another-least-squares-example).\n",
    "<br>But `least Square` is not the only way to fit `Linear Model`, it is One of Many Possible Ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Figure2.4\"></a>\n",
    "![image.png](Figures/Figure2.4.png)\n",
    "\n",
    "> A `linear model` fit by `least squares` to the `Income data` from <a href=\"#Figure2.3\">Figure 2.3</a>. \n",
    "<br>The observations are shown in __`red`__, and the __`yellow plane`__ indicates the\n",
    "__`least squares`__ fit to the data.\n",
    "\n",
    "-> The __Parametric Approach__ reduces the problem of estimating $f$ down to one of the estimating set of __parameters.__\n",
    "\n",
    "-> Assuming a `parametric form` for $f$ simplifies the problem of `estimating` $f$ because it is generally much `easier` to estimate a `set of parameters`, such as $β_{0}, β_{1}, β_{2}, \\dots, β_{p}$ in the `linear model`, than it is to fit an entirely `arbitrary function` $f$ .\n",
    "\n",
    "-> The Potential __Disadvantage__ of `Parametric Approach` is that the Model we choose will not match the Original Unknown Form of $f$. \n",
    "\n",
    "-> We can `solve` this `problem` by `choosing` the `flexible Model` that can `fit` many `different possible Forms` for $f$.\n",
    "\n",
    ">But in general, fitting more flexible Models `requires` estimating `big` number of `Features/Parameters`.\n",
    "<br> These more complex models can lead to a important topic called [_Overfitting_](https://www.youtube.com/watch?v=j9_yzC-x-js)\n",
    "<br>which essentially means they follow the __errors, or noise__, too closely\n",
    "\n",
    "<a href='#Figure2.4'>Figure 2.4</a> shows an example of the `parametric approach` applied to the\n",
    "`Income data` from <a href='#Figure2.3'>Figure 2.3</a>.\n",
    "\n",
    "We have `fit` a `linear model` of the form:\n",
    "\n",
    "$ income \\approx β_{0} + β_{1} × education + β_{2} × seniority.$\n",
    "\n",
    "Since we have assumed a __`linear relationship`__ between the `response`$y$ and the `two predictors`, the entire fitting problem reduces to estimating $β_{0} , β_{1} , and β_{2} $, which we do using __`least squares linear regression`__.\n",
    "\n",
    "Comparing <a href='#Figure2.3'>Figure 2.3</a> to <a href='#Figure2.4'>Figure 2.4</a>, \n",
    "\n",
    "we can see that the linear fit given in <a href='#Figure2.4'>Figure 2.4</a> is not quite\n",
    "right: \n",
    "\n",
    "the true $f$ has some `curvature` that is `not captured` in the `linear fit`.\n",
    "\n",
    "However, the `linear fit` still appears to do a `reasonable job` of capturing the\n",
    "`positive relationship` between `years of education and income` , as well as the slightly `less positive relationship` between `seniority and income` . \n",
    "\n",
    "It may be that with such a small number of observations, this is the best we can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Non-Parametric Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Non-Parametric Methods` don't take explicit assumption about the Functional Form of $f$.\n",
    "\n",
    "Instead they `seek` an estimate of $f$ that gets as `close` to the `data points` as possible without being `too rough` or `wiggly`. \n",
    "\n",
    "Such approaches can have a `major advantage` over `parametric approaches`: by avoiding the `assumption` of a `particular functional form` for $f$ , they have the `potential` to accurately fit a `wider range` of `possible shapes` for $f$.\n",
    "\n",
    "Any `parametric approach` brings with it the `possibility` that the `functional form` used to `estimate` $f$ is  `very different` from the `true` $f$ , in which case the `resulting model` will not fit the data well.\n",
    "\n",
    "In contrast, `non-parametric` approaches __completely avoid this danger__, since essentially `no assumption` about the `form` of $f$ is made.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Disadvantage:</b><br>since they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for $f$ .\n",
    "</div>\n",
    "\n",
    "<a id=\"Figure2.5\"></a>\n",
    "![image.png](Figures/Figure2.5.png)\n",
    "\n",
    "An `example` of a `non-parametric approach` to `fitting the Income data` is\n",
    "shown in <a href=\"#Figure2.5\">Figure 2.5</a>.\n",
    "\n",
    "A `thin-plate spline` is used to estimate $f$. \n",
    "\n",
    "This `approach` does `not impose` any `pre-specified model` on $f$. \n",
    "\n",
    "It `instead` attempts to `produce` an `estimate` for $f$ that is as `close as possible to the observed\n",
    "data`, subject to the fit—that is, the `yellow surface` in <a href=\"#Figure2.5\">Figure 2.5</a>—_being Smooth_\n",
    "\n",
    "In this case, the `non-parametric fit` has `produced` a `remarkably accurate estimate` of the `true` $f$ shown in <a href=\"#Figure2.3\">Figure 2.3</a>. \n",
    "\n",
    "In order to `fit` a `thin-plate spline`, the `data analyst` must `select` a `level of smoothness`. \n",
    "\n",
    "<a href=\"#Figure2.6\">Figure 2.6</a> shows the same `thin-plate spline fit` using a `lower level of smoothness`, allowing for a `rougher fit`. \n",
    "\n",
    "The resulting estimate __`fits the observed data perfectly`!__\n",
    "\n",
    "However, the `spline fit` shown in <a href=\"#Figure2.6\">Figure 2.6</a> is `far more variable` than the true function $f$ ,from <a href=\"#Figure2.3\">Figure 2.3</a>. \n",
    "\n",
    "This is an `example` of `overfitting the data`, which we discussed previously. \n",
    "\n",
    "It is an `undesirable situation` because the fit obtained will `not yield accurate estimates` of the `response` on __new observations that were not part of the original training data set__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x \\in A$\n",
    "\n",
    "\\mathbf{x} = \\langle x_{1}, x_{2}, \\dots, x_{n}\\rangle$ (\\bm from the bm pacakge would be better)\n",
    "\n",
    "$\\epsilon - \\varepsilon E$\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
