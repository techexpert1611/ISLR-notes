{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3 Linear Regression\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&bullet; __Linear Regression__ is useful tool for predicting a `Quantitive Response`.\n",
    "\n",
    "-> Very Simple Approach for Supervised Learning.\n",
    "\n",
    "__NOTE: It comes Under Supervised Learning Methods.]__\n",
    "\n",
    "\n",
    ">___Recall___ the `Advertising data` from <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Chapter%202.ipynb#Chapter-2:-Statastical-Learning\">Chapter 2</a>. \n",
    "<br><a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Figures/Figure2.1.png\">Figure 2.1</a> displays `sales` (in thousands of units) for a `particular product` as a function of `advertising budgets` (in thousands of dollars) __for `TV` , `radio` , and `newspaper media`__.\n",
    "<br>Suppose that in our role as `statistical consultants` we are `asked to suggest`, on the `basis of this data`, a `marketing plan for next year` that will `result in high product sales`. \n",
    "\n",
    "> ___What `information` would be `useful` in order to `provide` such a `recommendation`?___ \n",
    "\n",
    "Here are a few important questions that we might seek to address: \n",
    "\n",
    "1. __Is there a relationship between advertising budget and sales?__\n",
    "    \n",
    "    Our `first goal` should be to `determine` whether the `data provide evidence` of an `association` between `advertising expenditure and sales`.\n",
    "\n",
    "    If the `evidence is weak`, then one might argue that `no money should be spent on advertising`!\n",
    "\n",
    "\n",
    "2. __How strong is the relationship between advertising budget and sales?__\n",
    "    \n",
    "    Assuming that there is a `relationship between advertising and sales`, we would like to `know the strength` of this `relationship`. \n",
    "    \n",
    "    In `other words`, given a certain `advertising budget`, can we `predict sales` with a `high level` of `accuracy`? This would be a `strong relationship`. \n",
    "    \n",
    "    Or is a `prediction` of `sales based` on `advertising expenditure only` slightly `better` than a `random guess`? \n",
    "    \n",
    "    This would be a `weak relationship`.\n",
    "    \n",
    "    \n",
    "3. __Which media contribute to sales?__\n",
    "\n",
    "    Do `all three` __media—TV, radio, and newspaper—contribute to sales__, or do `just one or two` of the `media contribute`? \n",
    "    \n",
    "    To answer this `question`, we `must find` a `way` to `separate out` the `individual effects` of `each medium` when we have `spent money` on `all three media`.\n",
    "\n",
    "\n",
    "4. __How accurately can we estimate the effect of each medium on sales?__\n",
    "\n",
    "    For `every dollar spent` on `advertising` in a `particular medium`, by `what amount` will `sales increase`? \n",
    "    \n",
    "    How `accurately` can `we predict` this `amount of increase`?\n",
    "\n",
    "\n",
    "5. __How accurately can we predict future sales?__\n",
    "\n",
    "    For any given level of `television, radio, or newspaper advertising`, what is `our prediction` for `sales`, and what is the `accuracy` of `this prediction`?\n",
    "\n",
    "\n",
    "6. __Is the relationship linear?__\n",
    "\n",
    "    If there is `approximately` a `straight-line relationship` between `advertising expenditure` in the `various media` and `sales`, then `linear regression` is an `appropriate tool`. \n",
    "    \n",
    "    If not, then it may still be possible to `transform` the `predictor or the response` so that `linear regression` can be used.\n",
    "\n",
    "\n",
    "7. __Is there synergy among the advertising media?__\n",
    "\n",
    "    Perhaps `spending` $50,000$ on `television advertising` and $50,000$ on `radio advertising` `results` in `more sales` than `allocating` $100,000$ to `either television` or `radio individually`. \n",
    "    \n",
    "    In `marketing`, this is `known` as a __synergy effect__, while in `statistics` it is `called` an __interaction effect__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Simple linear regression`__ lives up to its name: it is a very `straightforward`\n",
    "approach for `predicting` a __quantitative response__ $Y$ on the basis of a `single predictor variable` $X$. \n",
    "\n",
    "It assumes that there is `approximately` a __`linear relationship between X and Y`__. \n",
    "\n",
    "Mathematically, we can write this `linear relationship` as:\n",
    "\n",
    "<a id=\"Formula3.1\"></a>\n",
    "<font size=5>$ Y \\approx \\beta_{0}+\\beta_{1}X$</font>\n",
    "\n",
    "You might read “$\\approx$” as _“is approximately modeled as”_.\n",
    "\n",
    ">For example, \n",
    "<br>$X$ may represent `TV advertising` and \n",
    "<br>$Y$ may represent `sales` .\n",
    "<br>Then we can `regress sales onto TV` by `fitting the model`\n",
    "<br><br>$ sales \\approx \\beta_{0}+\\beta_{1} TV$\n",
    "\n",
    "In <a href=\"#Formula3.1\">Equation 3.1</a>, $β_{0}$ and $β_{1}$ are `two unknown constants` that represent\n",
    "the __intercept__ and __slope__ terms in the `linear model`. \n",
    "\n",
    "Together, $β_{0}$ and $β_{1}$ are `known` as the `model` __coefficients or parameters__. \n",
    "\n",
    "Once we have used our `training data` to `produce estimates` $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ for the `model coefficients`, we\n",
    "can `predict future sales` on the basis of a `particular value of TV advertising` by computing\n",
    "\n",
    "<a id=\"Formula3.2\"></a>\n",
    ">$ \\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x$\n",
    ">>where $\\hat{y}$ indicates a `prediction of` $Y$ on the basis of $X = x$.\n",
    "<br>Here we use a dash symbol, $'$ , to `denote` the `estimated value` for an `unknown` __parameter or coefficient__, or to `denote` the `predicted value` of the `response`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Estimating the Coefficients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `practice`, $β_{0}$ and $β_{1}$ are `unknown`. \n",
    "\n",
    "So before we can use <a href=\"#Formula3.1\">(3.1)</a> to make `predictions`, we must `use data` to `estimate` the `coefficients`.\n",
    "\n",
    "$(x_{1} , y_{1} ), (x_{2} , y_{2} ), \\dots , (x_{n} , y_{n} )$\n",
    "\n",
    "\n",
    "Let `represent` $n$ `observation pairs`, each of which consists of a `measurement of` $X$ and a `measurement of` $Y$ . \n",
    "\n",
    ">In the __`Advertising` example__, \n",
    "<br>this `data set` consists of the `TV advertising budget` and `product sales` in n = 200 `different markets`.(Recall that the data are displayed in <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Figures/Figure2.1.png\">Figure 2.1</a>.) \n",
    "<br><br>__Our `goal`__ is to `obtain coefficient estimates` $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ such that the `linear model` <a href=\"#Formula3.1\">(3.1)</a> `fits` the available `data well-that` is, so that $y_{i} \\approx \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i}$ for $i = 1,\\dots , n$.\n",
    "<br><br>In `other words`, we `want` to `find` an __`intercept`__ $\\hat{\\beta}_{0}$ and a __`slope`__ $\\hat{\\beta}_{1}$ such that the `resulting line` is as `close` as `possible` to the $n = 200$ `data points`. \n",
    "\n",
    "\n",
    "&bullet; There are a `number of ways` of `measuring closeness`. \n",
    "\n",
    "However, by far the `most common approach` involves `minimizing` the __`least squares`__ `criterion`, and we take that approach for us in this sessions.\n",
    "\n",
    "\n",
    "<a id=\"Figure3.1\"></a>\n",
    "![image.png](Figures/Figure3.1.png)\n",
    ">__`FIGURE 3.1`.__ For the `Advertising data`, the __`least squares` fit__ for the `regression` of `sales onto TV` is shown. \n",
    "<br>The `fit` is `found` by `minimizing the sum` of `squared errors`. Each `grey line segment` represents an `error`, and the `fit makes` a `compromise` by `averaging their squares`. \n",
    "<br>In this `case` a `linear fit captures` the essence of\n",
    "the `relationship`, although it is somewhat `deficient` in `the left of the plot`.\n",
    "\n",
    "\n",
    "Let $\\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} \\hat{x}_{i}$ be the `prediction` for $Y$ based on the $ith$ value of $X$.\n",
    "\n",
    "Then $e_{i} = y_{i} − \\hat{y}_{i}$ represents the $ith$ __`residual`__—this is the `difference between` the $ith$ `observed response value` and the $ith$  `response value` that is `predicted by our linear model`. \n",
    "\n",
    "We define the __residual sum of squares (RSS)__ as\n",
    "\n",
    "<font size=\"5\"> $ RSS = e_{1}^2+e_{2}^2+\\dots+e_{n}^2$</font>\n",
    "\n",
    "or equivalently as\n",
    "\n",
    "<font size=\"5\"> $ RSS = (y_{1} − \\hat{\\beta}_{0} − \\hat{\\beta}_{1} x_{1})^2 + (y_{2} − \\hat{\\beta}_{0} − \\hat{\\beta}_{1} x_{2})^2+ \\dots+ (y_{n} − \\hat{\\beta}_{0} − \\hat{\\beta}_{1} x_{n})^2$</font>\n",
    "\n",
    "\n",
    ">The __`least squares` approach__ chooses $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ to `minimize the RSS`. \n",
    "<br>Using some calculus, one can show that the minimizers are\n",
    "\n",
    "<a id=\"Formula3.4\"></a>\n",
    "><font size=\"5\">\n",
    "$ \\hat{\\beta}_{1}$ = $\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x}) (y_{i}-\\bar{y})}{\\sum_{i=1}^{n} (x_{i} - \\bar{x})^2} $\n",
    "<br>$ \\hat{\\beta}_{0} = \\bar{y} − \\hat{\\beta}_{1} \\bar{x}$\n",
    "</font>\n",
    "\n",
    ">where <br><font size=4>$ \\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$</font> and <font size=>$ \\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^{n} x_{i}$</font> are the `sample means`. \n",
    "<br>In other words, <a href=\"#Formula3.4\">(3.4)</a> defines the `least squares coefficient` estimates for `simple linear regression`.\n",
    "\n",
    "<a href=\"#Figure3.1\">Figure 3.1</a> `displays` the `simple linear regression fit` to the `Advertising data`, where $\\hat{\\beta}_{0} = 7.03$ and $\\hat{\\beta}_{1} = 0.0475$. \n",
    "\n",
    "In other words, according to this `approximation`, an `additional` $1,000$ `spent` on `TV advertising` is `associated with selling` approximately 47.5 additional `units of the product`. \n",
    "\n",
    "<a id=\"Figure3.2\"></a>\n",
    "![image.png](Figures/Figure3.2.png)\n",
    "\n",
    "\n",
    "In <a href=\"#Figure3.2\">Figure 3.2</a>, we have `computed RSS` for a `number of values` of $\\beta_{0} and \\beta_{1}$ , using the `advertising data with sales` as the `response` and `TV` as the `predictor`. \n",
    "\n",
    "In each `plot`, the `red dot` represents the `pair of least squares estimates` $ \\hat{\\beta}_{0}, \\hat{\\beta}_{1}$ given by <a href=\"#Formula3.4\">(3.4)</a>. \n",
    "\n",
    "These values `clearly minimize` the __`RSS`__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Assessing the Accuracy of the Coefficient Estimates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from <a href=\"#Formula2.1\">(2.1)</a> that we `assume` that the `true relationship between X and Y` takes the `form` $Y = f (X) + \\epsilon$ for some `unknown function` $f$ , \n",
    ">where \n",
    "<br>$\\epsilon$ is a `mean-zero random error` term. \n",
    "<br>If $f$ is to be `approximated` by a `linear function`, then we can `write` this `relationship` as\n",
    "<br><br><a id=\"Formula3.5\"></a>\n",
    "<font size=4>$Y = \\beta_{0} + \\beta_{1} X + \\epsilon$.</font>\n",
    "\n",
    ">Here $\\beta_{0}$ is the \n",
    "<br>__intercept__ term—that is, __the expected value of $Y$ when $X = 0$, \n",
    "\n",
    ">and $\\beta_{1}$ is the \n",
    "<br>__slope__—__the average increase in $Y$ associated with a one-unit increase in $X$. \n",
    "\n",
    ">The __error term__ is a `catch-all` for what we miss with this simple model: \n",
    "<br>the true relationship is probably not linear, there may be other variables that cause variation in $Y$ , and there may be measurement error. \n",
    "<br>We typically assume that the error term is independent of $X$.\n",
    "\n",
    "***\n",
    "The `model` given by <a href=\"#Formula3.5\">(3.5)</a> defines the `population regression line`, which is the `best linear approximation` to the `true relationship between` $X$ and $Y$ . \n",
    "\n",
    "The `least squares regression coefficient estimates` The `model` given by <a href=\"#Formula3.4\">(3.4)</a> `characterize` the `least squares` line The `model` given by <a href=\"#Formula3.2\">(3.2)</a>. \n",
    "\n",
    "<a id=\"Figure3.3\"></a>\n",
    "![image.png](Figures/Figure3.3.png)\n",
    ">__FIGURE 3.3__. A `simulated data set`. \n",
    "\n",
    ">__Left__: The `red line` represents the `true relationship`, $f (X) = 2 + 3X$, which is `known` as the `population regression line`. \n",
    "<br.The `blue line` is the `least squares line`; it is the `least squares estimate` for $f (X)$ based\n",
    "on the `observed data`, shown in `black`. \n",
    "\n",
    ">__Right__: The `population regression line` is\n",
    "again `shown in red`, and the `least squares line` in `dark blue`. \n",
    "<br>In `light blue`, ten `least squares lines` are shown, `each computed` on the basis of a `separate random set` of `observations`. \n",
    "\n",
    "Each `least squares line` is `different`, but `on average`, the `least squares lines` are `quite close` to the `population regression line`\n",
    "\n",
    "The __left-hand__ panel of The `model` given by <a href=\"#Figure3.3\">Figure 3.3</a> displays these two lines in a simple simulated example.\n",
    "\n",
    "We created 100 random $X_{s}$, and generated 100 corresponding $Y_{s}$ from the model.\n",
    "\n",
    "<font size=4>$Y = 2 + 3X + \\epsilon$</font>\n",
    "\n",
    "Where $\\epsilon$ was generated from a `normal distribution` with `mean zero`.\n",
    "\n",
    "The `red line` in the `left-hand panel` of <a href=\"#Figure3.3\">Figure 3.3</a> displays the `true relationship`,\n",
    "$f (X) = 2 + 3X $, while the `blue line` is the `least squares estimate` based on the `observed data`. \n",
    "\n",
    "The `true relationship` is `generally not known` for `real data`, but the `least squares line` can always be `computed using` the `coefficient estimates` given in <a href=\"#Formula3.4\">(3.4)</a>\n",
    "\n",
    "***\n",
    "***\n",
    "The `analogy` between `linear regression` and `estimation of the mean` of a `random variable` is an apt one based on the concept of __bias__. \n",
    "\n",
    "If we use the `sample mean` $\\hat{\\mu}$ to estimate $\\mu$, this `estimate` is __unbiased__, in the sense that\n",
    "`on average`, we `expect` $\\hat{\\mu}$ to equal $\\mu$.\n",
    "\n",
    "__What exactly does this mean?__\n",
    "\n",
    "[ Visit __Page No : 65 of ISLR BOOK___]\n",
    "***\n",
    "\n",
    "\n",
    "***\n",
    "We `continue` the `analogy` with the `estimation` of the `population mean` $\\mu$ of a `random variable` $Y$ .\n",
    "\n",
    "A `natural question` is as follows: __how accurate is the sample mean $\\hat{\\mu}$ as an estimate of μ?__\n",
    "\n",
    "We have established that the `average` of $\\hat{\\mu}$'s `over many data sets` will be `very close` to μ, but that a\n",
    "`single estimate` $\\hat{\\mu}$ may be a `substantial underestimate` or `overestimate` of $\\mu$.\n",
    "\n",
    "__How `far off` will that `single estimate` of $\\hat{\\mu}$be? __\n",
    "\n",
    "In general, we answer this `question` by `computing the standard error` of $\\hat{\\mu}$, written as $SE(\\hat{\\mu})$. \n",
    "\n",
    "We have the well-known formula\n",
    "\n",
    "<a id=\"Formula3.7\"></a>\n",
    "<font size=5> $Var(\\hat{\\mu}) = SE(\\hat{\\mu})^2 = \\frac{\\sigma^2}{n}$ </font>\n",
    ">where $\\sigma$ is the __standard deviation__ of each of the `realizations` $y_{i}$ of $Y^2$.\n",
    "***\n",
    "\n",
    "***\n",
    "`Roughly speaking`, the __standard error__ tells us the `average amount` that this\n",
    "`estimate` $\\hat{\\mu}$ `differs` from the `actual value` of $\\mu$.\n",
    "***\n",
    "\n",
    "***\n",
    "<a href=\"#Formula3.7\">Equation 3.7</a> also tells us __`how this deviation shrinks with n`__—the `more observations` we have, the `smaller the standard error` of $\\hat{\\mu}$\n",
    "\n",
    "In a `similar vein`, we can `wonder` how `close` \\hat{\\beta}_{0}\n",
    "and \\hat{\\beta}_{1} are to the `true values` $\\beta_{0}$ and $\\beta_{1}$ .\n",
    "\n",
    "To `compute` the __`standard errors`__ associated with $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ , we use the following formulas:\n",
    "\n",
    "\n",
    "<a id=\"Formula3.8\"></a>\n",
    "<font size=5> $ SE ( \\hat{\\beta}_{0})^2 = \\sigma^2 [ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i = 1}^{n} ( x_{i} - \\bar{x}_{i})^2} ], SE ( \\hat{\\beta}_{1})^2 = \\sigma^2 [ \\frac{1}{n} + \\frac{\\sigma^2}{\\sum_{i = 1}^{n} ( x_{i} - \\bar{x}_{i})^2} ]$ </font>\n",
    ">where $\\sigma^2 = Var(\\epsilon)$\n",
    "\n",
    "[Visit __Book Page No 66__]\n",
    "\n",
    "In general, $\\sigma^2$ is `not known`, but `can be estimated` from the `data`. \n",
    "\n",
    "The estimate of $\\sigma$ is `known` as the __residual standard error__, and is given by the formula\n",
    "\n",
    "<font size=5> $ RSE = \\sqrt{ \\frac{RSS}{(n - 2)}}$</font>\n",
    "\n",
    "***\n",
    "`Standard errors` can be used to `compute confidence intervals`. \n",
    "\n",
    "A $95\\%$ `confidence interval` is `defined` as a `range of values` such that `with` $95 \\%$ `probability`, the `range` will contain the `true unknown value of the parameter`.\n",
    "\n",
    "The `range` is `defined` in terms of `lower and upper limits` computed from the\n",
    "`sample of data`. \n",
    "\n",
    "For `linear regression`, the $95 \\%$ `confidence interval` for $\\beta_{1}$ approximately takes the form\n",
    "\n",
    "<a id=\"Formula3.9\"></a>\n",
    "<font size=4> $ \\hat{\\beta}_{1} \\pm 2 $ &sdot; $SE (\\hat{\\beta}_{1}) $ </font>\n",
    "\n",
    "That is, there is approximately a 95 % chance that the interval will contain the true value of $\\beta_{1}$\n",
    "\n",
    "<a id=\"Formula3.10\"></a>\n",
    "<font size=4> [$ \\hat{\\beta}_{1} - 2 $&sdot;$ SE (\\hat{\\beta}_{1}) $,$ \\hat{\\beta}_{1} + 2 $ &sdot; $SE (\\hat{\\beta}_{1}) $ ]</font>\n",
    "\n",
    "Similarly, a `confidence interval` for $\\beta_{0}$ approximately takes the form\n",
    "\n",
    "<a id=\"Formula3.11\"></a>\n",
    "<font size=4> $ \\hat{\\beta}_{0} \\pm 2 $ &sdot; $SE (\\hat{\\beta}_{0}) $ </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the __advertising data__, \n",
    "<br>the $95 \\%$ `confidence interval` for $\\beta_{0}$ is [6.130, 7.935] and \n",
    "<br>the $95\\% $ `confidence interval` for $\\beta_{1}$ is [0.042, 0.053].\n",
    "\n",
    "Therefore, we can `conclude` that in the `absence` of any `advertising`, `sales` will, on `average`, `fall somewhere` between 6,130 and 7,940 units. \n",
    "\n",
    "Furthermore,for each $1,000$USD `increase` in `television advertising`, there will be an `average increase` in sales of between 42 and 53 units.\n",
    "\n",
    "***\n",
    "`Standard errors` can also be used to `perform` __hypothesis tests__ on the `coefficients`. \n",
    "\n",
    "The `most common hypothesis test` involves `testing the null hypothesis` of\n",
    "\n",
    "<a id=\"Formula3.12\"></a>\n",
    "<font size=3> $H_{0} $: There is no relationship between $X$ and $Y$</font>\n",
    "\n",
    "versus the alternative hypothesis\n",
    "\n",
    "<a id=\"Formula3.13\"></a>\n",
    "<font size=3> $H_{a} $: There is some relationship between $X$ and $Y$</font>\n",
    "\n",
    "\n",
    "Mathematically, this corresponds to `testing`\n",
    "\n",
    "<font size=3> $H_{0} : \\beta_{1} = 0$</font>\n",
    "\n",
    "Versus \n",
    "\n",
    "<font size=3> $H_{0} : \\beta_{1} \\neq 0$</font>\n",
    "\n",
    "***\n",
    "Page No 67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$β_{0}$ and $β_{1}$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
