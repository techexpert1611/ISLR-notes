{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3 Linear Regression\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&bullet; __Linear Regression__ is useful tool for predicting a `Quantitive Response`.\n",
    "\n",
    "-> Very Simple Approach for Supervised Learning.\n",
    "\n",
    "__NOTE: It comes Under Supervised Learning Methods.]__\n",
    "\n",
    "\n",
    ">___Recall___ the `Advertising data` from <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Chapter%202.ipynb#Chapter-2:-Statastical-Learning\">Chapter 2</a>. \n",
    "<br><a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Figures/Figure2.1.png\">Figure 2.1</a> displays `sales` (in thousands of units) for a `particular product` as a function of `advertising budgets` (in thousands of dollars) __for `TV` , `radio` , and `newspaper media`__.\n",
    "<br>Suppose that in our role as `statistical consultants` we are `asked to suggest`, on the `basis of this data`, a `marketing plan for next year` that will `result in high product sales`. \n",
    "\n",
    "> ___What `information` would be `useful` in order to `provide` such a `recommendation`?___ \n",
    "\n",
    "Here are a few important questions that we might seek to address: \n",
    "\n",
    "1. __Is there a relationship between advertising budget and sales?__\n",
    "    \n",
    "    Our `first goal` should be to `determine` whether the `data provide evidence` of an `association` between `advertising expenditure and sales`.\n",
    "\n",
    "    If the `evidence is weak`, then one might argue that `no money should be spent on advertising`!\n",
    "\n",
    "\n",
    "2. __How strong is the relationship between advertising budget and sales?__\n",
    "    \n",
    "    Assuming that there is a `relationship between advertising and sales`, we would like to `know the strength` of this `relationship`. \n",
    "    \n",
    "    In `other words`, given a certain `advertising budget`, can we `predict sales` with a `high level` of `accuracy`? This would be a `strong relationship`. \n",
    "    \n",
    "    Or is a `prediction` of `sales based` on `advertising expenditure only` slightly `better` than a `random guess`? \n",
    "    \n",
    "    This would be a `weak relationship`.\n",
    "    \n",
    "    \n",
    "3. __Which media contribute to sales?__\n",
    "\n",
    "    Do `all three` __media—TV, radio, and newspaper—contribute to sales__, or do `just one or two` of the `media contribute`? \n",
    "    \n",
    "    To answer this `question`, we `must find` a `way` to `separate out` the `individual effects` of `each medium` when we have `spent money` on `all three media`.\n",
    "\n",
    "\n",
    "4. __How accurately can we estimate the effect of each medium on sales?__\n",
    "\n",
    "    For `every dollar spent` on `advertising` in a `particular medium`, by `what amount` will `sales increase`? \n",
    "    \n",
    "    How `accurately` can `we predict` this `amount of increase`?\n",
    "\n",
    "\n",
    "5. __How accurately can we predict future sales?__\n",
    "\n",
    "    For any given level of `television, radio, or newspaper advertising`, what is `our prediction` for `sales`, and what is the `accuracy` of `this prediction`?\n",
    "\n",
    "\n",
    "6. __Is the relationship linear?__\n",
    "\n",
    "    If there is `approximately` a `straight-line relationship` between `advertising expenditure` in the `various media` and `sales`, then `linear regression` is an `appropriate tool`. \n",
    "    \n",
    "    If not, then it may still be possible to `transform` the `predictor or the response` so that `linear regression` can be used.\n",
    "\n",
    "\n",
    "7. __Is there synergy among the advertising media?__\n",
    "\n",
    "    Perhaps `spending` $50,000$ on `television advertising` and $50,000$ on `radio advertising` `results` in `more sales` than `allocating` $100,000$ to `either television` or `radio individually`. \n",
    "    \n",
    "    In `marketing`, this is `known` as a __synergy effect__, while in `statistics` it is `called` an __interaction effect__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Simple linear regression`__ lives up to its name: it is a very `straightforward`\n",
    "approach for `predicting` a __quantitative response__ $Y$ on the basis of a `single predictor variable` $X$. \n",
    "\n",
    "It assumes that there is `approximately` a __`linear relationship between X and Y`__. \n",
    "\n",
    "Mathematically, we can write this `linear relationship` as:\n",
    "\n",
    "<a id=\"Formula3.1\"></a>\n",
    "<font size=5>$ Y \\approx \\beta_{0}+\\beta_{1}X$</font>\n",
    "\n",
    "You might read “$\\approx$” as _“is approximately modeled as”_.\n",
    "\n",
    ">For example, \n",
    "<br>$X$ may represent `TV advertising` and \n",
    "<br>$Y$ may represent `sales` .\n",
    "<br>Then we can `regress sales onto TV` by `fitting the model`\n",
    "<br><br>$ sales \\approx \\beta_{0}+\\beta_{1} TV$\n",
    "\n",
    "In <a href=\"#Formula3.1\">Equation 3.1</a>, $β_{0}$ and $β_{1}$ are `two unknown constants` that represent\n",
    "the __intercept__ and __slope__ terms in the `linear model`. \n",
    "\n",
    "Together, $β_{0}$ and $β_{1}$ are `known` as the `model` __coefficients or parameters__. \n",
    "\n",
    "Once we have used our `training data` to `produce estimates` $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ for the `model coefficients`, we\n",
    "can `predict future sales` on the basis of a `particular value of TV advertising` by computing\n",
    "\n",
    "<a id=\"Formula3.2\"></a>\n",
    ">$ \\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x$\n",
    ">>where $\\hat{y}$ indicates a `prediction of` $Y$ on the basis of $X = x$.\n",
    "<br>Here we use a dash symbol, $'$ , to `denote` the `estimated value` for an `unknown` __parameter or coefficient__, or to `denote` the `predicted value` of the `response`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Estimating the Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `practice`, $β_{0}$ and $β_{1}$ are `unknown`. \n",
    "\n",
    "So before we can use <a href=\"#Formula3.1\">(3.1)</a> to make `predictions`, we must `use data` to `estimate` the `coefficients`.\n",
    "\n",
    "$(x_{1} , y_{1} ), (x_{2} , y_{2} ), \\dots , (x_{n} , y_{n} )$\n",
    "\n",
    "\n",
    "Let `represent` $n$ `observation pairs`, each of which consists of a `measurement of` $X$ and a `measurement of` $Y$ . \n",
    "\n",
    ">In the __`Advertising` example__, \n",
    "<br>this `data set` consists of the `TV advertising budget` and `product sales` in n = 200 `different markets`.(Recall that the data are displayed in <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Figures/Figure2.1.png\">Figure 2.1</a>.) \n",
    "<br><br>__Our `goal`__ is to `obtain coefficient estimates` $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ such that the `linear model` <a href=\"#Formula3.1\">(3.1)</a> `fits` the available `data well-that` is, so that $y_{i} \\approx \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i}$ for $i = 1,\\dots , n$.\n",
    "<br><br>In `other words`, we `want` to `find` an __`intercept`__ $\\hat{\\beta}_{0}$ and a __`slope`__ $\\hat{\\beta}_{1}$ such that the `resulting line` is as `close` as `possible` to the $n = 200$ `data points`. \n",
    "\n",
    "\n",
    "&bullet; There are a `number of ways` of `measuring closeness`. \n",
    "\n",
    "However, by far the `most common approach` involves `minimizing` the __`least squares`__ `criterion`, and we take that approach for us in this sessions.\n",
    "\n",
    "\n",
    "<a id=\"Figure3.1\"></a>\n",
    "![image.png](Figures/Figure3.1.png)\n",
    ">__`FIGURE 3.1`.__ For the `Advertising data`, the __`least squares` fit__ for the `regression` of `sales onto TV` is shown. \n",
    "<br>The `fit` is `found` by `minimizing the sum` of `squared errors`. Each `grey line segment` represents an `error`, and the `fit makes` a `compromise` by `averaging their squares`. \n",
    "<br>In this `case` a `linear fit captures` the essence of\n",
    "the `relationship`, although it is somewhat `deficient` in `the left of the plot`.\n",
    "\n",
    "\n",
    "Let $\\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} \\hat{x}_{i}$ be the `prediction` for $Y$ based on the $ith$ value of $X$.\n",
    "\n",
    "Then $e_{i} = y_{i} − \\hat{y}_{i}$ represents the $ith$ __`residual`__—this is the `difference between` the $ith$ `observed response value` and the $ith$  `response value` that is `predicted by our linear model`. \n",
    "\n",
    "We define the __residual sum of squares (RSS)__ as\n",
    "\n",
    "<font size=\"5\"> $ RSS = e_{1}^2+e_{2}^2+\\dots+e_{n}^2$</font>\n",
    "\n",
    "or equivalently as\n",
    "\n",
    "<font size=\"5\"> $ RSS = (y_{1} − \\hat{\\beta}_{0} − \\hat{\\beta}_{1} x_{1})^2 + (y_{2} − \\hat{\\beta}_{0} − \\hat{\\beta}_{1} x_{2})^2+ \\dots+ (y_{n} − \\hat{\\beta}_{0} − \\hat{\\beta}_{1} x_{n})^2$</font>\n",
    "\n",
    "\n",
    ">The __`least squares` approach__ chooses $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ to `minimize the RSS`. \n",
    "<br>Using some calculus, one can show that the minimizers are\n",
    "\n",
    "<a id=\"Formula3.4\"></a>\n",
    "><font size=\"5\">\n",
    "$ \\hat{\\beta}_{1}$ = $\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x}) (y_{i}-\\bar{y})}{\\sum_{i=1}^{n} (x_{i} - \\bar{x})^2} $\n",
    "<br>$ \\hat{\\beta}_{0} = \\bar{y} − \\hat{\\beta}_{1} \\bar{x}$\n",
    "</font>\n",
    "\n",
    ">where <br><font size=4>$ \\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$</font> and <font size=>$ \\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^{n} x_{i}$</font> are the `sample means`. \n",
    "<br>In other words, <a href=\"#Formula3.4\">(3.4)</a> defines the `least squares coefficient` estimates for `simple linear regression`.\n",
    "\n",
    "<a href=\"#Figure3.1\">Figure 3.1</a> `displays` the `simple linear regression fit` to the `Advertising data`, where $\\hat{\\beta}_{0} = 7.03$ and $\\hat{\\beta}_{1} = 0.0475$. \n",
    "\n",
    "In other words, according to this `approximation`, an `additional` $1,000$ `spent` on `TV advertising` is `associated with selling` approximately 47.5 additional `units of the product`. \n",
    "\n",
    "<a id=\"Figure3.2\"></a>\n",
    "![image.png](Figures/Figure3.2.png)\n",
    "\n",
    "\n",
    "In <a href=\"#Figure3.2\">Figure 3.2</a>, we have `computed RSS` for a `number of values` of $\\beta_{0} and \\beta_{1}$ , using the `advertising data with sales` as the `response` and `TV` as the `predictor`. \n",
    "\n",
    "In each `plot`, the `red dot` represents the `pair of least squares estimates` $ \\hat{\\beta}_{0}, \\hat{\\beta}_{1}$ given by <a href=\"#Formula3.4\">(3.4)</a>. \n",
    "\n",
    "These values `clearly minimize` the __`RSS`__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Assessing the Accuracy of the Coefficient Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from <a href=\"#Formula2.1\">(2.1)</a> that we `assume` that the `true relationship between X and Y` takes the `form` $Y = f (X) + \\epsilon$ for some `unknown function` $f$ , \n",
    ">where \n",
    "<br>$\\epsilon$ is a `mean-zero random error` term. \n",
    "<br>If $f$ is to be `approximated` by a `linear function`, then we can `write` this `relationship` as\n",
    "<br><br><a id=\"Formula3.5\"></a>\n",
    "<font size=4>$Y = \\beta_{0} + \\beta_{1} X + \\epsilon$.</font>\n",
    "\n",
    ">Here $\\beta_{0}$ is the \n",
    "<br>__intercept__ term—that is, __the expected value of $Y$ when $X = 0$, \n",
    "\n",
    ">and $\\beta_{1}$ is the \n",
    "<br>__slope__—__the average increase in $Y$ associated with a one-unit increase in $X$. \n",
    "\n",
    ">The __error term__ is a `catch-all` for what we miss with this simple model: \n",
    "<br>the true relationship is probably not linear, there may be other variables that cause variation in $Y$ , and there may be measurement error. \n",
    "<br>We typically assume that the error term is independent of $X$.\n",
    "\n",
    "***\n",
    "The `model` given by <a href=\"#Formula3.5\">(3.5)</a> defines the `population regression line`, which is the `best linear approximation` to the `true relationship between` $X$ and $Y$ . \n",
    "\n",
    "The `least squares regression coefficient estimates` The `model` given by <a href=\"#Formula3.4\">(3.4)</a> `characterize` the `least squares` line The `model` given by <a href=\"#Formula3.2\">(3.2)</a>. \n",
    "\n",
    "<a id=\"Figure3.3\"></a>\n",
    "![image.png](Figures/Figure3.3.png)\n",
    ">__FIGURE 3.3__. A `simulated data set`. \n",
    "\n",
    ">__Left__: The `red line` represents the `true relationship`, $f (X) = 2 + 3X$, which is `known` as the `population regression line`. \n",
    "<br.The `blue line` is the `least squares line`; it is the `least squares estimate` for $f (X)$ based\n",
    "on the `observed data`, shown in `black`. \n",
    "\n",
    ">__Right__: The `population regression line` is\n",
    "again `shown in red`, and the `least squares line` in `dark blue`. \n",
    "<br>In `light blue`, ten `least squares lines` are shown, `each computed` on the basis of a `separate random set` of `observations`. \n",
    "\n",
    "Each `least squares line` is `different`, but `on average`, the `least squares lines` are `quite close` to the `population regression line`\n",
    "\n",
    "The __left-hand__ panel of The `model` given by <a href=\"#Figure3.3\">Figure 3.3</a> displays these two lines in a simple simulated example.\n",
    "\n",
    "We created 100 random $X_{s}$, and generated 100 corresponding $Y_{s}$ from the model.\n",
    "\n",
    "<font size=4>$Y = 2 + 3X + \\epsilon$</font>\n",
    "\n",
    "Where $\\epsilon$ was generated from a `normal distribution` with `mean zero`.\n",
    "\n",
    "The `red line` in the `left-hand panel` of <a href=\"#Figure3.3\">Figure 3.3</a> displays the `true relationship`,\n",
    "$f (X) = 2 + 3X $, while the `blue line` is the `least squares estimate` based on the `observed data`. \n",
    "\n",
    "The `true relationship` is `generally not known` for `real data`, but the `least squares line` can always be `computed using` the `coefficient estimates` given in <a href=\"#Formula3.4\">(3.4)</a>\n",
    "\n",
    "***\n",
    "***\n",
    "The `analogy` between `linear regression` and `estimation of the mean` of a `random variable` is an apt one based on the concept of __bias__. \n",
    "\n",
    "If we use the `sample mean` $\\hat{\\mu}$ to estimate $\\mu$, this `estimate` is __unbiased__, in the sense that\n",
    "`on average`, we `expect` $\\hat{\\mu}$ to equal $\\mu$.\n",
    "\n",
    "__What exactly does this mean?__\n",
    "\n",
    "[ Visit __Page No : 65 of ISLR BOOK___]\n",
    "***\n",
    "\n",
    "\n",
    "***\n",
    "We `continue` the `analogy` with the `estimation` of the `population mean` $\\mu$ of a `random variable` $Y$ .\n",
    "\n",
    "A `natural question` is as follows: __how accurate is the sample mean $\\hat{\\mu}$ as an estimate of μ?__\n",
    "\n",
    "We have established that the `average` of $\\hat{\\mu}$'s `over many data sets` will be `very close` to μ, but that a\n",
    "`single estimate` $\\hat{\\mu}$ may be a `substantial underestimate` or `overestimate` of $\\mu$.\n",
    "\n",
    "__How `far off` will that `single estimate` of $\\hat{\\mu}$be? __\n",
    "\n",
    "In general, we answer this `question` by `computing the standard error` of $\\hat{\\mu}$, written as $SE(\\hat{\\mu})$. \n",
    "\n",
    "We have the well-known formula\n",
    "\n",
    "<a id=\"Formula3.7\"></a>\n",
    "<font size=5> $Var(\\hat{\\mu}) = SE(\\hat{\\mu})^2 = \\frac{\\sigma^2}{n}$ </font>\n",
    ">where $\\sigma$ is the __standard deviation__ of each of the `realizations` $y_{i}$ of $Y^2$.\n",
    "***\n",
    "\n",
    "***\n",
    "`Roughly speaking`, the __standard error__ tells us the `average amount` that this\n",
    "`estimate` $\\hat{\\mu}$ `differs` from the `actual value` of $\\mu$.\n",
    "***\n",
    "\n",
    "***\n",
    "<a href=\"#Formula3.7\">Equation 3.7</a> also tells us __`how this deviation shrinks with n`__—the `more observations` we have, the `smaller the standard error` of $\\hat{\\mu}$\n",
    "\n",
    "In a `similar vein`, we can `wonder` how `close` \\hat{\\beta}_{0}\n",
    "and \\hat{\\beta}_{1} are to the `true values` $\\beta_{0}$ and $\\beta_{1}$ .\n",
    "\n",
    "To `compute` the __`standard errors`__ associated with $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ , we use the following formulas:\n",
    "\n",
    "\n",
    "<a id=\"Formula3.8\"></a>\n",
    "<font size=5> $ SE ( \\hat{\\beta}_{0})^2 = \\sigma^2 [ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i = 1}^{n} ( x_{i} - \\bar{x}_{i})^2} ], SE ( \\hat{\\beta}_{1})^2 = \\sigma^2 [ \\frac{1}{n} + \\frac{\\sigma^2}{\\sum_{i = 1}^{n} ( x_{i} - \\bar{x}_{i})^2} ]$ </font>\n",
    ">where $\\sigma^2 = Var(\\epsilon)$\n",
    "\n",
    "[Visit __Book Page No 66__]\n",
    "\n",
    "In general, $\\sigma^2$ is `not known`, but `can be estimated` from the `data`. \n",
    "\n",
    "The estimate of $\\sigma$ is `known` as the __residual standard error__, and is given by the formula\n",
    "\n",
    "<font size=5> $ RSE = \\sqrt{ \\frac{RSS}{(n - 2)}}$</font>\n",
    "\n",
    "***\n",
    "`Standard errors` can be used to `compute confidence intervals`. \n",
    "\n",
    "A $95\\%$ `confidence interval` is `defined` as a `range of values` such that `with` $95 \\%$ `probability`, the `range` will contain the `true unknown value of the parameter`.\n",
    "\n",
    "The `range` is `defined` in terms of `lower and upper limits` computed from the\n",
    "`sample of data`. \n",
    "\n",
    "For `linear regression`, the $95 \\%$ `confidence interval` for $\\beta_{1}$ approximately takes the form\n",
    "\n",
    "<a id=\"Formula3.9\"></a>\n",
    "<font size=4> $ \\hat{\\beta}_{1} \\pm 2 $ &sdot; $SE (\\hat{\\beta}_{1}) $ </font>\n",
    "\n",
    "That is, there is approximately a 95 % chance that the interval will contain the true value of $\\beta_{1}$\n",
    "\n",
    "<a id=\"Formula3.10\"></a>\n",
    "<font size=4> [$ \\hat{\\beta}_{1} - 2 $&sdot;$ SE (\\hat{\\beta}_{1}) $,$ \\hat{\\beta}_{1} + 2 $ &sdot; $SE (\\hat{\\beta}_{1}) $ ]</font>\n",
    "\n",
    "Similarly, a `confidence interval` for $\\beta_{0}$ approximately takes the form\n",
    "\n",
    "<a id=\"Formula3.11\"></a>\n",
    "<font size=4> $ \\hat{\\beta}_{0} \\pm 2 $ &sdot; $SE (\\hat{\\beta}_{0}) $ </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the __advertising data__, \n",
    "<br>the $95 \\%$ `confidence interval` for $\\beta_{0}$ is [6.130, 7.935] and \n",
    "<br>the $95\\% $ `confidence interval` for $\\beta_{1}$ is [0.042, 0.053].\n",
    "\n",
    "Therefore, we can `conclude` that in the `absence` of any `advertising`, `sales` will, on `average`, `fall somewhere` between 6,130 and 7,940 units. \n",
    "\n",
    "Furthermore,for each $1,000$USD `increase` in `television advertising`, there will be an `average increase` in sales of between 42 and 53 units.\n",
    "\n",
    "***\n",
    "`Standard errors` can also be used to `perform` __hypothesis tests__ on the `coefficients`. \n",
    "\n",
    "The `most common hypothesis test` involves `testing the null hypothesis` of\n",
    "\n",
    "<a id=\"Formula3.12\"></a>\n",
    "<font size=3> $H_{0} $: There is no relationship between $X$ and $Y$</font>\n",
    "\n",
    "versus the alternative hypothesis\n",
    "\n",
    "<a id=\"Formula3.13\"></a>\n",
    "<font size=3> $H_{a} $: There is some relationship between $X$ and $Y$</font>\n",
    "\n",
    "\n",
    "Mathematically, this corresponds to `testing`\n",
    "\n",
    "<font size=3> $H_{0} : \\beta_{1} = 0$</font>\n",
    "\n",
    "Versus \n",
    "\n",
    "<font size=3> $H_{0} : \\beta_{1} \\neq 0$</font>\n",
    "\n",
    "since if $\\beta_{1} = 0$ then the model <a href=\"#Formula3.5\">(3.5)</a> reduces to $Y = \\beta_{0} + \\epsilon$, and $X$ is not `associated` with $Y$ . \n",
    "\n",
    "To `test` the `null hypothesis`, we need to `determine` whether $\\hat{\\beta}_{1}$ , our estimate for $\\beta_{1}$, is `sufficiently far` from `zero` that we can be `confident` that $\\beta_{1}$ is `non-zero`.\n",
    "\n",
    "___How far is far enough?___\n",
    "This of course `depends on` the `accuracy` of $\\hat{\\beta}_{1}$ —that is, it `depends` on $SE( \\hat{\\beta}_{1} )$. If $SE( \\hat{\\beta}_{1} )$ is `small`, then `even relatively small values` of $\\hat{\\beta}_{1}$ may `provide strong evidence` that $\\beta_{1} \\neq 0$, and hence that `there` is a `relationship between `$X$ and $Y$ . \n",
    "\n",
    "***\n",
    "In contrast, if $SE(  \\hat{\\beta}_{1}  ) $is `large`, then $ \\hat{\\beta}_{1} $ `must be large` in `absolute value` in order for us to `reject the null hypothesis`. \n",
    "\n",
    "In practice, we compute a __t-statistic__, given by\n",
    "<a id=\"Formula3.14\"></a>\n",
    "><font size=5> $ t = \\frac{ \\hat{\\beta}_{1} - 0}{SE(  \\hat{\\beta}_{1}  )} $ </font>\n",
    "\n",
    "which `measures` the `number of standard deviations` that $\\hat{\\beta}_{1}$ is `away` from\n",
    "0. \n",
    "\n",
    "If there `really` is `no relationship between` $X$ and $Y$ , then `we expect` that <a href=\"#Formula3.14\">(3.14)</a> will have a `t-distribution` with $n − 2$ `degrees of freedom`. \n",
    "\n",
    "The `t-distribution` has a `bell shape` and for `values of n` greater than approximately 30 it is `quite similar` to the `normal distribution`. \n",
    "\n",
    "Consequently, it is a `simple matter` to `compute the probability` of `observing` any `number equal` to $|t|$ or `larger in absolute value`, assuming $\\beta_{1}$. We call this `probability` the __`p-value`__.\n",
    "\n",
    "Roughly speaking, we `interpret` the __`p-value`__ as follows: a `small p-value` indicates that it is `unlikely` to observe such a `substantial association` between `the predictor and the response` due to `chance`, in the `absence` of any `real association` between `the predictor` and `the response`. \n",
    "\n",
    "Hence, if we see a `small p-value`,then we can `infer` that `there is an association` `between the predictor and the response`. \n",
    "\n",
    "We `reject` the __null hypothesis__—that is, we declare a `relationship` to `exist between` $X$ and $Y$ —if the __`p-value`__ is `small enough`. \n",
    "\n",
    "Typical __`p-value`__ cutoffs for `rejecting` the `null hypothesis` are 5 or 1 %. \n",
    "\n",
    "When $n = 30$, these correspond to __`t-statistics`__ <a href=\"#Formula3.14\">(3.14)</a> of around 2 and 2.75, `respectively`.\n",
    "\n",
    "<a id=\"Table3.1\"></a>\n",
    "\n",
    "|    &nbsp;|Coefficient |Std. Error | t-statistics | p-values |\n",
    "|----------|------------|-----------|--------------|----------|\n",
    "|Intercept | 7.0325| 0.4578| 15.36| < 0.0001|\n",
    "|Tv        | 0.0475| 0.0027| 17.67| < 0.0001|\n",
    "\n",
    ">__TABLE 3.1__. For the `Advertising data`, `coefficients` of the `least squares` model\n",
    "for the `regression` of `number of units sold` on `TV advertising budget`. \n",
    "<br>An increase of $1000 in the `TV advertising budget` is `associated with an increase in sales by\n",
    "around 50 units` (Recall that the sales variable is in thousands of units, and the\n",
    "TV variable is in thousands of dollars).\n",
    "\n",
    "Table 3.1 `provides` `details` of the `least squares model` for the `regression` of\n",
    "`number of units sold on TV advertising budget for the Advertising data`.\n",
    "\n",
    "Notice that the `coefficients` for $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ are `very large` relative to their\n",
    "`standard errors`, so the `t-statistics` are `also large`; the `probabilities of seeing\n",
    "such values` if $H_{0}$ is `true` are `virtually zero`. \n",
    "\n",
    "Hence we can conclude that $\\beta_{0} \\neq 0$and $\\beta_{1} \\neq 0$.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Assessing the Accuracy of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have `rejected` the __null hypothesis__ <a href=\"#Formula3.12\">(3.12)</a> in `favor` of the __`alternative\n",
    "hypothesis`__ <a href=\"#Formula3.13\">(3.13)</a>, it is `natural` to want to `quantify` the `extent` to which the\n",
    "`model fits the data`. \n",
    "\n",
    "The `quality` of a __`linear regression` fit__ is typically `assessed` using `two related quantities`: \n",
    "1. __the residual standard error (RSE)__ and \n",
    "2. __the $R^2$__ statistic.\n",
    "\n",
    "<a href=\"#Table3.2\">Table 3.2</a> displays \n",
    "the __`RSE`__, \n",
    "<br>the $R^2$ `statistic`, and \n",
    "<br>the __`F-statistic`__ (to be described in Section 3.2.2) \n",
    "<br>for the `linear regression` of `number of units sold on TV advertising budget`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Standard Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Video to understand RSE](https://www.youtube.com/watch?v=rLwn7OKcoqk)\n",
    "\n",
    "__Recall__ from the model <a href=\"#Formula3.5\">(3.5)</a> that `associated` with `each observation` is an\n",
    "`error term` $\\epsilon$. \n",
    "\n",
    "Due to the `presence` of these `error terms`, even if we knew the\n",
    "__`true regression line`__ (i.e. even if $\\beta_{0}$ and $\\beta_{1}$ were known), we `would not` be\n",
    "able to `perfectly predict`$Y$ from $X$. \n",
    "\n",
    "The __`RSE`__ is an `estimate` of the `standard deviation of` $\\epsilon$. \n",
    "\n",
    "<a id=\"Table3.2\"></a>\n",
    "\n",
    "|                Quantity|Value |\n",
    "|------------------------|------|\n",
    "|Residual Standard Error |  3.26|\n",
    "|$R^2$                   | 0.612|\n",
    "|F-statistic             | 312.1|\n",
    "\n",
    ">__TABLE 3.2__. For the `Advertising data`, more `information` about the `least squares model` for the `regression` of `number of units sold on TV advertising budget`.\n",
    "\n",
    "Roughly speaking, it is the `average amount` that the `response\n",
    "will deviate` from the `true regression line`. \n",
    "<a id=\"Formula3.15\"></a>\n",
    ">It is computed using the formula\n",
    "<br><br><font size=5> $ RSE = \\sqrt{\\frac{1}{n - 2} RSS} = \\sqrt{\\frac{1}{n - 2} \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^2} $ </font>\n",
    "<br><br>Note that `RSS` was `defined` in <a href=\"#3.1.1-Estimating-the-Coefficients\">Section 3.1.1</a>, and is given by the `formula`\n",
    "\n",
    "<a id=\"Formula3.16\"></a>\n",
    "><font size=5> $  RSS = \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^2 $ </font>\n",
    "\n",
    "In the `case` of the `advertising data`, we see from the `linear regression output` in <a href=\"#Table3.2\">Table 3.2</a> that the __`RSE is 3.26`__. \n",
    "\n",
    "In other words, `actual sales` in `each market` __deviate__ from the `true regression line` by `approximately 3,260\n",
    "units`, on `average`. \n",
    "\n",
    "Another way to think about this is that even if the `model were correct` and the `true values` of the `unknown coefficients` $\\beta_{0}$ and $\\beta_{1}$ were __`known exactly`__, any `prediction` of `sales on the basis of TV advertising` would still be off by `about 3,260 units on average`. \n",
    "\n",
    "Of course, `whether or not` 3,260 units is an `acceptable prediction error` __depends__ on the `problem context`. \n",
    "\n",
    "In the `advertising data set`, the `mean value` of `sales` over `all markets` is `approximately 14,000 units`, and so the `percentage error` is 3,260/14,000 = 23 %.\n",
    "\n",
    "The __`RSE`__ is considered a measure of the ___lack of fit of the model___ <a href=\"#Formula3.5\">(3.5)</a> to `the data`. \n",
    "\n",
    "If the `predictions obtained` using the `model` are `very close` to the `true outcome values`—that is, if $ \\hat{y}_{i}  = y_{i}$ for $i = 1, \\dots , n$—then <a href=\"#Formula3.15\">(3.15)</a> will `be small`, and we `can conclude` that `the model fits the data very well`. \n",
    "\n",
    "On the other hand, if $\\hat{y}_{i}$ is `very far` from $ y_{i}$ for `one or more observations`, then the __`RSE`__ may be `quite large`, indicating that the __`model doesn’t fit the data well`__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $R^2$ Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RSE` provides an `absolute` measure of the ___lack of fit of the model___ <a href=\"#Formula3.5\">(3.5)</a> to `the data`. \n",
    "\n",
    "But since it is `measured` in the `units of` $Y$ , it is not always `clear` what `constitutes a good RSE`. \n",
    "\n",
    "The $R^2$ __`statistic`__ provides an alternative measure of fit. \n",
    "\n",
    "It `takes the form` of a `proportion`—the `proportion of variance explained`—and so it always `takes` on a `value between` 0 and 1, and is `independent` of the `scale` of $Y$.\n",
    "\n",
    "To calculate $R^2$ , we use the `formula`\n",
    "\n",
    "<a id=\"Formula3.17\"></a>\n",
    "><font size=5> $ R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS} $ </font>\n",
    ">>where \n",
    "\n",
    ">>$TSS = (y_{i} − \\bar{y})^2 $ is the `total sum of squares`, and \n",
    "\n",
    ">>`RSS` is defined in <a href=\"#Formula3.16\">(3.16)</a>. \n",
    "<br><br>`TSS measures` the `total variance` in the `response` $Y$ , and can be `thought` of as the `amount of variability` inherent in the `response` before the\n",
    "`regression is performed`. \n",
    "<br><br>In contrast, `RSS measures` the `amount of variability` that is `left unexplained` after performing the `regression`. \n",
    "<br><br>Hence, $TSS − RSS$ `measures` the `amount of variability` in the `response` that is `explained` (or\n",
    "removed) by `performing the regression`, and $R^2$ `measures` the `proportion\n",
    "of variability` in $Y$ that can be `explained` using $X$.\n",
    "\n",
    "An $R^2$ `statistic` that is `close` to 1 `indicates` that a __`large proportion of the variability`__ in the response has been `explained by the regression`. \n",
    "\n",
    "A number `near` 0 `indicates` that the `regression did not explain` much of the `variability in the response`; \n",
    "\n",
    "this might occur because the `linear model is wrong`, or the `inherent error` $sigma^2$ is `high`, or `both`. \n",
    "\n",
    "In <a href=\"#Table3.2\">Table 3.2</a>, the $R^2$ was 0.61, and so just under `two-thirds of the variability` in `sales is explained by a linear regression` on `TV` .\n",
    "\n",
    "&bullet: Interpretational advantage is that, the $R^2$ will always lie between 0 to 1.\n",
    "\n",
    "***\n",
    "$R^2$ __Values below 0.1 are more `Realistic`__\n",
    "***\n",
    "\n",
    "The $R^2$ __`statistic`__ is a __`measure of the linear relationship` between $X$ and\n",
    "Y__ . \n",
    "\n",
    "Recall that __`correlation`__, defined as \n",
    "\n",
    "<a id=\"Formula3.18\"></a>\n",
    "><font size=5> $ Cor(X,Y) = \\frac{ \\sum_{i=1}^{n} (x_{i} - \\bar{x})(y_{i} - \\bar{y})}{ \\sqrt{ \\sum_{i=1}^{n} (x_{i} - \\bar{x})^2 }  \\sqrt{ \\sum_{i=1}^{n} (y_{i} - \\bar{y})^2 }}  $ </font>\n",
    "\n",
    "is also a measure of the `linear relationship` between $X$ and $Y$.\n",
    "\n",
    "This suggests that we `might` be able to `use` $r = Cor(X, Y )$ instead of $R^2$ in `order to assess` the `fit of the linear model`. \n",
    "\n",
    "In fact, it can be shown that in the `simple linear regression setting`, $R^2 = r^2$ . \n",
    "\n",
    "In other words, the `squared correlation` and the $R^2$ `statistic` are __identical__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `practice` we often have more than one `predictor`.\n",
    "\n",
    "But while praccticing more(Specially in real life scenario we have more than one `Predictor`.\n",
    "\n",
    ">For __example__, in the `Advertising data`, we have `examined` the `relationship between` __sales and TV advertising__. \n",
    "<br><br>We also have `data` for the `amount of money spent advertising` on __the radio and in newspapers__, and we may want to `know` whether `either of these two media` is `associated with sales`. \n",
    "<br><br>__How can we `extend` our `analysis of the advertising data` in `order to accommodate` these `two additional predictors`?__\n",
    "\n",
    "The approach of `fitting` a `separate simple linear regression model` for __each predictor__ is `not entirely satisfactory`. \n",
    "\n",
    ">__First__ of all, it is `unclear` how to make a `single prediction` of `sales` given levels of the three advertising media budgets, since `each of the budgets is associated` with a `separate regression equation`. \n",
    "\n",
    ">__Second__, `each of the three regression equations` ignores the `other two media` in `forming estimates` for the `regression coefficients`. We `will see shortly` that if the `media budgets` are `correlated with each other` in the `200 markets` that constitute our `data set`, then this can `lead` to `very misleading` estimates of the `individual media effects on sales`.\n",
    "\n",
    "***\n",
    "Instead of `fitting a separate simple linear regression model` for each `predictor`, a `better approach` is to `extend` the `simple linear regression model` <a href=\"#Formula3.5\">(3.5)</a> so that it can `directly accommodate multiple predictors`. \n",
    "\n",
    "We `can do` this by `giving each predictor` a __separate slope coefficient__ in a `single model`.\n",
    "***\n",
    "In general, suppose that we have $p$ `distinct predictors`. \n",
    "Then the `multiple linear regression model` takes the form\n",
    "\n",
    "<a id=\"Formula3.19\"></a>\n",
    "><font size=5> $ Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\dots + \\beta_{p} X_{p} + \\epsilon $ </font>\n",
    "\n",
    "***\n",
    "***\n",
    "<font size=3><center>Simple regression of sales on radio</center></font>\n",
    "\n",
    "|    &nbsp;|Coefficient |Std. Error | t-statistics | p-values |\n",
    "|----------|------------|-----------|--------------|----------|\n",
    "|Intercept | 9.312| 0.563| 16.54| < 0.0001|\n",
    "|Radio     | 0.203| 0.020| 9.92| < 0.0001|\n",
    "\n",
    "\n",
    "<font size=3><center>Simple regression of sales on Newspaper</center></font>\n",
    "\n",
    "|    &nbsp;|Coefficient |Std. Error | t-statistics | p-values |\n",
    "|----------|------------|-----------|--------------|----------|\n",
    "|Intercept | 12.351| 0.621| 16.54| < 0.0001|\n",
    "|Newspaper | 0.055| 0.017| 3.30|  0.00115|\n",
    "\n",
    ">__TABLE 3.3__. More `simple linear regression models` for the `Advertising data`. \n",
    "<br>`Coefficients` of the `simple linear regression model` for `number of units sold` on \n",
    "<br>__Top__: `radio advertising budget` and \n",
    "<br>__Bottom__: `newspaper advertising budget`. \n",
    "\n",
    ">A &dollar;1,000 `increase` in `spending on radio advertising` is associated with an `average increase` in\n",
    "sales by around 203 units, while the `same increase` in `spending on newspaper` advertising is associated with an `average increase` in `sales by around 55 units` (Note that the sales variable is in thousands of units, and the radio and newspaper variables are in thousands of dollars).\n",
    "***\n",
    "\n",
    "`where` $X_{j}$ represents the $jth$ `predictor` and $\\beta_{j}$ `quantifies` the `association\n",
    "between` that `variable and the response`. \n",
    "\n",
    "We __interpret__ $\\beta_{j}$ as the `average\n",
    "effect` on $Y$ of a `one unit increase` in $X_{j}$, `holding` all other `predictors fixed`.\n",
    "\n",
    "In the `advertising example`, <a href=\"#Formula3.19\">(3.19)</a> becomes\n",
    "\n",
    "<a id=\"Formula3.20\"></a>\n",
    "><font size=5> $ sales = \\beta_{0} + \\beta_{1} TV + \\beta_{2} radio + \\beta_{p} newspaper + \\epsilon $ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Estimating the Regression Coefficients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was the `case` in the `simple linear regression setting`, the __`regression coefficients`__ \n",
    "$ \\beta_{0} ,\\beta_{1}, \\beta_{2}, \\dots , \\beta_{p}$ in <a href=\"#Formula3.19\">(3.19)</a> are `unknown`, and `must be estimated`. \n",
    "\n",
    "Given estimates $ \\hat{\\beta}_{0} , \\hat{\\beta}_{1} , \\dots ,\\hat{\\beta}_{p} $ , we can make predictions using the formula\n",
    "\n",
    "><font size=5>$ \\hat{y} = \\hat{\\beta}_{0} X_{0} + \\hat{\\beta}_{1} X_{1} + \\hat{\\beta}_{1} X_{1} + \\dots + \\hat{\\beta}_{p} X_{p} $</font>\n",
    "\n",
    "The `parameters` are `estimated` using the `same least squares approach` that `we saw` in the `context` of `simple linear regression`. \n",
    "\n",
    "We `choose` $ \\beta_{0} ,\\beta_{1}, \\beta_{2}, \\dots , \\beta_{p}$ to `minimize` the `sum of squared residuals` __(RSS).__\n",
    "\n",
    "<a href=\"#Formula3.22\"></a>\n",
    "><font size=5> $  RSS = \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^2 $ </font>\n",
    ">><font size=5> $ = \\sum_{i=1}^{n} (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1} X_{i1} - \\hat{\\beta}_{2} X_{i2} -  \\dots - \\hat{\\beta}_{p} X_{ip} )^2 $ </font>\n",
    "\n",
    "<a id=\"Figure3.4\"></a>\n",
    "![image.png](Figures/Figure3.4.png)\n",
    ">__FIGURE 3.4__. In a `three-dimensional setting`, with `two predictors` and `one response`, the `least squares regression line` becomes a `plane`. \n",
    "<br>The `plane` is `chosen to minimize` the `sum of the squared vertical distances` between `each observation` (shown in red) and the `plane`.\n",
    "\n",
    "***\n",
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "\n",
    "<a href=\"#Table3.4\"><b>Table 3.4</b></a> displays the `multiple regression coefficient estimates` when __TV,\n",
    "radio, and newspaper advertising budgets are used to predict product sales\n",
    "using the Advertising data__. \n",
    "\n",
    "We `interpret` these `results` as follows: \n",
    "1. for a given `amount of TV and newspaper advertising`, `spending` an additional &dollar;1,000 on `radio advertising` leads to an __increase__ in `sales by approximately` 189 units. \n",
    "    \n",
    "    `Comparing these coefficient` estimates to those `displayed` in <a href=\"#Table3.1\">Tables 3.1</a> and <a href=\"#Table3.3\">3.3</a>, we `notice` that the `multiple regression coefficient estimates` for __TV and radio are pretty similar to the simple linear regression coefficient estimates__. \n",
    "    \n",
    "    However, while the `newspaper regression coefficient estimate` in <a href=\"#Table3.3\">Table 3.3</a> was `significantly non-zero`, the `coefficient estimate` for newspaper in the `multiple regression model` is __`close to zero`__, and the corresponding $p$__-value__ is `no longer significant`, with a `value around` 0.86. \n",
    "    \n",
    "<a id=\"Table3.4\"></a>\n",
    "\n",
    "|    &nbsp;|Coefficient |Std. Error | t-statistics | p-values |\n",
    "|----------|------------|-----------|--------------|----------|\n",
    "|Intercept | 2.939| 0.3119| 9.42| < 0.0001|\n",
    "|Tv        | 0.046| 0.0014| 32.81| < 0.0001|\n",
    "|Radio     | 0.189| 0.0086| 21.89| < 0.0001|\n",
    "|Newspaper |-0.001| 0.0059| -0.18|  0.8599|\n",
    "\n",
    ">__TABLE 3.4__. For the `Advertising data`, `least squares coefficient estimates` of the\n",
    "`multiple linear regression` of `number of units sold` on `radio`, `TV`, and `newspaper\n",
    "advertising` budgets.\n",
    "\n",
    "___Does it make sense for the multiple regression to suggest no relationship between sales and newspaper while the simple linear regression implies the opposite?___\n",
    "\n",
    "In fact it does, see the below table:\n",
    "\n",
    "<a id=\"Table3.5\"></a>\n",
    "\n",
    "|    &nbsp;|TV |radio| newspaper | sales |\n",
    "|----------|------------|-----------|--------------|----------|\n",
    "|Tv        | 1.0000| 0.0548| 0.0567| 0.7822|\n",
    "|Radio     |&nbsp;| 1.0000| 0.3541| 0.5762|\n",
    "|Newspaper |&nbsp;| &nbsp;| 1.0000|  0.2283|\n",
    "|sales | &nbsp;| &nbsp;|&nbsp;| 1.0000|\n",
    "\n",
    ">__TABLE 3.5__. `Correlation matrix` for `TV`, `radio`, `newspaper`, and `sales` for the\n",
    "`Advertising data`.\n",
    "\n",
    "[To understand Correlation matrix in deep, Visit Book __Page No 88__]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Some Important Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we perform `multiple linear regression`, we usually are `interested` in `answering` a __few important questions__.\n",
    "\n",
    "1. Is at least one of the predictors X 1 , X 2 , . . . , X p useful in predicting the response?\n",
    "2. Do all the predictors help to explain Y , or is only a subset of the predictors useful?\n",
    "3. How well does the model fit the data?\n",
    "4. Given a set of predictor values, what response value should we predict, and how accurate is our prediction?\n",
    "\n",
    "__[Must Read All the Answers Given in Book Page No 75 to 82]__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$β_{0}$ and $β_{1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
