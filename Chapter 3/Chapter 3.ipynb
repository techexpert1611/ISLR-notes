{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3 Linear Regression\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&bullet; __Linear Regression__ is useful tool for predicting a `Quantitive Response`.\n",
    "\n",
    "-> Very Simple Approach for Supervised Learning.\n",
    "\n",
    "__NOTE: It comes Under Supervised Learning Methods.]__\n",
    "\n",
    "\n",
    ">___Recall___ the `Advertising data` from <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Chapter%202.ipynb#Chapter-2:-Statastical-Learning\">Chapter 2</a>. \n",
    "<br><a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Figures/Figure2.1.png\">Figure 2.1</a> displays `sales` (in thousands of units) for a `particular product` as a function of `advertising budgets` (in thousands of dollars) __for `TV` , `radio` , and `newspaper media`__.\n",
    "<br>Suppose that in our role as `statistical consultants` we are `asked to suggest`, on the `basis of this data`, a `marketing plan for next year` that will `result in high product sales`. \n",
    "\n",
    "> ___What `information` would be `useful` in order to `provide` such a `recommendation`?___ \n",
    "\n",
    "Here are a few important questions that we might seek to address: \n",
    "\n",
    "1. __Is there a relationship between advertising budget and sales?__\n",
    "    \n",
    "    Our `first goal` should be to `determine` whether the `data provide evidence` of an `association` between `advertising expenditure and sales`.\n",
    "\n",
    "    If the `evidence is weak`, then one might argue that `no money should be spent on advertising`!\n",
    "\n",
    "\n",
    "2. __How strong is the relationship between advertising budget and sales?__\n",
    "    \n",
    "    Assuming that there is a `relationship between advertising and sales`, we would like to `know the strength` of this `relationship`. \n",
    "    \n",
    "    In `other words`, given a certain `advertising budget`, can we `predict sales` with a `high level` of `accuracy`? This would be a `strong relationship`. \n",
    "    \n",
    "    Or is a `prediction` of `sales based` on `advertising expenditure only` slightly `better` than a `random guess`? \n",
    "    \n",
    "    This would be a `weak relationship`.\n",
    "    \n",
    "    \n",
    "3. __Which media contribute to sales?__\n",
    "\n",
    "    Do `all three` __media—TV, radio, and newspaper—contribute to sales__, or do `just one or two` of the `media contribute`? \n",
    "    \n",
    "    To answer this `question`, we `must find` a `way` to `separate out` the `individual effects` of `each medium` when we have `spent money` on `all three media`.\n",
    "\n",
    "\n",
    "4. __How accurately can we estimate the effect of each medium on sales?__\n",
    "\n",
    "    For `every dollar spent` on `advertising` in a `particular medium`, by `what amount` will `sales increase`? \n",
    "    \n",
    "    How `accurately` can `we predict` this `amount of increase`?\n",
    "\n",
    "\n",
    "5. __How accurately can we predict future sales?__\n",
    "\n",
    "    For any given level of `television, radio, or newspaper advertising`, what is `our prediction` for `sales`, and what is the `accuracy` of `this prediction`?\n",
    "\n",
    "\n",
    "6. __Is the relationship linear?__\n",
    "\n",
    "    If there is `approximately` a `straight-line relationship` between `advertising expenditure` in the `various media` and `sales`, then `linear regression` is an `appropriate tool`. \n",
    "    \n",
    "    If not, then it may still be possible to `transform` the `predictor or the response` so that `linear regression` can be used.\n",
    "\n",
    "\n",
    "7. __Is there synergy among the advertising media?__\n",
    "\n",
    "    Perhaps `spending` $50,000$ on `television advertising` and $50,000$ on `radio advertising` `results` in `more sales` than `allocating` $100,000$ to `either television` or `radio individually`. \n",
    "    \n",
    "    In `marketing`, this is `known` as a __synergy effect__, while in `statistics` it is `called` an __interaction effect__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Simple linear regression`__ lives up to its name: it is a very `straightforward`\n",
    "approach for `predicting` a __quantitative response__ $Y$ on the basis of a `single predictor variable` $X$. \n",
    "\n",
    "It assumes that there is `approximately` a __`linear relationship between X and Y`__. \n",
    "\n",
    "Mathematically, we can write this `linear relationship` as:\n",
    "\n",
    "<a id=\"Formula3.1\"></a>\n",
    "<font size=5>$ Y \\approx \\beta_{0}+\\beta_{1}X$</font>\n",
    "\n",
    "You might read “$\\approx$” as _“is approximately modeled as”_.\n",
    "\n",
    ">For example, \n",
    "<br>$X$ may represent `TV advertising` and \n",
    "<br>$Y$ may represent `sales` .\n",
    "<br>Then we can `regress sales onto TV` by `fitting the model`\n",
    "<br><br>$ sales \\approx \\beta_{0}+\\beta_{1} TV$\n",
    "\n",
    "In <a href=\"#Formula3.1\">Equation 3.1</a>, $β_{0}$ and $β_{1}$ are `two unknown constants` that represent\n",
    "the __intercept__ and __slope__ terms in the `linear model`. \n",
    "\n",
    "Together, $β_{0}$ and $β_{1}$ are `known` as the `model` __coefficients or parameters__. \n",
    "\n",
    "Once we have used our `training data` to `produce estimates` $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ for the `model coefficients`, we\n",
    "can `predict future sales` on the basis of a `particular value of TV advertising` by computing\n",
    "\n",
    "<a id=\"Formula3.2\"></a>\n",
    ">$ \\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x$\n",
    ">>where $\\hat{y}$ indicates a `prediction of` $Y$ on the basis of $X = x$.\n",
    "<br>Here we use a dash symbol, $'$ , to `denote` the `estimated value` for an `unknown` __parameter or coefficient__, or to `denote` the `predicted value` of the `response`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Estimating the Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `practice`, $β_{0}$ and $β_{1}$ are `unknown`. \n",
    "\n",
    "So before we can use <a href=\"#Formula3.1\">(3.1)</a> to make `predictions`, we must `use data` to `estimate` the `coefficients`.\n",
    "\n",
    "$(x_{1} , y_{1} ), (x_{2} , y_{2} ), \\dots , (x_{n} , y_{n} )$\n",
    "\n",
    "\n",
    "Let `represent` $n$ `observation pairs`, each of which consists of a `measurement of` $X$ and a `measurement of` $Y$ . \n",
    "\n",
    ">In the __`Advertising` example__, \n",
    "<br>this `data set` consists of the `TV advertising budget` and `product sales` in n = 200 `different markets`.(Recall that the data are displayed in <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Figures/Figure2.1.png\">Figure 2.1</a>.) \n",
    "<br><br>__Our `goal`__ is to `obtain coefficient estimates` $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ such that the `linear model` <a href=\"#Formula3.1\">(3.1)</a> `fits` the available `data well-that` is, so that $y_{i} \\approx \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i}$ for $i = 1,\\dots , n$.\n",
    "<br><br>In `other words`, we `want` to `find` an __`intercept`__ $\\hat{\\beta}_{0}$ and a __`slope`__ $\\hat{\\beta}_{1}$ such that the `resulting line` is as `close` as `possible` to the $n = 200$ `data points`. \n",
    "\n",
    "\n",
    "&bullet; There are a `number of ways` of `measuring closeness`. \n",
    "\n",
    "However, by far the `most common approach` involves `minimizing` the __`least squares`__ `criterion`, and we take that approach for us in this sessions.\n",
    "\n",
    "\n",
    "<a id=\"Figure3.1\"></a>\n",
    "![image.png](Figures/Figure3.1.png)\n",
    ">__`FIGURE 3.1`.__ For the `Advertising data`, the __`least squares` fit__ for the `regression` of `sales onto TV` is shown. \n",
    "<br>The `fit` is `found` by `minimizing the sum` of `squared errors`. Each `grey line segment` represents an `error`, and the `fit makes` a `compromise` by `averaging their squares`. \n",
    "<br>In this `case` a `linear fit captures` the essence of\n",
    "the `relationship`, although it is somewhat `deficient` in `the left of the plot`.\n",
    "\n",
    "\n",
    "Let $\\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} \\hat{x}_{i}$ be the `prediction` for $Y$ based on the $ith$ value of $X$.\n",
    "\n",
    "Then $e_{i} = y_{i} − \\hat{y}_{i}$ represents the $ith$ __`residual`__—this is the `difference between` the $ith$ `observed response value` and the $ith$  `response value` that is `predicted by our linear model`. \n",
    "\n",
    "We define the __residual sum of squares (RSS)__ as\n",
    "\n",
    "<font size=\"5\"> $ RSS = e_{1}^2+e_{2}^2+\\dots+e_{n}^2$</font>\n",
    "\n",
    "or equivalently as\n",
    "\n",
    "<font size=\"5\"> $ RSS = (y_{1} − \\hat{\\beta}_{0} − \\hat{\\beta}_{1} x_{1})^2 + (y_{2} − \\hat{\\beta}_{0} − \\hat{\\beta}_{1} x_{2})^2+ \\dots+ (y_{n} − \\hat{\\beta}_{0} − \\hat{\\beta}_{1} x_{n})^2$</font>\n",
    "\n",
    "\n",
    ">The __`least squares` approach__ chooses $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ to `minimize the RSS`. \n",
    "<br>Using some calculus, one can show that the minimizers are\n",
    "\n",
    "<a id=\"Formula3.4\"></a>\n",
    "><font size=\"5\">\n",
    "$ \\hat{\\beta}_{1}$ = $\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x}) (y_{i}-\\bar{y})}{\\sum_{i=1}^{n} (x_{i} - \\bar{x})^2} $\n",
    "<br>$ \\hat{\\beta}_{0} = \\bar{y} − \\hat{\\beta}_{1} \\bar{x}$\n",
    "</font>\n",
    "\n",
    ">where <br><font size=4>$ \\bar{y} \\equiv \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$</font> and <font size=>$ \\bar{x} \\equiv \\frac{1}{n} \\sum_{i=1}^{n} x_{i}$</font> are the `sample means`. \n",
    "<br>In other words, <a href=\"#Formula3.4\">(3.4)</a> defines the `least squares coefficient` estimates for `simple linear regression`.\n",
    "\n",
    "<a href=\"#Figure3.1\">Figure 3.1</a> `displays` the `simple linear regression fit` to the `Advertising data`, where $\\hat{\\beta}_{0} = 7.03$ and $\\hat{\\beta}_{1} = 0.0475$. \n",
    "\n",
    "In other words, according to this `approximation`, an `additional` $1,000$ `spent` on `TV advertising` is `associated with selling` approximately 47.5 additional `units of the product`. \n",
    "\n",
    "<a id=\"Figure3.2\"></a>\n",
    "![image.png](Figures/Figure3.2.png)\n",
    "\n",
    "\n",
    "In <a href=\"#Figure3.2\">Figure 3.2</a>, we have `computed RSS` for a `number of values` of $\\beta_{0} and \\beta_{1}$ , using the `advertising data with sales` as the `response` and `TV` as the `predictor`. \n",
    "\n",
    "In each `plot`, the `red dot` represents the `pair of least squares estimates` $ \\hat{\\beta}_{0}, \\hat{\\beta}_{1}$ given by <a href=\"#Formula3.4\">(3.4)</a>. \n",
    "\n",
    "These values `clearly minimize` the __`RSS`__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Assessing the Accuracy of the Coefficient Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from <a href=\"#Formula2.1\">(2.1)</a> that we `assume` that the `true relationship between X and Y` takes the `form` $Y = f (X) + \\epsilon$ for some `unknown function` $f$ , \n",
    ">where \n",
    "<br>$\\epsilon$ is a `mean-zero random error` term. \n",
    "<br>If $f$ is to be `approximated` by a `linear function`, then we can `write` this `relationship` as\n",
    "<br><br><a id=\"Formula3.5\"></a>\n",
    "<font size=4>$Y = \\beta_{0} + \\beta_{1} X + \\epsilon$.</font>\n",
    "\n",
    ">Here $\\beta_{0}$ is the \n",
    "<br>__intercept__ term—that is, __the expected value of $Y$ when $X = 0$, \n",
    "\n",
    ">and $\\beta_{1}$ is the \n",
    "<br>__slope__—__the average increase in $Y$ associated with a one-unit increase in $X$. \n",
    "\n",
    ">The __error term__ is a `catch-all` for what we miss with this simple model: \n",
    "<br>the true relationship is probably not linear, there may be other variables that cause variation in $Y$ , and there may be measurement error. \n",
    "<br>We typically assume that the error term is independent of $X$.\n",
    "\n",
    "***\n",
    "The `model` given by <a href=\"#Formula3.5\">(3.5)</a> defines the `population regression line`, which is the `best linear approximation` to the `true relationship between` $X$ and $Y$ . \n",
    "\n",
    "The `least squares regression coefficient estimates` The `model` given by <a href=\"#Formula3.4\">(3.4)</a> `characterize` the `least squares` line The `model` given by <a href=\"#Formula3.2\">(3.2)</a>. \n",
    "\n",
    "<a id=\"Figure3.3\"></a>\n",
    "![image.png](Figures/Figure3.3.png)\n",
    ">__FIGURE 3.3__. A `simulated data set`. \n",
    "\n",
    ">__Left__: The `red line` represents the `true relationship`, $f (X) = 2 + 3X$, which is `known` as the `population regression line`. \n",
    "<br.The `blue line` is the `least squares line`; it is the `least squares estimate` for $f (X)$ based\n",
    "on the `observed data`, shown in `black`. \n",
    "\n",
    ">__Right__: The `population regression line` is\n",
    "again `shown in red`, and the `least squares line` in `dark blue`. \n",
    "<br>In `light blue`, ten `least squares lines` are shown, `each computed` on the basis of a `separate random set` of `observations`. \n",
    "\n",
    "Each `least squares line` is `different`, but `on average`, the `least squares lines` are `quite close` to the `population regression line`\n",
    "\n",
    "The __left-hand__ panel of The `model` given by <a href=\"#Figure3.3\">Figure 3.3</a> displays these two lines in a simple simulated example.\n",
    "\n",
    "We created 100 random $X_{s}$, and generated 100 corresponding $Y_{s}$ from the model.\n",
    "\n",
    "<font size=4>$Y = 2 + 3X + \\epsilon$</font>\n",
    "\n",
    "Where $\\epsilon$ was generated from a `normal distribution` with `mean zero`.\n",
    "\n",
    "The `red line` in the `left-hand panel` of <a href=\"#Figure3.3\">Figure 3.3</a> displays the `true relationship`,\n",
    "$f (X) = 2 + 3X $, while the `blue line` is the `least squares estimate` based on the `observed data`. \n",
    "\n",
    "The `true relationship` is `generally not known` for `real data`, but the `least squares line` can always be `computed using` the `coefficient estimates` given in <a href=\"#Formula3.4\">(3.4)</a>\n",
    "\n",
    "***\n",
    "***\n",
    "The `analogy` between `linear regression` and `estimation of the mean` of a `random variable` is an apt one based on the concept of __bias__. \n",
    "\n",
    "If we use the `sample mean` $\\hat{\\mu}$ to estimate $\\mu$, this `estimate` is __unbiased__, in the sense that\n",
    "`on average`, we `expect` $\\hat{\\mu}$ to equal $\\mu$.\n",
    "\n",
    "__What exactly does this mean?__\n",
    "\n",
    "[ Visit __Page No : 65 of ISLR BOOK___]\n",
    "***\n",
    "\n",
    "\n",
    "***\n",
    "We `continue` the `analogy` with the `estimation` of the `population mean` $\\mu$ of a `random variable` $Y$ .\n",
    "\n",
    "A `natural question` is as follows: __how accurate is the sample mean $\\hat{\\mu}$ as an estimate of μ?__\n",
    "\n",
    "We have established that the `average` of $\\hat{\\mu}$'s `over many data sets` will be `very close` to μ, but that a\n",
    "`single estimate` $\\hat{\\mu}$ may be a `substantial underestimate` or `overestimate` of $\\mu$.\n",
    "\n",
    "__How `far off` will that `single estimate` of $\\hat{\\mu}$be? __\n",
    "\n",
    "In general, we answer this `question` by `computing the standard error` of $\\hat{\\mu}$, written as $SE(\\hat{\\mu})$. \n",
    "\n",
    "We have the well-known formula\n",
    "\n",
    "<a id=\"Formula3.7\"></a>\n",
    "<font size=5> $Var(\\hat{\\mu}) = SE(\\hat{\\mu})^2 = \\frac{\\sigma^2}{n}$ </font>\n",
    ">where $\\sigma$ is the __standard deviation__ of each of the `realizations` $y_{i}$ of $Y^2$.\n",
    "***\n",
    "\n",
    "***\n",
    "`Roughly speaking`, the __standard error__ tells us the `average amount` that this\n",
    "`estimate` $\\hat{\\mu}$ `differs` from the `actual value` of $\\mu$.\n",
    "***\n",
    "\n",
    "***\n",
    "<a href=\"#Formula3.7\">Equation 3.7</a> also tells us __`how this deviation shrinks with n`__—the `more observations` we have, the `smaller the standard error` of $\\hat{\\mu}$\n",
    "\n",
    "In a `similar vein`, we can `wonder` how `close` \\hat{\\beta}_{0}\n",
    "and \\hat{\\beta}_{1} are to the `true values` $\\beta_{0}$ and $\\beta_{1}$ .\n",
    "\n",
    "To `compute` the __`standard errors`__ associated with $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ , we use the following formulas:\n",
    "\n",
    "\n",
    "<a id=\"Formula3.8\"></a>\n",
    "<font size=5> $ SE ( \\hat{\\beta}_{0})^2 = \\sigma^2 [ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i = 1}^{n} ( x_{i} - \\bar{x}_{i})^2} ], SE ( \\hat{\\beta}_{1})^2 = \\sigma^2 [ \\frac{1}{n} + \\frac{\\sigma^2}{\\sum_{i = 1}^{n} ( x_{i} - \\bar{x}_{i})^2} ]$ </font>\n",
    ">where $\\sigma^2 = Var(\\epsilon)$\n",
    "\n",
    "[Visit __Book Page No 66__]\n",
    "\n",
    "In general, $\\sigma^2$ is `not known`, but `can be estimated` from the `data`. \n",
    "\n",
    "The estimate of $\\sigma$ is `known` as the __residual standard error__, and is given by the formula\n",
    "\n",
    "<font size=5> $ RSE = \\sqrt{ \\frac{RSS}{(n - 2)}}$</font>\n",
    "\n",
    "***\n",
    "`Standard errors` can be used to `compute confidence intervals`. \n",
    "\n",
    "A $95\\%$ `confidence interval` is `defined` as a `range of values` such that `with` $95 \\%$ `probability`, the `range` will contain the `true unknown value of the parameter`.\n",
    "\n",
    "The `range` is `defined` in terms of `lower and upper limits` computed from the\n",
    "`sample of data`. \n",
    "\n",
    "For `linear regression`, the $95 \\%$ `confidence interval` for $\\beta_{1}$ approximately takes the form\n",
    "\n",
    "<a id=\"Formula3.9\"></a>\n",
    "<font size=4> $ \\hat{\\beta}_{1} \\pm 2 $ &sdot; $SE (\\hat{\\beta}_{1}) $ </font>\n",
    "\n",
    "That is, there is approximately a 95 % chance that the interval will contain the true value of $\\beta_{1}$\n",
    "\n",
    "<a id=\"Formula3.10\"></a>\n",
    "<font size=4> [$ \\hat{\\beta}_{1} - 2 $&sdot;$ SE (\\hat{\\beta}_{1}) $,$ \\hat{\\beta}_{1} + 2 $ &sdot; $SE (\\hat{\\beta}_{1}) $ ]</font>\n",
    "\n",
    "Similarly, a `confidence interval` for $\\beta_{0}$ approximately takes the form\n",
    "\n",
    "<a id=\"Formula3.11\"></a>\n",
    "<font size=4> $ \\hat{\\beta}_{0} \\pm 2 $ &sdot; $SE (\\hat{\\beta}_{0}) $ </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the __advertising data__, \n",
    "<br>the $95 \\%$ `confidence interval` for $\\beta_{0}$ is [6.130, 7.935] and \n",
    "<br>the $95\\% $ `confidence interval` for $\\beta_{1}$ is [0.042, 0.053].\n",
    "\n",
    "Therefore, we can `conclude` that in the `absence` of any `advertising`, `sales` will, on `average`, `fall somewhere` between 6,130 and 7,940 units. \n",
    "\n",
    "Furthermore,for each $1,000$USD `increase` in `television advertising`, there will be an `average increase` in sales of between 42 and 53 units.\n",
    "\n",
    "***\n",
    "`Standard errors` can also be used to `perform` __hypothesis tests__ on the `coefficients`. \n",
    "\n",
    "The `most common hypothesis test` involves `testing the null hypothesis` of\n",
    "\n",
    "<a id=\"Formula3.12\"></a>\n",
    "<font size=3> $H_{0} $: There is no relationship between $X$ and $Y$</font>\n",
    "\n",
    "versus the alternative hypothesis\n",
    "\n",
    "<a id=\"Formula3.13\"></a>\n",
    "<font size=3> $H_{a} $: There is some relationship between $X$ and $Y$</font>\n",
    "\n",
    "\n",
    "Mathematically, this corresponds to `testing`\n",
    "\n",
    "<font size=3> $H_{0} : \\beta_{1} = 0$</font>\n",
    "\n",
    "Versus \n",
    "\n",
    "<font size=3> $H_{0} : \\beta_{1} \\neq 0$</font>\n",
    "\n",
    "since if $\\beta_{1} = 0$ then the model <a href=\"#Formula3.5\">(3.5)</a> reduces to $Y = \\beta_{0} + \\epsilon$, and $X$ is not `associated` with $Y$ . \n",
    "\n",
    "To `test` the `null hypothesis`, we need to `determine` whether $\\hat{\\beta}_{1}$ , our estimate for $\\beta_{1}$, is `sufficiently far` from `zero` that we can be `confident` that $\\beta_{1}$ is `non-zero`.\n",
    "\n",
    "___How far is far enough?___\n",
    "This of course `depends on` the `accuracy` of $\\hat{\\beta}_{1}$ —that is, it `depends` on $SE( \\hat{\\beta}_{1} )$. If $SE( \\hat{\\beta}_{1} )$ is `small`, then `even relatively small values` of $\\hat{\\beta}_{1}$ may `provide strong evidence` that $\\beta_{1} \\neq 0$, and hence that `there` is a `relationship between `$X$ and $Y$ . \n",
    "\n",
    "***\n",
    "In contrast, if $SE(  \\hat{\\beta}_{1}  ) $is `large`, then $ \\hat{\\beta}_{1} $ `must be large` in `absolute value` in order for us to `reject the null hypothesis`. \n",
    "\n",
    "In practice, we compute a __t-statistic__, given by\n",
    "<a id=\"Formula3.14\"></a>\n",
    "><font size=5> $ t = \\frac{ \\hat{\\beta}_{1} - 0}{SE(  \\hat{\\beta}_{1}  )} $ </font>\n",
    "\n",
    "which `measures` the `number of standard deviations` that $\\hat{\\beta}_{1}$ is `away` from\n",
    "0. \n",
    "\n",
    "If there `really` is `no relationship between` $X$ and $Y$ , then `we expect` that <a href=\"#Formula3.14\">(3.14)</a> will have a `t-distribution` with $n − 2$ `degrees of freedom`. \n",
    "\n",
    "The `t-distribution` has a `bell shape` and for `values of n` greater than approximately 30 it is `quite similar` to the `normal distribution`. \n",
    "\n",
    "Consequently, it is a `simple matter` to `compute the probability` of `observing` any `number equal` to $|t|$ or `larger in absolute value`, assuming $\\beta_{1}$. We call this `probability` the __`p-value`__.\n",
    "\n",
    "Roughly speaking, we `interpret` the __`p-value`__ as follows: a `small p-value` indicates that it is `unlikely` to observe such a `substantial association` between `the predictor and the response` due to `chance`, in the `absence` of any `real association` between `the predictor` and `the response`. \n",
    "\n",
    "Hence, if we see a `small p-value`,then we can `infer` that `there is an association` `between the predictor and the response`. \n",
    "\n",
    "We `reject` the __null hypothesis__—that is, we declare a `relationship` to `exist between` $X$ and $Y$ —if the __`p-value`__ is `small enough`. \n",
    "\n",
    "Typical __`p-value`__ cutoffs for `rejecting` the `null hypothesis` are 5 or 1 %. \n",
    "\n",
    "When $n = 30$, these correspond to __`t-statistics`__ <a href=\"#Formula3.14\">(3.14)</a> of around 2 and 2.75, `respectively`.\n",
    "\n",
    "<a id=\"Table3.1\"></a>\n",
    "\n",
    "|    &nbsp;|Coefficient |Std. Error | t-statistics | p-values |\n",
    "|----------|------------|-----------|--------------|----------|\n",
    "|Intercept | 7.0325| 0.4578| 15.36| < 0.0001|\n",
    "|Tv        | 0.0475| 0.0027| 17.67| < 0.0001|\n",
    "\n",
    ">__TABLE 3.1__. For the `Advertising data`, `coefficients` of the `least squares` model\n",
    "for the `regression` of `number of units sold` on `TV advertising budget`. \n",
    "<br>An increase of $1000 in the `TV advertising budget` is `associated with an increase in sales by\n",
    "around 50 units` (Recall that the sales variable is in thousands of units, and the\n",
    "TV variable is in thousands of dollars).\n",
    "\n",
    "Table 3.1 `provides` `details` of the `least squares model` for the `regression` of\n",
    "`number of units sold on TV advertising budget for the Advertising data`.\n",
    "\n",
    "Notice that the `coefficients` for $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ are `very large` relative to their\n",
    "`standard errors`, so the `t-statistics` are `also large`; the `probabilities of seeing\n",
    "such values` if $H_{0}$ is `true` are `virtually zero`. \n",
    "\n",
    "Hence we can conclude that $\\beta_{0} \\neq 0$and $\\beta_{1} \\neq 0$.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Assessing the Accuracy of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have `rejected` the __null hypothesis__ <a href=\"#Formula3.12\">(3.12)</a> in `favor` of the __`alternative\n",
    "hypothesis`__ <a href=\"#Formula3.13\">(3.13)</a>, it is `natural` to want to `quantify` the `extent` to which the\n",
    "`model fits the data`. \n",
    "\n",
    "The `quality` of a __`linear regression` fit__ is typically `assessed` using `two related quantities`: \n",
    "1. __the residual standard error (RSE)__ and \n",
    "2. __the $R^2$__ statistic.\n",
    "\n",
    "<a href=\"#Table3.2\">Table 3.2</a> displays \n",
    "the __`RSE`__, \n",
    "<br>the $R^2$ `statistic`, and \n",
    "<br>the __`F-statistic`__ (to be described in Section 3.2.2) \n",
    "<br>for the `linear regression` of `number of units sold on TV advertising budget`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Standard Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Video to understand RSE](https://www.youtube.com/watch?v=rLwn7OKcoqk)\n",
    "\n",
    "__Recall__ from the model <a href=\"#Formula3.5\">(3.5)</a> that `associated` with `each observation` is an\n",
    "`error term` $\\epsilon$. \n",
    "\n",
    "Due to the `presence` of these `error terms`, even if we knew the\n",
    "__`true regression line`__ (i.e. even if $\\beta_{0}$ and $\\beta_{1}$ were known), we `would not` be\n",
    "able to `perfectly predict`$Y$ from $X$. \n",
    "\n",
    "The __`RSE`__ is an `estimate` of the `standard deviation of` $\\epsilon$. \n",
    "\n",
    "<a id=\"Table3.2\"></a>\n",
    "\n",
    "|                Quantity|Value |\n",
    "|------------------------|------|\n",
    "|Residual Standard Error |  3.26|\n",
    "|$R^2$                   | 0.612|\n",
    "|F-statistic             | 312.1|\n",
    "\n",
    ">__TABLE 3.2__. For the `Advertising data`, more `information` about the `least squares model` for the `regression` of `number of units sold on TV advertising budget`.\n",
    "\n",
    "Roughly speaking, it is the `average amount` that the `response\n",
    "will deviate` from the `true regression line`. \n",
    "<a id=\"Formula3.15\"></a>\n",
    ">It is computed using the formula\n",
    "<br><br><font size=5> $ RSE = \\sqrt{\\frac{1}{n - 2} RSS} = \\sqrt{\\frac{1}{n - 2} \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^2} $ </font>\n",
    "<br><br>Note that `RSS` was `defined` in <a href=\"#3.1.1-Estimating-the-Coefficients\">Section 3.1.1</a>, and is given by the `formula`\n",
    "\n",
    "<a id=\"Formula3.16\"></a>\n",
    "><font size=5> $  RSS = \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^2 $ </font>\n",
    "\n",
    "In the `case` of the `advertising data`, we see from the `linear regression output` in <a href=\"#Table3.2\">Table 3.2</a> that the __`RSE is 3.26`__. \n",
    "\n",
    "In other words, `actual sales` in `each market` __deviate__ from the `true regression line` by `approximately 3,260\n",
    "units`, on `average`. \n",
    "\n",
    "Another way to think about this is that even if the `model were correct` and the `true values` of the `unknown coefficients` $\\beta_{0}$ and $\\beta_{1}$ were __`known exactly`__, any `prediction` of `sales on the basis of TV advertising` would still be off by `about 3,260 units on average`. \n",
    "\n",
    "Of course, `whether or not` 3,260 units is an `acceptable prediction error` __depends__ on the `problem context`. \n",
    "\n",
    "In the `advertising data set`, the `mean value` of `sales` over `all markets` is `approximately 14,000 units`, and so the `percentage error` is 3,260/14,000 = 23 %.\n",
    "\n",
    "The __`RSE`__ is considered a measure of the ___lack of fit of the model___ <a href=\"#Formula3.5\">(3.5)</a> to `the data`. \n",
    "\n",
    "If the `predictions obtained` using the `model` are `very close` to the `true outcome values`—that is, if $ \\hat{y}_{i}  = y_{i}$ for $i = 1, \\dots , n$—then <a href=\"#Formula3.15\">(3.15)</a> will `be small`, and we `can conclude` that `the model fits the data very well`. \n",
    "\n",
    "On the other hand, if $\\hat{y}_{i}$ is `very far` from $ y_{i}$ for `one or more observations`, then the __`RSE`__ may be `quite large`, indicating that the __`model doesn’t fit the data well`__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $R^2$ Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RSE` provides an `absolute` measure of the ___lack of fit of the model___ <a href=\"#Formula3.5\">(3.5)</a> to `the data`. \n",
    "\n",
    "But since it is `measured` in the `units of` $Y$ , it is not always `clear` what `constitutes a good RSE`. \n",
    "\n",
    "The $R^2$ __`statistic`__ provides an alternative measure of fit. \n",
    "\n",
    "It `takes the form` of a `proportion`—the `proportion of variance explained`—and so it always `takes` on a `value between` 0 and 1, and is `independent` of the `scale` of $Y$.\n",
    "\n",
    "To calculate $R^2$ , we use the `formula`\n",
    "\n",
    "<a id=\"Formula3.17\"></a>\n",
    "><font size=5> $ R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS} $ </font>\n",
    ">>where \n",
    "\n",
    ">>$TSS = (y_{i} − \\bar{y})^2 $ is the `total sum of squares`, and \n",
    "\n",
    ">>`RSS` is defined in <a href=\"#Formula3.16\">(3.16)</a>. \n",
    "<br><br>`TSS measures` the `total variance` in the `response` $Y$ , and can be `thought` of as the `amount of variability` inherent in the `response` before the\n",
    "`regression is performed`. \n",
    "<br><br>In contrast, `RSS measures` the `amount of variability` that is `left unexplained` after performing the `regression`. \n",
    "<br><br>Hence, $TSS − RSS$ `measures` the `amount of variability` in the `response` that is `explained` (or\n",
    "removed) by `performing the regression`, and $R^2$ `measures` the `proportion\n",
    "of variability` in $Y$ that can be `explained` using $X$.\n",
    "\n",
    "An $R^2$ `statistic` that is `close` to 1 `indicates` that a __`large proportion of the variability`__ in the response has been `explained by the regression`. \n",
    "\n",
    "A number `near` 0 `indicates` that the `regression did not explain` much of the `variability in the response`; \n",
    "\n",
    "this might occur because the `linear model is wrong`, or the `inherent error` $sigma^2$ is `high`, or `both`. \n",
    "\n",
    "In <a href=\"#Table3.2\">Table 3.2</a>, the $R^2$ was 0.61, and so just under `two-thirds of the variability` in `sales is explained by a linear regression` on `TV` .\n",
    "\n",
    "&bullet: Interpretational advantage is that, the $R^2$ will always lie between 0 to 1.\n",
    "\n",
    "***\n",
    "$R^2$ __Values below 0.1 are more `Realistic`__\n",
    "***\n",
    "\n",
    "The $R^2$ __`statistic`__ is a __`measure of the linear relationship` between $X$ and\n",
    "Y__ . \n",
    "\n",
    "Recall that __`correlation`__, defined as \n",
    "\n",
    "<a id=\"Formula3.18\"></a>\n",
    "><font size=5> $ Cor(X,Y) = \\frac{ \\sum_{i=1}^{n} (x_{i} - \\bar{x})(y_{i} - \\bar{y})}{ \\sqrt{ \\sum_{i=1}^{n} (x_{i} - \\bar{x})^2 }  \\sqrt{ \\sum_{i=1}^{n} (y_{i} - \\bar{y})^2 }}  $ </font>\n",
    "\n",
    "is also a measure of the `linear relationship` between $X$ and $Y$.\n",
    "\n",
    "This suggests that we `might` be able to `use` $r = Cor(X, Y )$ instead of $R^2$ in `order to assess` the `fit of the linear model`. \n",
    "\n",
    "In fact, it can be shown that in the `simple linear regression setting`, $R^2 = r^2$ . \n",
    "\n",
    "In other words, the `squared correlation` and the $R^2$ `statistic` are __identical__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `practice` we often have more than one `predictor`.\n",
    "\n",
    "But while praccticing more(Specially in real life scenario we have more than one `Predictor`.\n",
    "\n",
    ">For __example__, in the `Advertising data`, we have `examined` the `relationship between` __sales and TV advertising__. \n",
    "<br><br>We also have `data` for the `amount of money spent advertising` on __the radio and in newspapers__, and we may want to `know` whether `either of these two media` is `associated with sales`. \n",
    "<br><br>__How can we `extend` our `analysis of the advertising data` in `order to accommodate` these `two additional predictors`?__\n",
    "\n",
    "The approach of `fitting` a `separate simple linear regression model` for __each predictor__ is `not entirely satisfactory`. \n",
    "\n",
    ">__First__ of all, it is `unclear` how to make a `single prediction` of `sales` given levels of the three advertising media budgets, since `each of the budgets is associated` with a `separate regression equation`. \n",
    "\n",
    ">__Second__, `each of the three regression equations` ignores the `other two media` in `forming estimates` for the `regression coefficients`. We `will see shortly` that if the `media budgets` are `correlated with each other` in the `200 markets` that constitute our `data set`, then this can `lead` to `very misleading` estimates of the `individual media effects on sales`.\n",
    "\n",
    "***\n",
    "Instead of `fitting a separate simple linear regression model` for each `predictor`, a `better approach` is to `extend` the `simple linear regression model` <a href=\"#Formula3.5\">(3.5)</a> so that it can `directly accommodate multiple predictors`. \n",
    "\n",
    "We `can do` this by `giving each predictor` a __separate slope coefficient__ in a `single model`.\n",
    "***\n",
    "In general, suppose that we have $p$ `distinct predictors`. \n",
    "Then the `multiple linear regression model` takes the form\n",
    "\n",
    "<a id=\"Formula3.19\"></a>\n",
    "><font size=5> $ Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\dots + \\beta_{p} X_{p} + \\epsilon $ </font>\n",
    "\n",
    "***\n",
    "***\n",
    "<font size=3><center>Simple regression of sales on radio</center></font>\n",
    "\n",
    "|    &nbsp;|Coefficient |Std. Error | t-statistics | p-values |\n",
    "|----------|------------|-----------|--------------|----------|\n",
    "|Intercept | 9.312| 0.563| 16.54| < 0.0001|\n",
    "|Radio     | 0.203| 0.020| 9.92| < 0.0001|\n",
    "\n",
    "\n",
    "<font size=3><center>Simple regression of sales on Newspaper</center></font>\n",
    "\n",
    "|    &nbsp;|Coefficient |Std. Error | t-statistics | p-values |\n",
    "|----------|------------|-----------|--------------|----------|\n",
    "|Intercept | 12.351| 0.621| 16.54| < 0.0001|\n",
    "|Newspaper | 0.055| 0.017| 3.30|  0.00115|\n",
    "\n",
    ">__TABLE 3.3__. More `simple linear regression models` for the `Advertising data`. \n",
    "<br>`Coefficients` of the `simple linear regression model` for `number of units sold` on \n",
    "<br>__Top__: `radio advertising budget` and \n",
    "<br>__Bottom__: `newspaper advertising budget`. \n",
    "\n",
    ">A &dollar;1,000 `increase` in `spending on radio advertising` is associated with an `average increase` in\n",
    "sales by around 203 units, while the `same increase` in `spending on newspaper` advertising is associated with an `average increase` in `sales by around 55 units` (Note that the sales variable is in thousands of units, and the radio and newspaper variables are in thousands of dollars).\n",
    "***\n",
    "\n",
    "`where` $X_{j}$ represents the $jth$ `predictor` and $\\beta_{j}$ `quantifies` the `association\n",
    "between` that `variable and the response`. \n",
    "\n",
    "We __interpret__ $\\beta_{j}$ as the `average\n",
    "effect` on $Y$ of a `one unit increase` in $X_{j}$, `holding` all other `predictors fixed`.\n",
    "\n",
    "In the `advertising example`, <a href=\"#Formula3.19\">(3.19)</a> becomes\n",
    "\n",
    "<a id=\"Formula3.20\"></a>\n",
    "><font size=5> $ sales = \\beta_{0} + \\beta_{1} TV + \\beta_{2} radio + \\beta_{p} newspaper + \\epsilon $ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Estimating the Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was the `case` in the `simple linear regression setting`, the __`regression coefficients`__ \n",
    "$ \\beta_{0} ,\\beta_{1}, \\beta_{2}, \\dots , \\beta_{p}$ in <a href=\"#Formula3.19\">(3.19)</a> are `unknown`, and `must be estimated`. \n",
    "\n",
    "Given estimates $ \\hat{\\beta}_{0} , \\hat{\\beta}_{1} , \\dots ,\\hat{\\beta}_{p} $ , we can make predictions using the formula\n",
    "\n",
    "><font size=5>$ \\hat{y} = \\hat{\\beta}_{0} X_{0} + \\hat{\\beta}_{1} X_{1} + \\hat{\\beta}_{1} X_{1} + \\dots + \\hat{\\beta}_{p} X_{p} $</font>\n",
    "\n",
    "The `parameters` are `estimated` using the `same least squares approach` that `we saw` in the `context` of `simple linear regression`. \n",
    "\n",
    "We `choose` $ \\beta_{0} ,\\beta_{1}, \\beta_{2}, \\dots , \\beta_{p}$ to `minimize` the `sum of squared residuals` __(RSS).__\n",
    "\n",
    "<a href=\"#Formula3.22\"></a>\n",
    "><font size=5> $  RSS = \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^2 $ </font>\n",
    ">><font size=5> $ = \\sum_{i=1}^{n} (y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1} X_{i1} - \\hat{\\beta}_{2} X_{i2} -  \\dots - \\hat{\\beta}_{p} X_{ip} )^2 $ </font>\n",
    "\n",
    "<a id=\"Figure3.4\"></a>\n",
    "![image.png](Figures/Figure3.4.png)\n",
    ">__FIGURE 3.4__. In a `three-dimensional setting`, with `two predictors` and `one response`, the `least squares regression line` becomes a `plane`. \n",
    "<br>The `plane` is `chosen to minimize` the `sum of the squared vertical distances` between `each observation` (shown in red) and the `plane`.\n",
    "\n",
    "***\n",
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "\n",
    "<a href=\"#Table3.4\"><b>Table 3.4</b></a> displays the `multiple regression coefficient estimates` when __TV,\n",
    "radio, and newspaper advertising budgets are used to predict product sales\n",
    "using the Advertising data__. \n",
    "\n",
    "We `interpret` these `results` as follows: \n",
    "1. for a given `amount of TV and newspaper advertising`, `spending` an additional &dollar;1,000 on `radio advertising` leads to an __increase__ in `sales by approximately` 189 units. \n",
    "    \n",
    "    `Comparing these coefficient` estimates to those `displayed` in <a href=\"#Table3.1\">Tables 3.1</a> and <a href=\"#Table3.3\">3.3</a>, we `notice` that the `multiple regression coefficient estimates` for __TV and radio are pretty similar to the simple linear regression coefficient estimates__. \n",
    "    \n",
    "    However, while the `newspaper regression coefficient estimate` in <a href=\"#Table3.3\">Table 3.3</a> was `significantly non-zero`, the `coefficient estimate` for newspaper in the `multiple regression model` is __`close to zero`__, and the corresponding $p$__-value__ is `no longer significant`, with a `value around` 0.86. \n",
    "    \n",
    "<a id=\"Table3.4\"></a>\n",
    "\n",
    "|    &nbsp;|Coefficient |Std. Error | t-statistics | p-values |\n",
    "|----------|------------|-----------|--------------|----------|\n",
    "|Intercept | 2.939| 0.3119| 9.42| < 0.0001|\n",
    "|Tv        | 0.046| 0.0014| 32.81| < 0.0001|\n",
    "|Radio     | 0.189| 0.0086| 21.89| < 0.0001|\n",
    "|Newspaper |-0.001| 0.0059| -0.18|  0.8599|\n",
    "\n",
    ">__TABLE 3.4__. For the `Advertising data`, `least squares coefficient estimates` of the\n",
    "`multiple linear regression` of `number of units sold` on `radio`, `TV`, and `newspaper\n",
    "advertising` budgets.\n",
    "\n",
    "___Does it make sense for the multiple regression to suggest no relationship between sales and newspaper while the simple linear regression implies the opposite?___\n",
    "\n",
    "In fact it does, see the below table:\n",
    "\n",
    "<a id=\"Table3.5\"></a>\n",
    "\n",
    "|    &nbsp;|TV |radio| newspaper | sales |\n",
    "|----------|------------|-----------|--------------|----------|\n",
    "|Tv        | 1.0000| 0.0548| 0.0567| 0.7822|\n",
    "|Radio     |&nbsp;| 1.0000| 0.3541| 0.5762|\n",
    "|Newspaper |&nbsp;| &nbsp;| 1.0000|  0.2283|\n",
    "|sales | &nbsp;| &nbsp;|&nbsp;| 1.0000|\n",
    "\n",
    ">__TABLE 3.5__. `Correlation matrix` for `TV`, `radio`, `newspaper`, and `sales` for the\n",
    "`Advertising data`.\n",
    "\n",
    "[To understand Correlation matrix in deep, Visit Book __Page No 88__]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Some Important Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we perform `multiple linear regression`, we usually are `interested` in `answering` a __few important questions__.\n",
    "\n",
    "1. Is at least one of the predictors X 1 , X 2 , . . . , X p useful in predicting the response?\n",
    "2. Do all the predictors help to explain Y , or is only a subset of the predictors useful?\n",
    "3. How well does the model fit the data?\n",
    "4. Given a set of predictor values, what response value should we predict, and how accurate is our prediction?\n",
    "\n",
    "__[Must Read All the Answers Given in Book Page No 89 to 96]__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Other Considerations in the Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Qualitative Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our `discussion` so far, we have `assumed` that `all variables` in our `linear regression model` are ___`quantitative`___. \n",
    "\n",
    "But in `practice`, this is `not necessarily` the `case`; often some `predictors` are ___`qualitative`___.\n",
    "\n",
    "\n",
    "__For example__, the `Credit data set` displayed in <a href=\"#Figure3.6\">Figure 3.6</a> records `balance` (average credit card debt for a number of individuals) `as well as` several `quantitative predictors`: __age , cards (number of credit cards), education (years of education), income (in thousands of dollars), limit (credit limit),\n",
    "and rating (credit rating)__.\n",
    "\n",
    "Each panel of <a href=\"Figure3.6\">Figure 3.6</a> is a `scatterplot` for a\n",
    "pair of `variables` whose `identities` are given by the `corresponding row` and\n",
    "`column labels`. \n",
    "\n",
    "__For `example`__, the `scatterplot` directly to the `right` of the word\n",
    "`“Balance”` depicts `balance versus age` , while the `plot directly` to the right\n",
    "of `“Age”` corresponds to `age versus cards` . \n",
    "\n",
    "In addition to these `quantitative variables`, we also have `four qualitative variables`: __gender , student (student status), status (marital status), and ethnicity (Caucasian, African American or Asian)__.\n",
    "\n",
    "<a id=\"Figure3.6\"></a>\n",
    "![image.png](Figures/Figure3.6.png)\n",
    "\n",
    ">__FIGURE 3.6__. The `Credit data set` contains `information` about `balance, age,\n",
    "cards, education, income, limit, and rating` for a `number` of `potential` customers.\n",
    "\n",
    "<a id=\"Table3.7\"></a>\n",
    "\n",
    "|    &nbsp;|Coefficient |Std. Error | t-statistics | p-values |\n",
    "|----------|------------|-----------|--------------|----------|\n",
    "|Intercept | 509.80| 33.13| 15.389| < 0.0001|\n",
    "|gender[Female] | 19.73| 46.05| 0.429| 0.6690|\n",
    "\n",
    ">__TABLE 3.7__. `Least squares coefficient` estimates `associated` with the `regression` of\n",
    "`balance onto gender` in the `Credit data set`. \n",
    "<br>The `linear model` is given in <a href=\"#Formula3.27\">(3.27)</a>.\n",
    "That is, `gender` is `encoded` as a `dummy variable`, as in <a href=\"#Formula3.26\">(3.26)</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictors with Only Two Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we wish to `investigate differences` in `credit card balance` between `males and females`, ignoring the `other variables` for the `moment`. \n",
    "\n",
    "If a `qualitative predictor` _(also known as a factor)__ only has `two levels`, or `possible values`, then `incorporating` it into a `regression model` is `very simple`. \n",
    "\n",
    "We `simply create` an `indicator` or __`dummy variable`__ that `takes` on `two possible numerical values`. \n",
    "\n",
    "_For example_, based on the `gender variable`, we can create a new variable that takes the form\n",
    "\n",
    "\n",
    "<a id=\"Formula3.26\"></a>\n",
    "![image.png](Figures/Formula3.26.png)\n",
    "\n",
    "and use this variable as a predictor in the regression equation. This results in the model\n",
    "\n",
    "<a id=\"Formula3.27\"></a>\n",
    "![image.png](Figures/Formula3.27.png)\n",
    "\n",
    "Now $ \\beta_{0}$ can be `interpreted` as the `average credit card balance among males`, $ \\beta_{0} + \\beta_{1} $ as the `average credit card balance among females`, and $\\beta_{1} $ as the `average difference` in `credit card balance` between `females and males`.\n",
    "\n",
    "<a href=\"#Table3.7\">Table 3.7</a> displays the `coefficient estimates` and `other information associated` with the `model` <a href=\"#Formula3.27\">(3.27)</a>. \n",
    "\n",
    "The `average credit card` __debt for males__ is estimated to be &dollar; 509.80, whereas __females__ are estimated to `carry` &dollar; 19.73 in `additional` __debt__ for a `total` of &dollar;509.80 + &dollar;19.73 = &dollar;529.53. \n",
    "\n",
    "However, we `notice` that the __p-value__ for the __dummy variable__ is `very high`. \n",
    "\n",
    "This indicates that `there is no statistical evidence` of a `difference in average credit card\n",
    "balance` between the `genders`.\n",
    "\n",
    "The decision to `code females as 1` and `males as 0` in <a href=\"#Formula3.27\">(3.27)</a> is `arbitrary`, and has no `effect` on the `regression fit`, but `does alter` the `interpretation` of the `coefficients`. \n",
    "\n",
    "If we `had coded males as 1` and `females as 0`, then the `estimates` for $β_{0}$ and $β_{1}$ would have been 529.53 and −19.73, respectively, `leading once again` to a `prediction of credit card` __debt__ of &dollar; 529.53 − &dollar;19.73 = &dollar; 509.80 for `males` and a `prediction` of &dollar;529.53 for `females`. \n",
    "\n",
    "Alternatively, instead of a 0/1 `coding scheme`, we could `create` a __dummy variable__\n",
    "\n",
    "![image.png](Figures/FormulaE1.png)\n",
    "\n",
    "![image.png](Figures/FormulaE2.png)\n",
    "\n",
    "Now $\\beta_{0}$ can be `interpreted` as the `overall average credit card balance` (ignoring the gender effect), and $\\beta_{1}$ is the `amount that females` are `above` the `average and males` are `below the average`. \n",
    "\n",
    "In this __example__, the estimate for $\\beta_{0}$ would be &dollar;519.665, `halfway between` the __male and female averages__ of &dollar; 509.80 and &dollar; 529.53. \n",
    "\n",
    "The estimate for $\\beta_{1}$ would be &dollar;9.865, which is `half of` &dollar;19.73, the `average difference between` __females and males__. \n",
    "\n",
    "It is `important` to `note` that the `final predictions` for the `credit balances` of `males and females` will be `identical regardless` of the `coding scheme used`. \n",
    "\n",
    "The only `difference` is in the `way` that the `coefficients are interpreted`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualitative Predictors with More than Two Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a `qualitative predictor` has `more than two levels`, a single `dummy variable` cannot `represent` all `possible values`. \n",
    "\n",
    "In this `situation`, we can `create additional dummy variables`. \n",
    "\n",
    "For example, for the `ethnicity variable` we `create two dummy variables`. \n",
    "\n",
    "The first could be\n",
    "![image.png](Figures/Formula3.28.png)\n",
    "\n",
    "The second could be\n",
    "![image.png](Figures/Formula3.29.png)\n",
    "\n",
    "Then `both of these variables` can be used in the `regression equation`, in `order` to obtain the `model`\n",
    "![image.png](Figures/Formula3.30.png)\n",
    "\n",
    "Now $\\beta_{0}$ can be `interpreted` as the `average credit card balance` for `African Americans`, $\\beta_{1}$ can be `interpreted` as the `difference in the average balance between the Asian and African American categories`, and $\\beta_{2}$ can be `interpreted` as the `difference in the average balance between the Caucasian and African American categories`. \n",
    "\n",
    "There will `always be one fewer` __dummy variable__ than the `number of levels`. \n",
    "The level with `no dummy variable`—African American in this `example`—is `known` as the __`baseline`__.\n",
    "\n",
    "<a id=\"Table3.8\"></a>\n",
    "\n",
    "|    &nbsp;|Coefficient |Std. Error | t-statistics | p-values |\n",
    "|----------|------------|-----------|--------------|----------|\n",
    "|Intercept | 531.00| 46.32| 11.464| < 0.0001|\n",
    "|ethnicity[Asian] | -18.69| 65.02| -0.287| 0.7740|\n",
    "|ethnicity[Caucasian] | -12.50| 56.68| -0.221| 0.8260|\n",
    "\n",
    ">__TABLE 3.8__. `Least squares coefficient` estimates `associated` with the `regression` of `balance onto ethnicity` in the `Credit data set`. \n",
    "<br>The `linear model` is given in <a href=\"#Formula3.30\">( 3.30 )</a>. \n",
    "<br>That is, `ethnicity` is `encoded` via `two dummy variables` <a href=\"#Formula3.28\">(3.28)</a> and <a href=\"#Formula3.29\">(3.29)</a>.\n",
    "\n",
    "__[Read More on Book page No 100]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Extensions of the Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `standard linear regression model` <a href=\"#Formula3.19\">(3.19)</a> provides `interpretable results` and `works quite well` on `many real-world problems`. \n",
    "\n",
    "However, it makes `several highly restrictive`__assumptions__ that are `often violated` in `practice`. \n",
    "\n",
    "`Two of the most important assumptions` state that the __relationship between the predictors and response are additive and linear__. \n",
    "\n",
    "The `additive assumption` means that `the effect of changes` in `a predictor` $X_{j}$ on `the response` $Y$ is `independent` of the `values of the other predictors`. \n",
    "\n",
    "The `linear assumption`\n",
    "states that `the change in the response` $Y$ due to a `one-unit change` in $X_{j}$ is `constant`, `regardless` of the `value` of $X_{j}$. \n",
    "\n",
    "In this `book`, we examine a `number of sophisticated methods` that `relax these two assumptions`. \n",
    "\n",
    "Here, we `briefly examine` some `common classical approaches` for `extending the linear model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the Additive Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous `analysis of the Advertising data`, \n",
    "<br>we `concluded` that `both TV and radio` seem to be `associated with sales`.\n",
    "\n",
    "The `linear models` that `formed` the `basis for this conclusion` assumed that the `effect on sales` of `increasing one advertising medium` is `independent` of the `amount spent on the other media`. \n",
    "\n",
    "__For example__, the `linear model` <a href=\"#Formula3.20\">(3.20)</a> states that __`the average effect on sales of a one-unit increase in TV is always`__ $\\beta_{1}$, `regardless` of `the amount`\n",
    "spent on radio.\n",
    "\n",
    "However, this `simple model may be incorrect`. \n",
    "\n",
    "Suppose that `spending money on radio advertising` actually `increases the effectiveness of TV advertising`, so that the `slope term for TV` should `increase as radio increases`.\n",
    "\n",
    "In this `situation`, given a `fixed budget` of &dollar;100,000, `spending half on radio` and `half on TV` may `increase sales more` than `allocating the entire amount` to `either TV or to radio`. \n",
    "\n",
    "In marketing, this is `known` as a __`synergy effect`__, and in `statistics` it is referred to as an __`interaction effect`__. \n",
    "\n",
    "<a href=\"#Figure3.5\">Figure 3.5</a> suggests that `such` an `effect` may be `present` in the `advertising data`. \n",
    "\n",
    "Notice that `when levels` of `either TV or radio are low`, then `the true sales are lower than predicted` by the `linear model`. \n",
    "\n",
    "But when `advertising is split` between `the two media`, then `the model` tends to `underestimate sales`.\n",
    "\n",
    "Consider the standard `linear regression model` with __two variables__,\n",
    "\n",
    "<font size=5><center>$ Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\epsilon$.</center></font>\n",
    "\n",
    "According to `this model`, if we `increase` $X_{1}$ by `one unit`, then $Y$ will `increase` by an `average` of $\\beta_{1}$ units. \n",
    "\n",
    "Notice that the `presence` of $X_{2}$ does `not alter` this `statement`—that is, __regardless of the value of $X_{2}$, a `one-unit increase` in $X_{1}$ will `lead` to a $\\beta_{1}$-unit increase in $Y$.__\n",
    "\n",
    "One `way` of extending this `model` to allow for __`interaction effects`__ is to `include a third predictor`, called an\n",
    "`interaction term`, which is `constructed by computing` the `product of`$X_{1}$ and $X_{2}$. \n",
    "\n",
    "This `results` in the `model`\n",
    "\n",
    "<a id=\"Formula3.31\"></a>\n",
    "<font size=5><center>$ Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\beta_{3} X_{1} X_{2} + \\epsilon$.</center></font>\n",
    "\n",
    "__How does `inclusion` of this `interaction term` relax the `additive assumption`?__\n",
    "\n",
    "Notice that <a href=\"#Formula3.31\">(3.31)</a> can be rewritten as\n",
    "\n",
    "<a id=\"Formula3.32\"></a>\n",
    "<font size=5><center>$ Y = \\beta_{0} + (\\beta_{1}+\\beta_{3} X_{2}) X_{1} + \\beta_{2} X_{2} +  \\epsilon$.<br>$ = \\beta_{0} + \\tilde{\\beta}_{1} X_{1} + \\beta_{2} X_{2} + \\epsilon$ </center></font>\n",
    "\n",
    ">where $\\tilde{\\beta}_{1} = \\beta_{1} + \\beta_{3} X_{2}$. \n",
    "<br>Since $\\tilde{\\beta}_{1}$ changes with $X_{2}$ , the `effect` of $X_{1}$ on $Y$ is `no longer constant`: __adjusting $X_{2}$ will `change` the `impact` of $X_{1}$ on $Y$.__\n",
    "\n",
    "<a id=\"Table3.9\"></a>\n",
    "\n",
    "|    &nbsp;|Coefficient |Std. Error | t-statistics | p-values |\n",
    "|----------|------------|-----------|--------------|----------|\n",
    "|Intercept | 6.7502| 0.248| 27.23| < 0.0001|\n",
    "|TV | 0.0191| 0.002| 12.70| < 0.0001|\n",
    "|radio | 0.0289| 0.009| 3.24| 0.0014|\n",
    "|TV * radio | 0.0011| 0.000| 20.73| < 0.0001|\n",
    "\n",
    ">__TABLE 3.9__. For the `Advertising data`, `least squares coefficient` estimates associated with the `regression of sales onto TV and radio,` with an `interaction term`, as in <a href=\"#Formula3.33\">(3.33)</a>.\n",
    "\n",
    "\n",
    "__For example__, `suppose` that we are `interested in studying the productivity of a factory`. \n",
    "\n",
    "We `wish to predict` the `number of units produced` on the `basis of the number of production lines` and `the total number of workers`.\n",
    "\n",
    "It `seems` likely that the `effect of increasing the number of production lines` will depend on `the number of workers`, since `if no workers are available to operate the lines`, then `increasing the number of lines will not increase production`. \n",
    "\n",
    "This suggests that `it would be appropriate` to `include` an `interaction term between lines and workers` in a `linear model` to `predict units`.\n",
    "\n",
    "Suppose that when we `fit the model`, __we obtain__\n",
    "\n",
    "<font size=5><center> $ units \\approx 1.2 + 3.4 × lines + 0.22 × workers + 1.4 × ( lines × workers ) $<br> $ = 1.2 + (3.4 + 1.4 × workers ) × lines + 0.22 × workers . $ </center></font>\n",
    "\n",
    "In `other words`, `adding an additional line` will `increase the number of units` _produced by 3.4 + 1.4 × workers_.\n",
    "\n",
    "Hence the `more workers we have`, the`stronger will be the effect of lines`.\n",
    "\n",
    "We now `return to the Advertising example`. \n",
    "\n",
    "A `linear model` that uses __radio , TV , and an interaction__ between the `two to predict sales` takes the\n",
    "form\n",
    "\n",
    "<a id=\"Formula3.33\"></a>\n",
    "<font size=5><center> $ sales = \\beta_{0} + \\beta_{1} * TV + \\beta_{2} * radio + \\beta_{3} * (radio + TV) + \\epsilon $ <br> $ = \\beta_{0} + (\\beta_{1} + \\beta_{3} * radio) * TV + \\beta_{2} * radio + \\epsilon $</center></font>\n",
    "\n",
    "We can `interpret` $\\beta_{3}$ as `the increase in the effectiveness of TV advertising` for a `one unit increase in radio advertising` (or vice-versa). \n",
    "\n",
    "The `coefficients` that `result` from `fitting the model` <a href=\"#Formula3.33\">(3.33)</a> are `given in Table` <a href=\"#Table3.9\">3.9</a>.\n",
    "\n",
    ">The results in Table <a href=\"#Table3.9\">3.9</a> strongly suggest that the model that includes the interaction term is superior to the model that contains only __`main effects`__.\n",
    "<br>The __p-value__ for the interaction term, $TV × radio $, is extremely low, indicating\n",
    "that there is strong evidence for $H_{a} : \\beta_{3} \\neq 0$. \n",
    "<br>In other words, it is clear that the true relationship is not additive. \n",
    "<br>The $R^2$ for the model <a href=\"#Formula3.33\">(3.33)</a> is $96.8 \\%$, compared to only $89.7 \\%$ for the model that predicts sales using TV and radio without an interaction term. \n",
    "<br>This means that $ (96.8 − 89.7)/(100 − 89.7) = 69\\% $ of the variability in sales that remains after fitting the additive model has been explained by the interaction term. The coefficient estimates in <a href=\"#Table3.9\">Table 3.9</a> suggest that an increase in TV advertising of &dollar; 1,000 is associated with increased sales of...\n",
    "\n",
    ">$ ( \\hat{ \\beta }_{1} + \\hat{ \\beta }_{3} * radio ) * 1,000 = 19 + 1.1 * radio\\ units.$\n",
    "\n",
    ">And an increase in radio advertising of &dollar;1,000 will be associated with an increase in sales of \n",
    "\n",
    ">$(\\hat{\\beta}_{2} + \\hat{\\beta}_{3} × TV ) × 1,000 = 29 + 1.1 × TV units $.\n",
    "\n",
    "In this __example__, the `p-values` associated with `TV , radio , and the interaction` term all are statistically significant (<a href=\"#Table3.9\">Table 3.9</a>), and so it is obvious that all three variables should be included in the model. However, it is sometimes the case that an interaction term has a very `small p-value`, but the associated main effects (in this case, TV and radio ) do not. The hierarchical principle states that if we include an interaction in a model, we should also include the main effects, even if the `p-values` associated with\n",
    "their coefficients are not significant. \n",
    "\n",
    "In other words, if the interaction between $X_{1}$ and $X_{2}$ seems important, then we should include both $X_{1}$ and $X_{2}$ in the model even if their coefficient estimates have `large p-values`. The rationale for this principle is that if $X_{1} * X_{2}$ is related to the `response`, then whether or not the coefficients of $X_{1}$ or $X_{2}$ are exactly zero is of little interest. Also $X_{1} * X_{2}$ is typically correlated with $X_{1}$ and $X_{2}$ , and so leaving them out tends to alter the meaning of the interaction.\n",
    "\n",
    "In the previous example, we considered an interaction between TV and radio , both of which are quantitative variables. However, the concept of interactions applies just as well to qualitative variables, or to a combination of quantitative and qualitative variables. In fact, an interaction between a qualitative variable and a quantitative variable has a particularly nice interpretation. Consider the Credit data set from <a href=\"#3.3.1-Qualitative-Predictors\">Section 3.3.1</a>, and suppose that we wish to predict balance using the income (quantitative) and student (qualitative) variables. In the absence of an interaction term, the model takes the form\n",
    "\n",
    "<a id=\"Formula3.34\"></a>\n",
    "![image.png](Figures/Formula3.34.png)\n",
    "\n",
    "Notice that this `amounts to fitting` two parallel lines to the `data`, one for `students` and one for `non-students`. \n",
    "\n",
    "The `lines` for `students and non-students` have `different intercepts`, $\\beta_{0} + \\beta_{2} versus \\beta_{0}$, but the same `slope`, $\\beta_{1}$. \n",
    "\n",
    "This is illustrated in the `left-hand panel` of <a href=\"#Figure3.7\">Figure 3.7</a>. \n",
    "\n",
    "The fact that the `lines are parallel` means that the average effect on balance of a one-unit increase in income does not depend on whether or not the individual is a student.\n",
    "\n",
    "This represents a potentially serious limitation of the model, since in fact a change in income may have a very different effect on the credit card balance of a student versus a non-student.\n",
    "\n",
    "<a id=\"Figure3.37\"></a>\n",
    "![image.png](Figures/Figure3.7.png)\n",
    "\n",
    ">__FIGURE 3.7__. For the `Credit data`, the `least squares lines` are shown for `prediction of balance` from income for students and non-students. \n",
    "\n",
    ">__Left__: The model <a href=\"#Formula3.34\">(3.34)</a> was fit. There is no `interaction between income and student`. \n",
    "\n",
    ">__Right__: The model <a href=\"#Formula3.35\">(3.35)</a> was fit. There is an `interaction term between income and student`.\n",
    "\n",
    "This limitation can be addressed by adding an interaction variable, created by multiplying income with the `dummy variable` for `student` . \n",
    "\n",
    "Our model now `becomes`\n",
    "\n",
    "<a id=\"Formula3.35\"></a>\n",
    "![image.png](Figures/Formula3.35.png)\n",
    "\n",
    "Once again, we have `two different regression lines` for the `students and the non-students`. \n",
    "\n",
    "But now `those regression lines have different intercepts`, $\\beta_{0} + \\beta_{2} versus \\beta_{0}$, as well as different slopes, $\\beta_{1} + \\beta_{3} versus \\beta_{1}$. \n",
    "\n",
    "This allows for the `possibility` that `changes in income may affect the credit card balances` of `students` and `non-students differently`. \n",
    "\n",
    "The `right-hand` panel of <a href=\"#Figure3.7\">Figure 3.7</a> shows the `estimated relationships between income and balance` for `students and non-students` in the `model` <a href=\"#Formula3.35\">(3.35)</a>. \n",
    "\n",
    "We note that the `slope` for `students` is `lower than the slope for non-students`. \n",
    "\n",
    "This suggests that `increases in income` are associated with `smaller increases in credit card balance` among `students as compared to non-students`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed previously, the `linear regression model` <a href=\"#Formula3.19\">(3.19)</a> assumes a `linear\n",
    "relationship between` the `response and predictors`. \n",
    "\n",
    "But in some `cases`, the\n",
    "`true relationship` between `the response and the predictors` may be `non-linear`. \n",
    "Here we `present` a `very simple way` to `directly extend` the `linear model` to accommodate `non-linear relationships`, using __`polynomial regression`__. \n",
    "\n",
    "\n",
    "Consider <a href=\"#Figure3.8\">Figure 3.8</a>, in which the `mpg` (gas mileage in miles per gallon) versus `horsepower` is shown for `a number of cars in the Auto data set`. The\n",
    "\n",
    "<a id=\"Figure3.8\"></a>\n",
    "![image.png](Figures/Figure3.8.png)\n",
    "\n",
    ">__FIGURE 3.8__. The `Auto data set`. For a `number of cars, mpg and horsepower` are\n",
    "shown. \n",
    "<br>The `linear regression fit` is shown in `orange`. \n",
    "<br>The `linear regression fit` for a `model` that `includes horsepower` $2$ is shown as a `blue curve`.\n",
    "<br>The `linear regression fit for a model` that `includes` __all polynomials of horsepower up to fifth-degree__ is shown in green.\n",
    "\n",
    "`orange line represents` the `linear regression fit`. There is a `pronounced relationship` between __mpg and horsepower__ , but it seems `clear` that this `relationship` is in `fact non-linear`: the `data suggest` a `curved relationship`. \n",
    "A simple approach for `incorporating non-linear associations` in a `linear model` is to `include transformed versions` of the `predictors in the model`. \n",
    "\n",
    "For example, the `points` in <a href=\"#Figure3.8\">Figure 3.8</a> seem to have a __quadratic shape__, suggesting that a `model of the form` may provide a __`better fit`__.\n",
    "\n",
    "<a id=\"Formula3.36\"></a>\n",
    "<font size=5><center> $ mpg = \\beta_{0} + \\beta_{1} * horsepower + \\beta_{2} * horsepower^2 + \\epsilon $ </center></font>\n",
    "\n",
    "<a href=\"#Formula3.36\">Equation 3.36</a> involves `predicting mpg` using a `non-linear` function of `horsepower` . \n",
    "\n",
    "But it is still a `linear model!` That is, <a href=\"#Formula3.36\">(3.36)</a> is simply a `multiple linear regression model` with\n",
    "<br> $X_{1} = horsepower$ and $ X_{2} = horsepower^2$ . \n",
    "\n",
    "So we can `use standard linear regression software` to\n",
    "estimate $\\beta_{0}, \\beta_{1}, and \\beta_{2}$ in `order` to `produce a non-linear fit`.\n",
    "\n",
    "The `blue curve` in <a href=\"#Figure3.8\">Figure 3.8</a> shows the `resulting quadratic fit` to the `data`. \n",
    "\n",
    "The __quadratic fit__ appears to be `substantially better` than the `fit obtained` when just the\n",
    "`linear term` is `included`. \n",
    "\n",
    "The $R^2$ of the `quadratic fit` is $0.688$, compared to $0.606$ for the `linear fit`, and the __p-value__ in <a href=\"#Table3.10\">Table 3.10</a> for the `quadratic term` is `highly significant`.\n",
    "\n",
    "<a id=\"Table3.10\"></a>\n",
    "\n",
    "|    &nbsp;|Coefficient |Std. Error | t-statistics | p-values |\n",
    "|----------|------------|-----------|--------------|----------|\n",
    "|Intercept | 56.9001| 1.8004| 31.6| < 0.0001|\n",
    "|horsepower | -0.4662| 0.0311| -15.0| < 0.0001|\n",
    "|$horsepower^2$ | 0.0012| 0.0001| 10.1| 0.0014|\n",
    "\n",
    ">__TABLE 3.10__. For the `Auto data set`, `least squares coefficient` estimates associated with the `regression of mpg` onto $horsepower$ and $horsepower^2$\n",
    "\n",
    "If including $horsepower^2$ led to such a big improvement in the model, \n",
    "\n",
    "__why not include $horsepower^3$ , $horsepower^4$ , or even $horsepower^5$ ?__ \n",
    "\n",
    "The `green curve` in <a href=\"#Figure3.8\">Figure 3.8</a> displays the `fit` that results from `including all polynomials` up to `fifth degree` in the model <a href=\"#Formula3.36\">(3.36)</a>. \n",
    "\n",
    "The resulting fit seems `unnecessarily wiggly`—that is, it is `unclear` that `including the additional terms really` has `led` to a `better fit` to the `data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Potential Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we `fit a linear regression model` to a `particular data set`, many `problems may occur`. \n",
    "\n",
    "___Most common among these are the following:___\n",
    "\n",
    "1. Non-linearity of the response-predictor relationships.\n",
    "2. Correlation of error terms.\n",
    "3. Non-constant variance of error terms.\n",
    "4. Outliers.\n",
    "5. High-leverage points.\n",
    "6. Collinearity.\n",
    "\n",
    "In practice, `identifying and overcoming` these `problems` is as much `an\n",
    "art as a science`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Non-linearity of the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `linear regression model` assumes that there is a `straight-line relationship` between the `predictors and the response`. \n",
    "\n",
    "If the `true relationship` is `far from linear`, then `virtually` all of the `conclusions that we draw` from the `fit are suspect`. \n",
    "\n",
    "In `addition`, the `prediction accuracy of the model` can be `significantly reduced`.\n",
    "\n",
    "__`Residual plots`__ are a useful `graphical tool` for `identifying` `non-linearity`.\n",
    "\n",
    "Given a `simple linear regression model`, we can `plot the residuals`, $e_{i} = y_{i} − \\hat{y}_{i}$, versus the `predictor` $x_{i}$. \n",
    "\n",
    "In the case of a `multiple regression model`,since there are `multiple predictors`, we `instead plot the residuals versus the predicted (or fitted) values` $\\hat{y}_{i}$ . \n",
    "\n",
    "Ideally, the ___`residual plot`___ will show `no discernible pattern`. \n",
    "\n",
    "The presence of a pattern may indicate a problem with some aspect of the `linear model`.\n",
    "\n",
    "<a id=\"Figure3.9\"></a>\n",
    "![image.png](Figures/Figure3.9.png)\n",
    "\n",
    ">__FIGURE 3.9__. `Plots of residuals` versus `predicted (or fitted) values` for the `Auto\n",
    "data set`. \n",
    "<br>In each `plot`, the `red line` is a `smooth fit` to the `residuals`, intended to make\n",
    "it easier to `identify a trend`. \n",
    "\n",
    ">__Left:__ A `linear regression of mpg on horsepower`. \n",
    "<br>A strong pattern in the residuals indicates non-linearity in the data.\n",
    "\n",
    ">__Right:__ A `linear regression of mpg on horsepower and` $horsepower^2$ . \n",
    "<br>There is little pattern in the residuals.\n",
    "\n",
    "The `left` panel of <a href=\"#Figure3.9\">Figure 3.9</a> displays a `residual plot` from the `linear\n",
    "regression of mpg onto horsepower` on the `Auto data set` that was `illustrated`\n",
    "in <a href=\"#Figure3.8\">Figure 3.8</a>. \n",
    ">The `red line` is a `smooth fit to the residuals`, which is displayed in order to `make it easier to identify any trends`. \n",
    "<br>The residuals exhibit a `clear U-shape`, which `provides` a strong `indication` of `non-linearity` in the\n",
    "`data`. \n",
    "<br>In `contrast`, the `right-hand panel` of `Figure 3.9` displays the `residual plot` that `results` from the `model` <a href=\"#Formula3.36\">(3.36)</a>, which `contains a quadratic term`.\n",
    "<br> There appears to be `little pattern in the residuals`, suggesting that `the quadratic term` improves the `fit to the data`.\n",
    "<br> If the `residual plot indicates` that there are `non-linear associations` in the `data`, then `a simple approach` is to `use non-linear transformations` of the\n",
    "`predictors`, \n",
    ">>such as $log X, X,$ and $X^2$ , in the `regression model`. \n",
    "<br> In the `later chapters of this book`, we will discuss other more `advanced non-linear approaches` for `addressing this issue`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Correlation of Error Terms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `important assumption` of the `linear regression model` is that the `error terms`, $\\epsilon_{1},\\epsilon_{2}, \\dots, \\epsilon_{n}$, are `uncorrelated`. \n",
    "\n",
    "&bullet; __What does this mean?__ \n",
    "\n",
    "For instance, if the `errors are uncorrelated`, then the `fact` that $\\epsilon_{i}$ is `positive` provides `little or no information` about the `sign of` $\\epsilon_{i+1}$. \n",
    "\n",
    "The __`standard errors that are computed`__ for the `estimated regression coefficients` or the `fitted values` are based on the `assumption of uncorrelated error terms`. \n",
    "\n",
    "If in fact there is `correlation among` the `error terms`, then the `estimated standard errors` will tend to `underestimate the true standard errors`. \n",
    "\n",
    "As a result, `confidence and prediction intervals` will be `narrower than they should be`. \n",
    "\n",
    "__For example__, a 95 % confidence interval may in `reality have` a `much lower probability` than 0.95 of containing the `true value of the parameter`. \n",
    "\n",
    "In addition, `p-values` associated with the `model will be lower than they should be`; this could cause us to `erroneously` conclude that a `parameter is statistically significant`. \n",
    "\n",
    "In short,__if the `error terms are correlated`, we may have an `unwarranted sense of confidence in our model`__.\n",
    "\n",
    "__Why might correlations among the error terms occur?__\n",
    "\n",
    "Such `correlations frequently occur` in the context of ___`time series`___ data, which consists of `observations` for which `measurements are obtained` at `discrete points` in `time`.\n",
    "\n",
    "In many cases, `observations` that are `obtained at adjacent time points` will have `positively correlated errors`. \n",
    "\n",
    "In `order to determine` if this is the `case for a given data set`, we can `plot the residuals from our model` as a `function of time`. \n",
    "\n",
    "If the `errors are uncorrelated`, then `there should` be `no discernible pattern`. \n",
    "\n",
    "On the other hand, if the error terms are positively correlated, then we may see ___`tracking`___ in the residuals—that is, adjacent residuals may have similar values. \n",
    "\n",
    "Figure 3.10 provides an illustration. In the top panel, we see the residuals from a linear regression fit to data generated with uncorrelated errors. \n",
    "\n",
    "There is no evidence of a time-related trend in the residuals.\n",
    "\n",
    "In contrast, the residuals in the bottom panel are from a data set in which adjacent errors had a correlation of 0.9. \n",
    "\n",
    "Now there is a clear pattern in the residuals—adjacent residuals tend to take on similar values. \n",
    "\n",
    "Finally, the center panel illustrates a more moderate case in which the residuals had a correlation of 0.5. \n",
    "\n",
    "There is still evidence of tracking, but the pattern is less clear.\n",
    "\n",
    "\n",
    "<a id=\"Figure3.10\"></a>\n",
    "![Figure3.10](Figures/Figure3.10.png)\n",
    ">__FIGURE 3.10.__ `Plots of residuals` from `simulated time series data sets` generated with `differing levels` of `correlation` $ρ$ between `error terms for adjacent time points`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Non-constant Variance of Error Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Read it from Book Page No 109__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Read it from Book Page No 110__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Read it from Book Page No 113__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 The Marketing Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Read it from Book Page No 116__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Comparison of Linear Regression with K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Read it from Book Page No 118__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Lab: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We will preform it in python not in R__\n",
    "\n",
    "__in different Notebook.__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
