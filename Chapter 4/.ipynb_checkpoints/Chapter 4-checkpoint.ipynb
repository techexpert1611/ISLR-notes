{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 : Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&bullet; The __linear regression model__ discussed in <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%203/Chapter%203.ipynb#3-Linear-Regression\">Chapter 3</a> assumes that the `response variable` <font size=3>$Y$</font> is __quantitative__. \n",
    "\n",
    "&bullet; But in `many situations`, the `response variable` is ___instead `qualitative`___. \n",
    "\n",
    ">For __example__, `eye color` is `qualitative`, taking on values _blue, brown, or green_. \n",
    "\n",
    "&bullet; Often `qualitative variables` are referred to as __`categorical`__ ; we will `use` these terms `interchangeably`. \n",
    "\n",
    "&bullet; In this chapter, we study `approaches` for `predicting qualitative responses`, a process that is `known` as __`classification`__. \n",
    "\n",
    "&bullet; `Predicting` a `qualitative response` for an `observation` can be `referred` to as ___`classifying`___ that observation, since it `involves assigning` the `observation to a category`, or `class`. \n",
    "\n",
    "&bullet; On the `other hand`, often the `methods used` for `classification` first `predict the probability` of each of the `categories` of a `qualitative variable`, as the `basis` for `making` the `classification`. \n",
    "\n",
    "&bullet; In this sense they also behave like `regression methods`.\n",
    "\n",
    "&bullet; There are many `possible classification techniques`, or __classifiers__, that one might use to `predict a qualitative response`. \n",
    "\n",
    "&bullet; We touched on some of these in <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Chapter%202.ipynb#2.1.5-Regression-Versus-Classification-Problems\">Sections 2.1.5</a> and <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Chapter%202.ipynb#2.2.3-The-Classification-Setting\">2.2.3</a>. \n",
    "\n",
    "&bullet; In this chapter we `discuss three` of the most widely-`used` `classifiers`: \n",
    "1. __`logistic regression`__, \n",
    "2. __`linear discriminant analysis`__, and\n",
    "3. __`K-nearest neighbors`__. \n",
    "\n",
    "We will focus on below topics in other chapters.\n",
    "\n",
    "1. generalized additive models, \n",
    "2. trees, \n",
    "3. random forests, and \n",
    "4. boosting, and \n",
    "5. support vector machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 An Overview of Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The classification problems happenes even more than regression problems.\n",
    "    \n",
    "__For Example__\n",
    "1. A person arrives at the emergency room with a `set of symptoms` that could `possibly be attributed` to `one of three` `medical conditions`.<br>__Which of the three conditions does the individual have?__\n",
    "\n",
    "2. An `online banking service` must be able to determine whether or not a `transaction being performed` on the `site is fraudulent`, on the `basis of the user’s IP address`, `past transaction history`, and `so forth`.\n",
    "\n",
    "3. On the basis of `DNA sequence data` for a `number of patients with and without a given disease`, a `biologist` would like to `figure out` which `DNA mutations are deleterious` (disease-causing) and `which are not`.\n",
    "\n",
    "&bullet; Just as in the `regression setting`, in the `classification setting` we have a `set of training observations` $(x_{1} , y_{1} ), \\dots , (x_{n} , y_{n} )$ that `we can use` to `build a classifier`. \n",
    "\n",
    "&bullet; We want `our classifier` to `perform well not only on` the `training data`, but also on `test observations` that were `not used` to `train the classifier`.\n",
    "\n",
    "<a id=\"Figure4.1\"></a>\n",
    "![image.png](Figures/Figure4.1.png)\n",
    ">__FIGURE 4.1__. The `Default data set`. \n",
    "\n",
    ">__Left:__ The `annual incomes` and `monthly credit card balances` of a `number of individuals`. \n",
    "<br>The individuals who defaulted on their credit card payments are shown in orange, and those who did not are shown in blue. \n",
    "\n",
    ">__Center:__ Boxplots of `balance` as a `function of default status`. \n",
    "\n",
    ">__Right:__ Boxplots of `income` as a `function of default status`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Why Not Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stated that Linear Regression is not appropriate in the case of a `Qualititive response`.\n",
    "\n",
    "___Why Not?___\n",
    "\n",
    "Suppose that we are `trying to predict` the `medical condition` of a `patient`\n",
    "in the `emergency room on the basis` of `her symptoms`. \n",
    "\n",
    "In this simplified example, there are `three possible diagnoses`: __stroke , drug overdose , and\n",
    "epileptic seizure__. \n",
    "\n",
    "We `could consider encoding these values` as a `quantitative response variable`, $Y$ , as follows:\n",
    "\n",
    "![image.png](Figures/FormulaE1.png)\n",
    "\n",
    "Using this `coding`, `least squares` could be `used to fit a linear regression model` to `predict` $Y$ on the `basis of a set of predictors` $X_{1} , \\dots , X_{p}$ . \n",
    "\n",
    "Unfortunately, this `coding implies` an `ordering on the outcomes`, `putting drug overdose` in `between stroke and epileptic seizure` , and `insisting` that the `difference between stroke and drug overdose` is the `same as the difference between drug overdose and epileptic seizure`. \n",
    "\n",
    "In practice there is `no particular reason` that `this needs to be the case`. \n",
    "\n",
    "For instance, one `could choose` an `equally reasonable coding`,which would `imply` a `totally different relationship` among the `three conditions`. \n",
    "\n",
    "Each of these `codings` would `produce` fundamentally `different linear\n",
    "models` that would `ultimately lead` to `different sets` of `predictions on test observations`\n",
    "\n",
    "![image.png](Figures/FormulaE2.png)\n",
    "\n",
    "If the response variable’s values did take on a natural ordering, such as\n",
    "mild, moderate, and severe, and we felt the gap between mild and moderate\n",
    "was similar to the gap between moderate and severe, then a 1, 2, 3 coding\n",
    "would be reasonable. Unfortunately, in general there is no natural way to\n",
    "convert a qualitative response variable with more than two levels into a\n",
    "quantitative response that is ready for linear regression.\n",
    "\n",
    "\n",
    "For a ___`binary (two level) qualitative response`___, the `situation is better`. \n",
    "For instance, perhaps there are `only two possibilities` for the `patient’s medical condition`: __stroke and drug overdose__. \n",
    "\n",
    "We could then `potentially use`\n",
    "the `dummy variable approach` from <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%203/Chapter%203.ipynb#3.3.1-Qualitative-Predictors\">Section 3.3.1</a> to code the response as follows:\n",
    "\n",
    "![image.png](Figures/FormulaE3.png)\n",
    "\n",
    "We could then `fit a linear regression` to this `binary response`, and `predict drug overdose` if $\\hat{y} > 0.5$ and `stroke` otherwise. \n",
    "\n",
    "In the `binary case` it is `not hard` to `show that` even `if we flip` the above `coding`, `linear regression` will produce the same `final predictions`.\n",
    "\n",
    "For a `binary response` with a $ 0/1 $ `coding` as above, `regression` by `least squares does make sense`; it can be shown that the $X \\hat{\\beta}$ obtained using `linear\n",
    "regression` is `in fact` an `estimate of` $Pr( drug\\ overdose |X)$ in this `special\n",
    "case`. \n",
    "However, if we use `linear regression`, some of `our estimates` might be `outside` the $[0, 1]$ `interval` (see <a href=\"#Figure4.2\">Figure 4.2</a>), `making them hard` to `interpret as probabilities`! \n",
    "\n",
    "Nevertheless, the `predictions provide` an `ordering` and `can be interpreted` as `crude probability estimates`. \n",
    "\n",
    "Curiously, it `turns out` that the `classifications` that we `get` if `we use linear regression` to `predict a binary response` will be the same as for the ___`linear discriminant analysis (LDA)`___\n",
    "\n",
    "However, the `dummy variable` approach `cannot` be `easily extended` to `accommodate qualitative responses` with `more than two levels`. \n",
    "For these `reasons`, it is `preferable` to `use` a `classification method` that is `truly suited`\n",
    "for `qualitative response values`, such as the `ones presented next`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the `Default data set`, where the `response default` falls into `one of two categories`, __`Yes or No`__ . \n",
    "\n",
    "Rather than `modeling` this `response` $Y$ `directly`, `logistic regression models` the `probability` that $Y$ belongs to a `particular category`.\n",
    "\n",
    "<a id=\"Figure4.2\"></a>\n",
    "![image.png](Figures/Figure4.2.png)\n",
    ">__FIGURE 4.2:__ `Classification` using the `Default data`.\n",
    "\n",
    ">__Left:__ `Estimated probability` of `default` using `linear regeression`. \n",
    "<br>Some `estimated probabilities` are `negative`! \n",
    "<br>The `orange ticks indicate` the __0/1 values__ `coded for default`(No or Yes).\n",
    "\n",
    ">__Right:__ `Predicted probailities` of `default` using `logistic regression`. \n",
    "<br>All `probabilities lie` between __0 and 1__.\n",
    "\n",
    "For the Default data, logistic regression models the probability of defaulor example, the probability of default given balance can be written as\n",
    "\n",
    "<font size=5><center>$Pr(default = Yes|balance ).$</center></font>\n",
    "\n",
    "The values of $Pr(default = Yes|balance )$, which we `abbreviate` $p$(balance ), will `range between` __0 and 1__. \n",
    " \n",
    "Then for `any given value of balance`, a `prediction` can be made for `default`. \n",
    "\n",
    "For __example__, `one might predict` __default = Yes__ for `any individual` for `whom` $p(balance) > 0.5$.\n",
    "\n",
    "Alternatively, if a `company wishes` to be `conservative in predicting individuals` who\n",
    "are at `risk for default`, then `they may choose` to `use a lower threshold`, such\n",
    "as $p(balance) > 0.1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 The Logistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How should we model the `relationship between` $p(X) = Pr(Y = 1|X)$ and\n",
    "$X$?__ \n",
    "<br>(For convenience we are using the generic 0/1 coding for the response).\n",
    "\n",
    "In <a href=\"#4.2-Why-Not-Linear-Regression?\">Section 4.2</a> we `talked` of `using` a `linear regression model` to represent `these probabilities`:\n",
    "\n",
    "<a id=\"Formula4.1\"></a>\n",
    "<font size=5><center>$p(X) = β0 + β1 X$</center></font>.\n",
    "\n",
    "If we use `this approach` to `predict` __default=Yes__ using `balance`, then we `obtain the model` shown in the `left-hand` panel of <a href=\"#Figure4.2\">Figure 4.2</a>. \n",
    "\n",
    "Here `we see` the `problem with this approach`: for `balances close to zero` we `predict` a __`negative probability of default`__; <br>if we were to `predict for very large balances`, we would `get values` __`bigger than 1`__. \n",
    "\n",
    "These `predictions` are `not sensible`, since `of course` the `true probability of default`, `regardless` of `credit card balance`, must `fall` between __0 and 1__. \n",
    "\n",
    "This `problem` is `not unique` to the `credit default data`. \n",
    "\n",
    "Any time a `straight line` is `fit to a binary response` that is `coded as 0 or 1`, in `principle` we can `always predict` $p(X) < 0$ for `some values` of $X$ and $p(X) > 1$ for `others` (unless the range of $X$ is `limited`).\n",
    "\n",
    "__To avoid this problem__, we `must model` $p(X)$ `using a function` that gives\n",
    "`outputs between 0 and 1` for `all values` of $X$. \n",
    "\n",
    "Many `functions meet` this `description`. \n",
    "\n",
    "In __`logistic regression`__, we use the __`logistic function`__,\n",
    "\n",
    "<a id=\"Formula4.2\"></a>\n",
    "<font size=5><center> $ p(X) = \\frac{e^{\\beta_{0}+\\beta_{1}X}}{1+e^{\\beta_{0}+\\beta_{1}X}} $ </center></font>\n",
    "\n",
    "\n",
    "To `fit` the `model` <a href=\"#Formula4.2\">(4.2)</a>, we `use a method` called __`maximum likelihood`__, which\n",
    "we discuss in the next section. \n",
    "\n",
    "The _right-hand panel_ of <a href=\"#Figure4.2\">Figure 4.2</a> illustrates the `fit` of the `logistic regression` model to the `Default data`. \n",
    "\n",
    "Notice that for `low balances` we now `predict` the `probability` of `default` as `close to`, but never `below`, __zero__. \n",
    "\n",
    "Likewise, for `high balances` we `predict` a `default probability` close to, but `never above`, __one__. \n",
    "\n",
    "The ___`logistic function`___ will always produce an `S-shaped curve` of this `form`, and so `regardless of the value` of $X$, we will obtain a `sensible prediction`. \n",
    "\n",
    "We also see that the `logistic model` is `better` able `to capture the range` of `probabilities` than is the `linear regression model` in the _left-hand plot_. \n",
    "\n",
    "The `average fitted probability` in `both cases` is $0.0333$ (averaged over the training data), which is the `same as the overall proportion` of `defaulters` in the `data set`.\n",
    "\n",
    "After a `bit of manipulation` of <a href=\"#Formula4.2\">(4.2)</a>, we find that\n",
    "\n",
    "<a id=\"Formula4.3\"></a>\n",
    "<font size=5><center> $ \\frac{p(X)}{1 - p(X)} = e^{\\beta_{0}+\\beta_{1}X}$ </center></font>\n",
    "\n",
    "The `quantity` <font size=5><center> $ \\frac{p(X)}{1 - p(X)}$ </center></font> is called the __`odds`__, and can `take on any value` between $0$ and $\\infty$. \n",
    "\n",
    "`Values` of the `odds close` to $0$ and $\\infty$ `indicate very low` and `very high probabilities` of `default`, respectively. \n",
    "\n",
    "__For example__, on `average`\n",
    "1 in 5 people with `an odds` of $\\frac{1}{4}$ will `default`, since $p(X) = 0.2$ `implies` an odds of $\\frac{0.2}{1-0.2} = frac{1}{4}$. \n",
    "\n",
    "Likewise on `average nine out of every ten people` with `odds of`  9 will default, since $p(X) = 0.9$ implies an `odds` of $\\frac{0.9}{1−0.9} = 9$.\n",
    "\n",
    "`Odds` are `traditionally used instead of probabilities` in `horse-racing`, since they `relate more naturally` to the `correct betting strategy`.\n",
    "\n",
    "By taking the `logarithm of both sides` of <a href=\"#Formula4.3\">(4.3)</a>, we arrive at\n",
    "\n",
    "<a id=\"Formula4.4\"></a>\n",
    "<font size=5><center> $ log \\Big(\\frac{p(X)}{1 - p(X)}\\Big) = \\beta_{0}+\\beta_{1}X$ </center></font>\n",
    "\n",
    "The _left-hand side_ is called the __`log-odds or logit`__. \n",
    "\n",
    "We see that the __`logistic regression model`__ <a href=\"#Formula4.2\">(4.2)</a> has a `logit` that is `linear` in $X$.\n",
    "\n",
    "__[Read More On Page No 132&133]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Estimating the Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `coefficients` $\\beta_{0}$ and $\\beta_{1}$ in <a href=\"#Formula4.2\">(4.2)</a> are __`unknown`__, and `must be estimated` based on the `available training data`. \n",
    "\n",
    "In <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%203/Chapter%203.ipynb\">Chapter 3</a>, we used the __`least squares approach`__ to `estimate the unknown linear regression coefficients`. \n",
    "\n",
    "Although we could use (non-linear) `least squares` to `fit the model` <a href=\"#Formula4.4\">(4.4)</a>, the `more\n",
    "general method` of ___`maximum likelihood`___ is `preferred`, since it has `better statistical properties`. \n",
    "\n",
    "The `basic intuition` behind using `maximum likelihood` to `fit a logistic regression model` is as follows: \n",
    "we seek estimates for $\\beta_{0}$ and $\\beta_{1}$ such that the `predicted probability` $\\hat{p}(x_{i})$ of `default` for `each individual`, using <a href=\"#Formula4.2\">(4.2)</a>, corresponds as `closely as possible` to the `individual’s observed default status`. \n",
    "\n",
    "In __other words__, we `try to find` $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ such that `plugging these estimates into the model` for $p(X)$, given in <a href=\"#Formula4.2\">(4.2)</a>, yields a `number close to one` for `all individuals who defaulted`, and a `number close to zero`\n",
    "for `all individuals who did not`. \n",
    "\n",
    "This `intuition` can be `formalized` using a\n",
    "`mathematical equation` called a ___`likelihood function`___:\n",
    "\n",
    "<a id=\"Formula4.5\"></a>\n",
    "![image.png](Figures/Formula4.5.png)\n",
    "\n",
    "The estimates $\\hat{\\beta}_{0}$ and $\\hat{\\beta}_{1}$ are `chosen` to `maximize this likelihood function`.\n",
    "\n",
    "\n",
    "Maximum likelihood is a `very general approach` that is `used to fit many of the non-linear models` that we `examine throughout` this book. \n",
    "\n",
    "In the `linear regression setting`, the `least squares approach` is in `fact a special case` of __`maximum likelihood`__. \n",
    "\n",
    "The `mathematical details` of `maximum likelihood` are beyond the `scope of this book`. \n",
    "\n",
    "However, in general, `logistic regression` and `other models` can be `easily fit using a statistical software package` such as Python/R , and so we `do not need` to `concern ourselves` with the `details` of the `maximum likelihood fitting procedure`.\n",
    "\n",
    "<a href=\"#Table4.1\">Table 4.1</a> shows the `coefficient estimates` and `related information` that `result from fitting` a `logistic regression model` on the `Default data` in `order to predict` the `probability` of __`default = Yes` using `balance`.__ \n",
    "\n",
    "We see that $\\hat{\\beta_{1}} = 0.0055$; this `indicates` that `an increase in balance is associated with` an `increase in the probability of default` . \n",
    "\n",
    "To be `precise`, a `one-unit increase` in `balance` is `associated with` an `increase in the log odds` of `default` by 0.0055 units.\n",
    "\n",
    "<a id=\"Table4.1\"></a>\n",
    "\n",
    "|     &nbsp;| Coefficient | Std. error | Z-Statistic | P-Value  |\n",
    "| --------- | ----------- | ---------- | ----------- | -------- |\n",
    "| Intercept | -10.6513    | 0.3612     | -29.5       | < 0.0001 |\n",
    "| Balance   | 0.0055      | 0.0002     | 24.9        | < 0.0001 |\n",
    "\n",
    ">__TABLE 4.1.__ For the `Default data`, estimated `coefficients` of the `logistic regression model` that `predicts the probability of default using balance`. \n",
    "<br>A `one-unit increase` in `balance` is `associated with` an `increase in the log odds` of `default` by\n",
    "0.0055 `units`.\n",
    "\n",
    "Important Things inn table which are almost same like Linear Regression.\n",
    "1. __Std.Error__ - We can measure accuracy of coefficient estimate by computing their Standard-Error.\n",
    "2. __Z-Statistics(t-statistics)__ - Checking Null Hypothesis. in above example.\n",
    "    __Z-statistics__ means - Probability of default does not depend on balance.\n",
    "    \n",
    "3. __P-Value__ is tiny so we can reject $H_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `coefficients have been estimated`, it is a `simple matter to compute` the `probability of default` for `any given credit card balance`. \n",
    "\n",
    "__For example__, using the `coefficient estimates` given in <a href=\"#Table4.1\">Table 4.1</a>, we `predict` that the `default probability` for an `individual with a balance of` &dollar;1, 000 is\n",
    "\n",
    "<font size=5>\n",
    "    <center>$ \\hat{p}(X) = \\frac{e^{\\beta_{0}+\\beta_{1}X}}{1 + e^{\\beta_{0}+\\beta_{1}X}} = \\frac{e^{− 10.6513 + 0.0055 * 1,000}}{1 + e^{− 10.6513 + 0.0055 * 1,000}} = 0.00576,$\n",
    "    </center>\n",
    "</font>\n",
    "\n",
    "which is `below` $1\\%$. \n",
    "\n",
    "In contrast, the `predicted probability of default` for `an individual with a balance of` &dollar;2, 000 is `much higher`, and `equals` 0.586 or 58.6 $\\%$\n",
    "\n",
    "\n",
    "One can use `qualitative predictors` with the `logistic regression model` using the `dummy variable approach` from <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%203/Chapter%203.ipynb#3.3.1-Qualitative-Predictors\">Section 3.3.1</a>. \n",
    "\n",
    "As an __example__, the `Default data set contains` the `qualitative variable student`. \n",
    "\n",
    "To `fit the model` we simply `create a dummy variable` that `takes on a value` of __1 for `students` and 0 for `non-students`__.\n",
    "\n",
    "The `logistic regression model` that results\n",
    "from `predicting probability` of default from `student status` can be `seen` in <a href=\"#Table4.2\">Table 4.2.</a>\n",
    "\n",
    "The `coefficient associated` with the `dummy variable` is `positive`,\n",
    "\n",
    "<a id=\"Table4.2\"></a>\n",
    "\n",
    "| &nbsp;        | Coefficient | Std. error | Z-Statistic | P-Value  |\n",
    "| ------------- | ----------- | ---------- | ----------- | -------- |\n",
    "| Intercept     | -3.5041     | 0.0707     | -49.55      | < 0.0001 |\n",
    "| Student [Yes] | 0.4049      | 0.1150     | 3.52        | 0.0004   |\n",
    "\n",
    ">__TABLE 4.2.__ For the `Default data`, estimated `coefficients of the logistic regression model` that `predicts the probability` of `default` using `student status`. \n",
    "<br>Student status is `encoded as a dummy variable`, with a `value of 1` for a `student` and a `value of 0` for a `non-student`, and `represented` by the `variable student[Yes]` in the `table`.\n",
    "\n",
    "and the associated __`p-value`__ is `statistically significant`. \n",
    "\n",
    "This `indicates` that `students tend to have higher default probabilities` than `non-students`:\n",
    "\n",
    "<font size=5><center> $ \\hat{P_{r}} ( default = Yes | student = Yes) = \\frac{e^{− 3.5041 + 0.4049 * 1}}{1 + e^{− 3.5041 + 0.4049 * 1}} =0.0431 $</center></font>\n",
    "\n",
    "<font size=5><center> $ \\hat{P_{r}} ( default = Yes | student = No) = \\frac{e^{− 3.5041 + 0.4049 * 0}}{1 + e^{− 3.5041 + 0.4049 * 0}} =0.0292 $</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider the `problem of predicting` a `binary response` using `multiple predictors`. \n",
    "\n",
    "By `analogy` with the `extension from` __simple to multiple linear regression__ in <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%203/Chapter%203.ipynb#3-Linear-Regression\">Chapter 3</a>, we can `generalize` <a href=\"#Formula4.4\">(4.4)</a> as follows:\n",
    "\n",
    "<a id=\"Formula4.6\"></a>\n",
    "<font size=5><center> $ log \\Big(\\frac{p(X)}{1 - p(X)}\\Big) = \\beta_{0}+\\beta_{1}X_{1}+ \\dots + \\beta_{p}X_{p}$ </center></font>\n",
    ">where $X = (X_{1} , \\dots , X_{p})$ are $p$ `predictors`. \n",
    "\n",
    "<a href=\"#Formula4.6\">Equation 4.6</a> can be `rewritten as`\n",
    "<a id=\"Formula4.7\"></a>\n",
    "<font size=5><center> $ p(X) = \\frac{e^{\\beta_{0}+\\beta_{1}X_{1}+\\dots+\\beta_{p}X_{p}}}{1 +e^{\\beta_{0}+\\beta_{1}X_{1}+\\dots+\\beta_{p}X_{p}}}$ </center></font>\n",
    "\n",
    "Just as in <a href=\"#4.3.2-Estimating-the-Regression-Coefficients\">Section 4.3.2</a>, we `use` the `maximum likelihood method` to estimate $ \\beta_{0},\\beta_{1},\\dots,\\beta_{p}$.\n",
    "\n",
    "| &nbsp;        | Coefficient | Std. error | Z-Statistic | P-Value  |\n",
    "| ------------- | ----------- | ---------- | ----------- | -------- |\n",
    "| Intercept     | -10.8690    | 0.4923     | −22.08      | < 0.0001 |\n",
    "| balance       | 0.0057      | 0.0002     | 24.74       | < 0.0001 |\n",
    "| income        | 0.0030      | 0.0082     | 0.37        | 0.7115   |\n",
    "| Student [Yes] | 0.6468      | 0.0.2362   | −2.74       | 0.0062   |\n",
    "\n",
    "This `simple example illustrates` the `dangers` and `subtleties` associated with `performing regressions involving only a single predictor` when other `predictors may also be relevant`. \n",
    "\n",
    "As in the `linear regression setting`, the `results obtained using` `one predictor may be quite different` from those `obtained using multiple predictors`, especially `when there is correlation among the predictors`. \n",
    "\n",
    "\n",
    "In general, the `phenomenon` seen in <a href=\"#Figure4.3\">Figure 4.3</a> is `known as` ___`confounding`___.\n",
    "\n",
    "<a id=\"Figure4.3\"></a>\n",
    "![image.png](Figures/Figure4.3.png)\n",
    "\n",
    "\n",
    "By `substituting estimates` for the `regression coefficients` from <a href=\"#Table4.3\">Table 4.3</a>\n",
    "into <a href=\"#Formula4.7\">(4.7)</a>, we can `make predictions`. \n",
    "\n",
    "__For example__, a `student with a credit\n",
    "card balance` of &dollar;1, 500 and `an income` of &dollar;40, 000 has an `estimated probability` of default of\n",
    "\n",
    "<a id=\"Formula4.8\"></a>\n",
    "<font size=5><center> $ \\hat{p}(X) = \\frac{e^{-10.869+0.00574*1500+ 0.003*40 - 0.6468*1}}{1 +e^{-10.869+0.00574*1500+ 0.003*40 - 0.6468*1}} = 0.058$ </center></font>\n",
    "\n",
    "A `non-student with the same balance` and `income` has an `estimated probability` of default of\n",
    "\n",
    "<a id=\"Formula4.8\"></a>\n",
    "<font size=5><center> $ \\hat{p}(X) = \\frac{e^{-10.869+0.00574*1500+ 0.003*40 - 0.6468*0}}{1 +e^{-10.869+0.00574*1500+ 0.003*40 - 0.6468*0}} = 0.105$ </center></font>\n",
    "\n",
    "(Here we multiply the income coefficient estimate from Table 4.3 by 40,\n",
    "rather than by 40,000, because in that table the model was fit with income\n",
    "measured in units of $1, 000.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 Logistic Regression for >2 Response Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&bullet;We sometimes wish to classify a response variable that has more than two classes. \n",
    "\n",
    "__For example__, in <a href=\"#4.2-Why-Not-Linear-Regression?\">Section 4.2</a> we had `three categories` of `medical condition in the emergency room`: __stroke , drug overdose , epileptic seizure__.\n",
    "\n",
    "In this `setting`, we `wish to model` both\n",
    "<br>$Pr(Y = stroke |X)$ and \n",
    "<br>$Pr(Y = drug\\ overdose |X)$, with `the remaining` \n",
    "<br>$Pr(Y = epileptic\\ seizure |X) = 1 − Pr(Y = stroke |X) − Pr(Y = drug\\ overdose |X)$. \n",
    "\n",
    "The `two-class` `logistic regression models` discussed in the `previous sections` have `multiple-class\n",
    "extensions`, but in `practice` they `tend not to be used` all that `often`. \n",
    "\n",
    "One of the `reasons` is that `the method we discuss` in the `next section`, `discriminant analysis`, is popular for `multiple-class classification`. \n",
    "\n",
    "So we do not go into the details of `multiple-class logistic regression` here, but `simply note that` such `an approach is possible`, and that software for it is `available` in Python Or R ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Logistic regression`__ involves `directly modeling` $Pr(Y = k|X = x)$ using the `logistic function`, given by <a href=\"#Formula4.7\">(4.7)</a> for the `case of two response classes`. \n",
    "\n",
    "In `statistical jargon`, we `model` the `conditional distribution` of `the response` $Y$ , `given the predictor(s)` $X$. \n",
    "\n",
    "We `now consider` an `alternative` and `less direct approach` to `estimating these probabilities`. \n",
    "\n",
    "In this `alternative approach`, we `model` the `distribution of the predictors` $X$ `separately` in `each of the\n",
    "response classes` (i.e. given $Y$ ), and then `use` __Bayes’ theorem__ to `flip these around` into `estimates for` $Pr(Y = k|X = x)$. \n",
    "\n",
    "When these `distributions` are `assumed to be normal`, it `turns out that` the `model is very similar` in `form\n",
    "to logistic regression`.\n",
    "\n",
    "__Why do we need another method, when we have logistic regression?__\n",
    "\n",
    "There are several reasons:\n",
    "- When the `classes` are `well-separated`, the `parameter estimates` for the `logistic regression model` are `surprisingly unstable`.  <br>___`Linear discriminant analysis`___ does `not suffer from this problem`.\n",
    "\n",
    "\n",
    "- If $n$ is `small` and the `distribution of the predictors` $X$ is `approximately normal` in `each of the classes`, the __`linear discriminant model`__ is `again more stable` than the `logistic regression model`.\n",
    "\n",
    "\n",
    "- As mentioned in <a href=\"#4.3.5-Logistic-Regression-for-%3E2-Response-Classes\">Section 4.3.5</a>, `linear discriminant analysis` is `popular when` we `have more than two response classes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Using Bayes’ Theorem for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Suppose` that we `wish to classify` an `observation` into `one of` $K$ `classes`, \n",
    ">where $K \\ge 2$. \n",
    "\n",
    "In `other words`, the `qualitative response variable` $Y$ can `take on` $K$ `possible distinct` and `unordered values`. \n",
    "\n",
    "Let $\\pi_{k}$ represent the `overall` or `prior probability` that a `randomly chosen observation comes from the` $kth$ `class`; this is the `probability` that a given `observation` is `associated with` the $kth$ `category of the response variable` $Y$ . \n",
    "\n",
    "Let $f_{k}( x ) \\equiv Pr(X = x|Y = k)$ `denote` the ___`density function`___ of $X$ for an `observation` that `comes from` the $kth$ class.\n",
    "\n",
    "In `other words`, $f_{k}(x)$ is `relatively large` __if there is a high probability__ that an `observation in the` $kth$ `class` has $X \\approx x$, and $f_{k}(x)$ is `small` if it is `very unlikely` that an `observation` in the $kth$ `class` has $X \\approx x$. \n",
    "\n",
    "Then ___`Bayes’ theorem`___ states that\n",
    "\n",
    "<a id=\"Formula4.10\"></a>\n",
    "<font size=5><center> $ Pr(Y = k|X = x) = \\frac{\\pi_{k}f_{k}(x)}{\\sum_{l=1}^{K}\\pi_{l}f_{l}(x)} $ </center></font>\n",
    "\n",
    "In accordance with our `earlier notation`, we will `use` the `abbreviation` $p_{k}(X) = Pr(Y = k|X)$. \n",
    "\n",
    "This `suggests` that `instead of directly computing` p_{k}(X) as in <a href=\"#4.3.1-The-Logistic-Model\">Section 4.3.1</a>, we can `simply plug in estimates` of $π_{k}$ and $f_{k}(X)$ into <a href=\"#Formula4.10\">(4.10)</a>. \n",
    "\n",
    "In general, `estimating` $π_{k}$ is `easy if we have a random sample of` $Y_{s}$ `from the population`: __we simply compute the fraction of the training observations that belong to the kth class.__\n",
    "\n",
    "However, `estimating` $f_{k}(X)$ tends to be `more challenging`, `unless` we `assume some simple forms` for `these\n",
    "densities`. \n",
    "\n",
    "We `refer to` $p_{k}(x)$ as the ___`posterior`___ __probability__ that an observation $X = x$ `belongs` to the $kth$ `class`. \n",
    "\n",
    "That is, it is the `probability that` the `observation belongs` to the $kth$ class, given the `predictor value` for `that observation`.\n",
    "\n",
    "\n",
    "We know from <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Chapter%202.ipynb\">Chapter 2</a> that the `Bayes classifier`, which `classifies` an `observation` to `the class` for which $p_{k}(X)$ is `largest`, has the `lowest possible error rate` out of `all classifiers`. \n",
    "\n",
    "(This is of course only true if the terms in (4.10) are all correctly specified.) \n",
    "Therefore, if we `can find a way` to `estimate` $f_{k}(X)$, then `we can develop a classifier` that `approximates the Bayes classifier`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Linear Discriminant Analysis for p = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, __assume that p = 1__—that is, we have only `one predictor`. \n",
    "\n",
    "We would `like to obtain` an `estimate for` $f_{k}(x)$ that `we can plug` into <a href=\"#Formula4.10\">(4.10)</a> in `order to estimate` $p_{k}(x)$. \n",
    "\n",
    "We will then `classify an observation` to `the class` for which $p_{k}(x)$ is `greatest`. \n",
    "\n",
    "In order to `estimate` $f_{k}(x)$, we will first `make some assumptions` about `its form`.\n",
    "\n",
    "\n",
    "Suppose we `assume that` $f_{k}(x)$ is ___`normal` or `Gaussian`___. \n",
    "\n",
    "In the `one-dimensional setting`, the `normal density takes` the `form`\n",
    "\n",
    "<a id=\"Formula4.11\"></a>\n",
    "<font size=5><center> $ f_{k}(x) =  \\frac{1}{\\sqrt{2\\pi\\sigma_{k}}} exp \\Big( - \\frac{1}{2\\sigma_{k}^2} (x - \\mu_{k})^2 \\Big) $ </center></font>\n",
    ">where $\\mu_{k}$ and $\\sigma_{k}^2$ are the `mean and variance parameters` for the $kth$ class.\n",
    "\n",
    "For now, let us `further assume that` $\\sigma_{1}^2 = \\dots = \\sigma_{K}$: that is, __there is a shared variance term across all K classes, which for simplicity we can denote by $\\sigma^2$.__ \n",
    "\n",
    "Plugging <a href=\"#Formula4.11\">(4.11)</a> into <a href=\"#Formula4.10\">(4.10)</a>, we find that\n",
    "\n",
    "<a id=\"Formula4.2\"></a>\n",
    "<font size=5><center> $ P_{k}(x) = \\frac{\\pi_{k}  \\frac{1}{\\sqrt{2\\pi\\sigma}} exp \\Big( - \\frac{1}{2\\sigma^2} (x - \\mu_{k})^2 \\Big)}{ \\sum_{l=1}^{K} \\pi_{l}  \\frac{1}{\\sqrt{2\\pi\\sigma}} exp \\Big( - \\frac{1}{2\\sigma^2} (x - \\mu_{l})^2 \\Big) }$</center></font>\n",
    "\n",
    "__(Note that in (4.12), $\\pi_{k}$ denotes the `prior probability` that an `observation belongs to` the $kth$ class, not to be confused with $\\pi \\approx 3.14159$, the `mathematical constant`.)__\n",
    "\n",
    "<a id=\"Figure4.4\"></a>\n",
    "![image.png](Figures/Figure4.4.png)\n",
    ">__FIGURE 4.4.__ \n",
    "<br>__Left:__ Two `one-dimensional` normal `density functions` are shown.\n",
    "<br>The `dashed vertical line` represents the `Bayes decision boundary`. \n",
    "\n",
    ">__Right:__ 20 `observations were drawn` from `each of the two classes`, and are shown as `histograms`.\n",
    "<br>The `Bayes decision boundary` is again shown as a `dashed vertical line`. \n",
    "<br>The `solid vertical line` represents the `LDA decision boundary estimated` from the `training data`.\n",
    "\n",
    "The `Bayes classifier` involves assigning an observation $X = x$ to the `class` for which <a href=\"#Formula4.12\">(4.12)</a> is `largest`. \n",
    "\n",
    "`Taking the log` of <a href=\"#Formula4.12\">(4.12)</a> and `rearranging` the terms, it is `not hard to show` that this is `equivalent to\n",
    "assigning the observation` to `the class` for which \n",
    "\n",
    "<a id=\"Formula4.13\"></a>\n",
    "<font size=5><center> $ \\delta_{k}(x) = x * \\frac{\\mu_{k}}{\\sigma^2} - \\frac{\\mu_{k}^2}{2\\sigma^2} + log(\\pi_{k}) $</center></font>\n",
    "\n",
    "is `largest`. \n",
    "\n",
    "For `instance`, if $K = 2$ and $\\pi_{1} = \\pi_{2}$  , then the `Bayes classifier assigns an observation to class` __1__ ; if $2x (\\mu_{1} - \\mu_{2}) \\gt \\mu_{1}^2 − \\mu_{2}^2$ , and to `class` __2__ otherwise. \n",
    "\n",
    "In this case, the __`Bayes decision boundary`__ corresponds to the `point` where\n",
    "\n",
    "<a id=\"Formula4.14\"></a>\n",
    "<font size=5><center> $  x = \\frac{ \\mu_{1}^2 - \\mu_{2}^2 }{ 2(\\mu_{1} - \\mu_{2})} = \\frac{\\mu_{1} + \\mu_{2}}{2} $</center></font>\n",
    "\n",
    "An `example` is shown in the `left-hand panel` of <a href=\"#Figure4.4\">Figure 4.4</a>. \n",
    "\n",
    "The two `normal density functions` that are displayed, $f_{1}(x)$ and $f_{2}(x)$, represent `two distinct classes`. \n",
    "\n",
    "The __`mean and variance`__ parameters for the `two density functions` are $\\mu_{1} = −1.25, \\mu_{2} = 1.25,$ and $\\sigma_{1}^2 = \\sigma_{2}^2 = 1$. \n",
    "\n",
    "The `two densities overlap`, and so given that $X = x$, there is `some uncertainty` about the `class` to which the `observation belongs`. \n",
    "\n",
    "If we `assume` that an `observation is equally likely` to come `from either class`—that is, $\\pi_{1}= \\pi_{1} = 0.5$—then by `inspection` of <a href=\"#Formula4.14\">(4.14)</a>, we see that the `Bayes classifier` assigns the `observation` to `class 1` __if $x \\lt 0$ and `class 2` otherwise__. \n",
    "\n",
    "Note that in this `case`, we can `compute` the `Bayes classifier` because `we know that` $X$ is `drawn` from a `Gaussian distribution` within `each class`, and `we know all of the parameters involved`. \n",
    "\n",
    "In a `real-life situation`, we `are not able` to `calculate the Bayes classifier`. \n",
    "\n",
    "In `practice`, even if we are `quite certain` of our `assumption that` $X$ is `drawn from a Gaussian distribution` within `each class`, we `still have to estimate` the `parameters`$\\mu_{1},\\mu_{2}, \\dots , \\mu_{K}, \\pi_{1}, \\dots, \\pi_{K} $, and $\\sigma_{2}$. \n",
    "\n",
    "The ___`linear discriminant analysis (LDA)`___ `method` approximates the `Bayes classifier` by `plugging estimates` for $\\pi_{k}, \\mu_{k}$, and $\\sigma_{2}$ into <a href=\"#Formula4.13\">(4.13)</a>. \n",
    "\n",
    "In particular, the following estimates are used:\n",
    "\n",
    "<a id=\"Formula4.15\"></a>\n",
    "<font size=5><center> $ \\hat{\\mu}_{k} =  \\frac{1}{n_{k}} \\sum_{i:y_{i} = k}x_{i} $ </center></font>\n",
    "\n",
    "<font size=5><center> $ \\hat{\\sigma}^2 =  \\frac{1}{n - K} \\sum_{k=1}^{K} \\sum_{i:y_{i} = k}(x_{i} - \\hat{\\mu}_{k})^2 $ </center></font>\n",
    ">where \n",
    "<br>$n$ is the `total number of training observations`, and \n",
    "<br>$n_{k}$ is the `number of training observations in the kth class`.\n",
    "<br>The estimate for $μ_{k}$ is `simply the average of all the training observations from the kth class`, while $\\hat{\\sigma}^2$ can be seen as a `weighted average of the sample variances for each of the K\n",
    "classes`. \n",
    "\n",
    "Sometimes we `have knowledge of the class membership probabilities` $\\pi_{1},\\dots,\\pi_{K}$, which can be `used directly`. \n",
    "\n",
    "In the `absence of any additional information`, __`LDA estimates`__ $\\pi_{k}$ using the `proportion of the training observations` that `belong to the` $kth$ `class`. \n",
    "\n",
    "In other words,\n",
    "\n",
    "<a id=\"Formula4.16\"></a>\n",
    "<font size=5><center> $ \\hat{\\pi}_{k} = n_{k}/n.$</center></font>\n",
    "\n",
    "The `LDA classifier plugs` the `estimates` given in <a href=\"#Formula4.15\">(4.15)</a> and <a href=\"#Formula4.16\">(4.16)</a> into <a href=\"#Formula4.13\">(4.13)</a>, and `assigns an observation` $X = x$ to the `class` for which\n",
    "\n",
    "<a id=\"Formula4.17\"></a>\n",
    "<font size=5><center> $ \\hat{\\delta}_{k}(x) = x* \\frac{\\hat{\\mu}_{k}}{\\hat{\\sigma}^2} - \\frac{\\hat{\\mu}_{k}^2}{2\\hat{\\sigma}^2} + log(\\hat{\\pi}_{k})$</center></font>\n",
    "\n",
    "is `largest`. \n",
    "\n",
    "The `word linear` in the `classifier’s name stems` from the `fact`\n",
    "that the `discriminant functions` $\\hat{\\delta}_{k}(x)$ in <a href=\"Formula4.17\">(4.17)</a> are `linear functions` of $x$ (as opposed to a more `complex function` of $x$).\n",
    "\n",
    "The `right-hand panel` of <a href=\"#Figure4.4\">Figure 4.4</a> displays a `histogram` of a `random sample of 20 observations` from `each class`. \n",
    "\n",
    "To `implement LDA`, we began by `estimating` $\\pi_{k}, \\mu_{k}$, and $\\sigma^2$ using <a href=\"#Formula4.15\">(4.15)</a> and <a href=\"#Formula4.16\">(4.16)</a>. \n",
    "\n",
    "We then `computed the decision boundary`, shown as a `black solid line`, that `results` from assigning an `observation to the class` for which <a href=\"#Formula4.17\">(4.17)</a> is `largest`. \n",
    "\n",
    "__All points to the `left of this line will be assigned to the green class`__, while __points to the `right of this line are assigned to the purple class`.__\n",
    "\n",
    "In this `case`, since $n_{1} = n_{2} = 20$, we have $\\hat{\\pi}_{1} = \\hat{\\pi}_{2}$. \n",
    "\n",
    "As a `result`, the `decision boundary corresponds to the midpoint` between `the sample means` for the `two classes`, $(\\hat{\\mu}_{1} + \\hat{\\mu}_{2} )/2$. \n",
    "\n",
    "The `figure indicates` that the `LDA decision boundary` is `slightly to the left` of the `optimal Bayes decision boundary`, which `instead equals` $(\\hat{\\mu}_{1} + \\hat{\\mu}_{2} )/2 = 0$. \n",
    "\n",
    "__How well does the LDA classifier perform on this data?__\n",
    "\n",
    "Since this is `simulated data`, we can `generate a large number of test observations` in `order to compute` the `Bayes error rate` and the `LDA test error rate`. \n",
    "\n",
    "These are 10.6 % and 11.1 %, respectively. In other words, the `LDA classifier’s error rate` is only 0.5 % above the `smallest possible error rate`! This indicates that `LDA is performing pretty well` on this `data set`.\n",
    "\n",
    "<a id=\"Figure4.5\"></a>\n",
    "![image.png](Figures/Figure4.5.png)\n",
    "\n",
    " __FIGURE 4.5.__ `Two`$ \\sigma_{k}^2$ `multivariate Gaussian density functions` are shown, with\n",
    "$p = 2$. \n",
    "\n",
    ">__Left:__ The `two predictors are uncorrelated`. \n",
    "\n",
    ">__Right:__ The `two variables have a correlation of 0.7`.\n",
    "\n",
    "To `reiterate`, `the LDA classifier results` from assuming that the `observations` within `each class come from` a `normal distribution with` a `class-specific` __mean vector and a common variance $\\sigma^2$ , and plugging estimates__ for these `parameters` into the `Bayes classifier`. \n",
    "\n",
    "In <a href=\"\">Section 4.4.4</a>, we will `consider` a `less stringent set of assumptions`, by allowing the `observations` in the $kth$ `class to have a class-specific variance`,$ \\sigma_{k}^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3 Linear Discriminant Analysis for p >1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now `extend` the `LDA classifier` to the `case of multiple predictors`. \n",
    "\n",
    "To do this, we `will assume` that $X = (X_{1} , X_{2} , \\dots , X_{p} )$ is drawn from a ___`multivariate Gaussian (or multivariate normal) distribution`___, with a `class-specific` __mean vector and a common covariance matrix__. \n",
    "\n",
    "We begin with a brief `review of such a distribution`.\n",
    "\n",
    "\n",
    "The __`multivariate Gaussian distribution`__ assumes that `each individual predictor` follows a `one-dimensional normal distribution`, as in <a href=\"#Formula4.11\">(4.11)</a>, with `some correlation between` each `pair of predictors`. \n",
    "\n",
    "Two examples of `multivariate Gaussian distributions` with $p = 2$ are shown in <a href=\"#Figure4.5\">Figure 4.5</a>. \n",
    "\n",
    "The `height` of the `surface at any particular point` represents the `probability` that both $X_{1}$ and $X_{2}$ fall in a `small region` around that `point`. \n",
    "\n",
    "In either panel, if the `surface is cut along` the $X_{1}$ `axis` or `along` the $X_{2}$ `axis`, the `resulting cross-section` will have the `shape of a one-dimensional normal distribution`. \n",
    "\n",
    "The `left-hand panel` of <a href=\"#Figure4.5\">Figure 4.5</a> illustrates an `example` in `which` $Var(X_{1}) = Var(X_{2} )$ and $Cor(X_{1} , X_{2} ) = 0$; this `surface` has a `characteristic bell shape`. \n",
    "\n",
    "However, the `bell shape will be distorted` if the `predictors are correlated` or have `unequal variances`, as is illustrated in the `right-hand panel` of <a href=\"#Figure4.5\">Figure 4.5</a>. \n",
    "\n",
    "In this situation, the `base of the bell will have an elliptical`, rather than `circular`, shape.\n",
    "\n",
    "<a id=\"Figure4.6\"></a>\n",
    "![image.png](Figures/Figure4.6.png)\n",
    ">__FIGURE 4.6.__ An `example with three classes`. \n",
    "<br>The `observations` from `each class`\n",
    "are drawn from a __`multivariate Gaussian distribution with p = 2`__, with a `class-specific mean vector` and a `common covariance matrix`. \n",
    "\n",
    ">__Left:__ Ellipses that contain 95 % of the probability for each of the three classes are shown. \n",
    "<br>The dashed lines are the Bayes decision boundaries. \n",
    "\n",
    ">__Right:__ 20 observations were generated from each class, and the corresponding LDA decision boundaries are indicated using solid black lines. \n",
    "<br>The Bayes decision boundaries are once again shown as dashed lines.\n",
    "\n",
    "To `indicate` that a $p$__`-dimensional`__ `random variable` $X$ has a `multivariate Gaussian distribution`, we write $X \\approx N (\\mu, \\sum)$. \n",
    "\n",
    "Here $E(X) = \\mu$ is `the mean of` $X$ (a vector with $p$ components), and $Cov(X) = \\sum$ is the\n",
    "$p × p$ covariance matrix of $X$. \n",
    "\n",
    "Formally, the ___`multivariate Gaussian density`__ is `defined` as\n",
    "\n",
    "<a id=\"Formula4.18\"></a>\n",
    "<font size=5><center> $ f(x) = \\frac{1}{(2\\pi)^{p/2}|\\sum|^{1/2}} exp \\Big ( -frac{1}{2} (x-\\mu)^T \\sum_{}^{-1} ( x - \\mu)\\Big ) $ </center></font>\n",
    "\n",
    "In the `case` of $p > 1$ `predictors`, the `LDA classifier assumes` that the\n",
    "`observations` in the $kth$ `class are drawn` from a `multivariate Gaussian distribution` $N (\\mu_{k} , \\sum)$, where $\\mu_{k}$ is a `class-specific mean vector`, and $\\sum$ is a\n",
    "`covariance matrix` that is `common to all` $K$ `classes`. \n",
    "\n",
    "Plugging the `density function` for the $kth$ class, $f_{k} (X = x)$, into <a href=\"#Formula4.10\">(4.10)</a> and performing a `little bit of algebra` reveals that the `Bayes classifier` assigns an `observation` $X = x$\n",
    "to the `class` for which\n",
    "\n",
    "<a id=\"Formula4.19\"></a>\n",
    "<font size=5><center> $ \\delta_{k}(x) = x^T\\sum_{}^{-1}\\mu_{k} -\\frac{1}{2}\\mu_{k}^{T}\\sum_{}^{-1}\\mu_{k}+log\\pi_{k}$ </center></font>\n",
    "\n",
    "is `largest`. \n",
    "\n",
    "This is the `vector/matrix version` of <a href=\"#Formula4.13\">(4.13)</a>.\n",
    "\n",
    "An example is shown in the `left-hand panel` of <a href=\"#Figure4.6\">Figure 4.6</a>. \n",
    "\n",
    "Three `equally sized Gaussian classes` are shown with `class-specific` `mean vectors` and a `common covariance matrix`. \n",
    "\n",
    "The `three ellipses represent regions` that contain 95 % of the `probability` for `each of the three classes`. \n",
    "\n",
    "The `dashed lines`are the `Bayes decision boundaries`. \n",
    "\n",
    "In `other words`, they `represent` the `set\n",
    "of values` $x$ for `which` $\\delta_{k}(x) = \\delta_{\\lambda}(x)$; i.e.\n",
    "\n",
    "<a id=\"Formula4.20\"></a>\n",
    "<font size=5><center> $ x^{T}\\sum_{}^{-1} \\mu_{k} - \\frac{1}{2} \\mu_{k}^{T} \\sum_{}^{-1} \\mu_{k} = x^{T}\\sum_{}^{-1} \\mu_{k} - \\frac{1}{2} \\mu_{l}^{T} \\sum_{}^{-1} \\mu_{l} $ </center></font>\n",
    "\n",
    "for $k \\neq l$. (The $log\\pi_{k}$ term from <a href=\"#Formula4.19\">(4.19)</a> has `disappeared` because each of\n",
    "the `three classes` has the `same number of training observations`; i.e. $\\pi_{k}$ is the same for each class.) \n",
    "\n",
    "Note that there are `three lines representing` the `Bayes decision boundaries` because there are `three pairs of classes` among the `three classes`. \n",
    "\n",
    "That is, `one Bayes decision boundary separates` __class 1 from class 2__, `one separates` __class 1 from class 3__, and `one separates` __class 2 from class 3__. \n",
    "\n",
    "These three `Bayes decision boundaries divide the predictor space into three regions`. \n",
    "\n",
    "The `Bayes classifier` will `classify` an observation according to the `region` in which it is located.\n",
    "\n",
    "Once again, we need to estimate the unknown parameters μ 1 , . . . , μ K ,\n",
    "π 1 , . . . , π K , and Σ; the formulas are similar to those used in the one-\n",
    "dimensional case, given in (4.15). To assign a new observation X = x,\n",
    "LDA plugs these estimates into (4.19) and classifies to the class for which\n",
    "δ̂ k (x) is largest. Note that in (4.19) δ k (x) is a linear function of x; that is,\n",
    "the LDA decision rule depends on x only through a linear combination of\n",
    "its elements. Once again, this is the reason for the word linear in LDA.\n",
    "In the right-hand panel of Figure 4.6, 20 observations drawn from each of\n",
    "the three classes are displayed, and the resulting LDA decision boundaries\n",
    "are shown as solid black lines. Overall, the LDA decision boundaries are\n",
    "pretty close to the Bayes decision boundaries, shown again as dashed lines.\n",
    "The test error rates for the Bayes and LDA classifiers are 0.0746 and 0.0770,\n",
    "respectively. This indicates that LDA is performing well on this data.\n",
    "\n",
    "We can perform LDA on the Default data in order to predict whether\n",
    "or not an individual will default on the basis of credit card balance and\n",
    "student status. The LDA model fit to the 10, 000 training samples results\n",
    "in a training error rate of 2.75 %. This sounds like a low error rate, but two\n",
    "caveats must be noted.\n",
    "\n",
    "\n",
    "- First of all, training error rates will usually be lower than test error rates, which are the real quantity of interest. In other words, we might expect this classifier to perform worse if we use it to predict whether or not a new set of individuals will default. The reason is that we specifically adjust the parameters of our model to do well on the training data. The higher the ratio of parameters p to number of samples n, the more we expect this ___`overfitting`___ to play a role. For these data we don’t expect this to be a problem, since p = 2 and n = 10, 000.\n",
    "- Second, since only 3.33 % of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that each individual will not default, regardless of his or her credit card balance and student status, will result in an error rate of 3.33 %. In other words, the trivial ___`null`___ classifier will achieve an error rate that is only a bit higher than the LDA training set error rate.\n",
    "\n",
    "<a id=\"Table4.4\"></a>\n",
    "![image.png](Figures/Table4.4.png)\n",
    ">__TABLE 4.4.__ A `confusion matrix compares` the `LDA predictions` to the `true default statuses` for the 10, 000 `training observations` in the `Default data set`. \n",
    "<br>`Elements on the diagonal` of the `matrix` represent `individuals` whose `default statuses` were `correctly predicted`, while `off-diagonal` elements `represent` individuals that were `misclassified`. \n",
    "<br>`LDA made incorrect predictions` for 23 individuals who did not `default` and for 252 individuals who `did default`.\n",
    "\n",
    "\n",
    "In practice, a `binary classifier` such as this `one can make two types of errors`: __it can incorrectly assign an individual who defaults to the no default category, or it can incorrectly assign an individual who does not default to the default category.__\n",
    "\n",
    "It is often of `interest` to determine which of these `two types of errors are being made`. \n",
    "\n",
    "A ___`confusion matrix`___, shown for the Default data in <a href=\"#Table4.4\">Table 4.4</a>, is a convenient way to display this `information`. \n",
    "\n",
    "The `table reveals that LDA predicted` that a `total of 104 people would default`. \n",
    "\n",
    "Of these people, __`81 actually defaulted and 23 did not`__.\n",
    "\n",
    "Hence only 23 out of 9, 667 of the individuals who did not default were incorrectly labeled. \n",
    "\n",
    "This looks like a `pretty low error rate`! However, of the 333 individuals who defaulted, 252 (or 75.7 %) were `missed by LDA`. \n",
    "\n",
    "So while the `overall error rate is low`, the `error rate among individuals who defaulted is very high`. \n",
    "\n",
    "From the `perspective of a credit card company` that is `trying to identify high-risk individuals`, an `error rate of` $252/333 = 75.7 \\%$ `among individuals who default may well be unacceptable`.\n",
    "\n",
    "Class-specific performance is also important in medicine and biology, where the terms sensitivity and specificity characterize the performance of a classifier or screening test. In this case the ___`sensitivity`___ is the percentage of true defaulters that are identified, a low 24.3 % in this case. The ___`specificity`___ is the percentage of non-defaulters that are correctly identified, here (1 −\n",
    "23/9, 667) × 100 = 99.8 %.\n",
    "\n",
    "Why does LDA do such a poor job of classifying the customers who default? In other words, why does it have such a low sensitivity? As we have seen, LDA is trying to approximate the Bayes classifier, which has the lowest total error rate out of all classifiers (if the Gaussian model is correct).That is, the Bayes classifier will yield the smallest possible total number of misclassified observations, irrespective of which class the errors come from. That is, some misclassifications will result from incorrectly assigning a customer who does not default to the default class, and others will result from incorrectly assigning a customer who defaults to the non-default\n",
    "class. \n",
    "\n",
    "<a id=\"Table4.5\"></a>\n",
    "![image.png](Figures/Table4.5.png)\n",
    ">__TABLE 4.5.__ A confusion matrix compares the LDA predictions to the true de-\n",
    "fault statuses for the 10, 000 training observations in the Default data set, using\n",
    "a modified threshold value that predicts default for any individuals whose posterior\n",
    "default probability exceeds 20 %.\n",
    "\n",
    "In contrast, a credit card company might particularly wish to avoid incorrectly classifying an individual who will default, whereas incorrectly classifying an individual who will not default, though still to be avoided,\n",
    "is less problematic. We will now see that it is possible to modify LDA in order to develop a classifier that better meets the credit card company’s needs.\n",
    "\n",
    "__Read `ROC Curve, Area Under (ROC)Curve-(AUC),sensitivity,specificity`, from Book Page 145 to 148__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4 Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have discussed, LDA assumes that the observations within each\n",
    "class are drawn from a multivariate Gaussian distribution with a class-\n",
    "specific mean vector and a covariance matrix that is common to all K\n",
    "classes. Quadratic discriminant analysis (QDA) provides an alternative\n",
    "approach. Like LDA, the QDA classifier results from assuming that the\n",
    "observations from each class are drawn from a Gaussian distribution, and\n",
    "plugging estimates for the parameters into Bayes’ theorem in order to per-\n",
    "form prediction. However, unlike LDA, QDA assumes that each class has\n",
    "its own covariance matrix. That is, it assumes that an observation from the\n",
    "kth class is of the form X ∼ N (μ k , Σ k ), where Σ k is a covariance matrix\n",
    "for the kth class. Under this assumption, the Bayes classifier assigns an\n",
    "observation X = x to the class for which\n",
    "\n",
    "\n",
    "\n",
    "is largest. So the QDA classifier involves plugging estimates for Σ k , μ k ,\n",
    "and π k into (4.23), and then assigning an observation X = x to the class\n",
    "for which this quantity is largest. Unlike in (4.19), the quantity x appears\n",
    "as a quadratic function in (4.23). This is where QDA gets its name.\n",
    "Why does it matter whether or not we assume that the K classes share a\n",
    "common covariance matrix? In other words, why would one prefer LDA to\n",
    "QDA, or vice-versa? The answer lies in the bias-variance trade-off. When\n",
    "there are p predictors, then estimating a covariance matrix requires esti-\n",
    "mating p(p+1)/2 parameters. QDA estimates a separate covariance matrix\n",
    "for each class, for a total of Kp(p+1)/2 parameters. With 50 predictors this\n",
    "\n",
    "<a id=\"Figure4.9\"></a>\n",
    "![image.png](Figures/Figure4.9.png)\n",
    ">__FIGURE 4.9.__\n",
    "\n",
    ">__Left:__ The Bayes (purple dashed), LDA (black dotted), and QDA\n",
    "(green solid) decision boundaries for a two-class problem with Σ 1 = Σ 2 . The\n",
    "shading indicates the QDA decision rule. Since the Bayes decision boundary is\n",
    "linear, it is more accurately approximated by LDA than by QDA. \n",
    "\n",
    ">__Right:___ Details\n",
    "are as given in the left-hand panel, except that Σ 1 \u0003 = Σ 2 . Since the Bayes decision\n",
    "boundary is non-linear, it is more accurately approximated by QDA than by LDA.\n",
    "\n",
    "is some multiple of 1,275, which is a lot of parameters. By instead assum-\n",
    "ing that the K classes share a common covariance matrix, the LDA model\n",
    "becomes linear in x, which means there are Kp linear coefficients to esti-\n",
    "mate. Consequently, LDA is a much less flexible classifier than QDA, and\n",
    "so has substantially lower variance. This can potentially lead to improved\n",
    "prediction performance. But there is a trade-off: if LDA’s assumption that\n",
    "the K classes share a common covariance matrix is badly off, then LDA\n",
    "can suffer from high bias. Roughly speaking, LDA tends to be a better bet\n",
    "than QDA if there are relatively few training observations and so reducing\n",
    "variance is crucial. In contrast, QDA is recommended if the training set is\n",
    "very large, so that the variance of the classifier is not a major concern, or if\n",
    "the assumption of a common covariance matrix for the K classes is clearly\n",
    "untenable.\n",
    "\n",
    "Figure 4.9 illustrates the performances of LDA and QDA in two scenarios.\n",
    "In the left-hand panel, the two Gaussian classes have a common correla-\n",
    "tion of 0.7 between X 1 and X 2 . As a result, the Bayes decision boundary\n",
    "is linear and is accurately approximated by the LDA decision boundary.\n",
    "The QDA decision boundary is inferior, because it suffers from higher vari-\n",
    "ance without a corresponding decrease in bias. In contrast, the right-hand\n",
    "panel displays a situation in which the orange class has a correlation of 0.7\n",
    "between the variables and the blue class has a correlation of −0.7. Now\n",
    "the Bayes decision boundary is quadratic, and so QDA more accurately\n",
    "approximates this boundary than does LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 A Comparison of Classification Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we have considered three different classification approaches:\n",
    "logistic regression, LDA, and QDA. In Chapter 2, we also discussed the\n",
    "K-nearest neighbors (KNN) method. We now consider the types of\n",
    "scenarios in which one approach might dominate the others.\n",
    "Though their motivations differ, the logistic regression and LDA methods\n",
    "are closely connected. Consider the two-class setting with p = 1 predictor,\n",
    "and let p 1 (x) and p 2 (x) = 1−p 1 (x) be the probabilities that the observation\n",
    "X = x belongs to class 1 and class 2, respectively. In the LDA framework,\n",
    "we can see from (4.12) to (4.13) (and a bit of simple algebra) that the log\n",
    "odds is given by\n",
    "\n",
    "<a id=\"Formula4.24\"></a>\n",
    "<font size=5><center> $ log \\Big ( \\frac{p_{1}(x)}{1 - p_{1}(x)} \\Big ) = log \\Big ( \\frac{p_{1}(x)}{p_{2}(x)} \\Big ) = c_{0}+c_{1}x $ </center></font>\n",
    ">where $c_{0}$ and $c_{1}$ are functions of $\\mu_{1} , \\mu_{2} $, and $\\sigma^2$ . From <a href=\"#Formula4.4\">(4.4)</a>, we know that in logistic regression,\n",
    "\n",
    "<a id=\"Formula4.25\"></a>\n",
    "<font size=5><center> $ log \\Big ( \\frac{p_{1}}{1 - p_{1}} \\Big ) = \\beta_{0}+\\beta_{1}x $ </center></font>\n",
    "\n",
    "Both (4.24) and (4.25) are linear functions of x. Hence, both logistic re-\n",
    "gression and LDA produce linear decision boundaries. The only difference\n",
    "between the two approaches lies in the fact that β 0 and β 1 are estimated\n",
    "using maximum likelihood, whereas c 0 and c 1 are computed using the esti-\n",
    "mated mean and variance from a normal distribution. This same connection\n",
    "between LDA and logistic regression also holds for multidimensional data\n",
    "with p > 1.\n",
    "\n",
    "Since logistic regression and LDA differ only in their fitting procedures,\n",
    "one might expect the two approaches to give similar results. This is often,\n",
    "but not always, the case. LDA assumes that the observations are drawn\n",
    "from a Gaussian distribution with a common covariance matrix in each\n",
    "class, and so can provide some improvements over logistic regression when\n",
    "this assumption approximately holds. Conversely, logistic regression can\n",
    "outperform LDA if these Gaussian assumptions are not met.\n",
    "\n",
    "Recall from Chapter 2 that KNN takes a completely different approach\n",
    "from the classifiers seen in this chapter. In order to make a prediction for\n",
    "an observation X = x, the K training observations that are closest to x are\n",
    "identified. Then X is assigned to the class to which the plurality of these\n",
    "observations belong. Hence KNN is a completely non-parametric approach:\n",
    "no assumptions are made about the shape of the decision boundary. There-\n",
    "fore, we can expect this approach to dominate LDA and logistic regression\n",
    "when the decision boundary is highly non-linear. On the other hand, KNN\n",
    "does not tell us which predictors are important; we don’t get a table of\n",
    "coefficients as in Table 4.3.\n",
    "\n",
    "\n",
    "<a id=\"Figure4.10\"></a>\n",
    "![image.png](Figures/Figure4.10.png)\n",
    ">__FIGURE 4.10.__ Boxplots of the test error rates for each of the linear scenarios\n",
    "described in the main text.\n",
    "\n",
    "<a id=\"Figure4.11\"></a>\n",
    "![image.png](Figures/Figure4.11.png)\n",
    ">__FIGURE 4.11.__ Boxplots of the test error rates for each of the non-linear sce-\n",
    "narios described in the main text.\n",
    "\n",
    "\n",
    "Finally, QDA serves as a compromise between the non-parametric KNN\n",
    "method and the linear LDA and logistic regression approaches. Since QDA\n",
    "assumes a quadratic decision boundary, it can accurately model a wider\n",
    "range of problems than can the linear methods. Though not as flexible\n",
    "as KNN, QDA can perform better in the presence of a limited number of\n",
    "training observations because it does make some assumptions about the\n",
    "form of the decision boundary.\n",
    "\n",
    "To illustrate the performances of these four classification approaches,\n",
    "we generated data from six different scenarios. In three of the scenarios,\n",
    "the Bayes decision boundary is linear, and in the remaining scenarios it\n",
    "is non-linear. For each scenario, we produced 100 random training data\n",
    "sets. On each of these training sets, we fit each method to the data and\n",
    "computed the resulting test error rate on a large test set. Results for the\n",
    "linear scenarios are shown in Figure 4.10, and the results for the non-linear\n",
    "scenarios are in Figure 4.11. The KNN method requires selection of K, the\n",
    "number of neighbors. We performed KNN with two values of K: K = 1, and a value of K that was chosen automatically using an approach called\n",
    "cross-validation, which we discuss further in Chapter 5.\n",
    "\n",
    "\n",
    "In each of the six scenarios, there were p = 2 predictors. The scenarios\n",
    "were as follows:\n",
    "\n",
    "Scenario 1: There were 20 training observations in each of two classes.\n",
    "The observations within each class were uncorrelated random normal\n",
    "variables with a different mean in each class. The left-hand panel\n",
    "of Figure 4.10 shows that LDA performed well in this setting, as\n",
    "one would expect since this is the model assumed by LDA. KNN\n",
    "performed poorly because it paid a price in terms of variance that\n",
    "was not offset by a reduction in bias. QDA also performed worse\n",
    "than LDA, since it fit a more flexible classifier than necessary. Since\n",
    "logistic regression assumes a linear decision boundary, its results were\n",
    "only slightly inferior to those of LDA.\n",
    "Scenario 2: Details are as in Scenario 1, except that within each\n",
    "class, the two predictors had a correlation of −0.5. The center panel\n",
    "of Figure 4.10 indicates little change in the relative performances of\n",
    "the methods as compared to the previous scenario.\n",
    "Scenario 3: We generated X 1 and X 2 from the t-distribution, with\n",
    "50 observations per class. The t-distribution has a similar shape to\n",
    "the normal distribution, but it has a tendency to yield more extreme\n",
    "points—that is, more points that are far from the mean. In this set-\n",
    "ting, the decision boundary was still linear, and so fit into the logistic\n",
    "regression framework. The set-up violated the assumptions of LDA,\n",
    "since the observations were not drawn from a normal distribution.\n",
    "The right-hand panel of Figure 4.10 shows that logistic regression\n",
    "outperformed LDA, though both methods were superior to the other\n",
    "approaches. In particular, the QDA results deteriorated considerably\n",
    "as a consequence of non-normality.\n",
    "Scenario 4: The data were generated from a normal distribution,\n",
    "with a correlation of 0.5 between the predictors in the first class,\n",
    "and correlation of −0.5 between the predictors in the second class.\n",
    "This setup corresponded to the QDA assumption, and resulted in\n",
    "quadratic decision boundaries. The left-hand panel of Figure 4.11\n",
    "shows that QDA outperformed all of the other approaches.\n",
    "Scenario 5: Within each class, the observations were generated from\n",
    "a normal distribution with uncorrelated predictors. However, the re-\n",
    "sponses were sampled from the logistic function using X 1 2 , X 2 2 , and\n",
    "X 1 × X 2 as predictors. Consequently, there is a quadratic decision\n",
    "boundary. The center panel of Figure 4.11 indicates that QDA once\n",
    "again performed best, followed closely by KNN-CV. The linear meth-\n",
    "ods had poor performance.\n",
    "\n",
    "Scenario 6: Details are as in the previous scenario, but the responses\n",
    "were sampled from a more complicated non-linear function. As a re-\n",
    "sult, even the quadratic decision boundaries of QDA could not ade-\n",
    "quately model the data. The right-hand panel of Figure 4.11 shows\n",
    "that QDA gave slightly better results than the linear methods, while\n",
    "the much more flexible KNN-CV method gave the best results. But\n",
    "KNN with K = 1 gave the worst results out of all methods. This\n",
    "highlights the fact that even when the data exhibits a complex non-\n",
    "linear relationship, a non-parametric method such as KNN can still\n",
    "give poor results if the level of smoothness is not chosen correctly.\n",
    "These six examples illustrate that no one method will dominate the oth-\n",
    "ers in every situation. When the true decision boundaries are linear, then\n",
    "the LDA and logistic regression approaches will tend to perform well. When\n",
    "the boundaries are moderately non-linear, QDA may give better results.\n",
    "Finally, for much more complicated decision boundaries, a non-parametric\n",
    "approach such as KNN can be superior. But the level of smoothness for a\n",
    "non-parametric approach must be chosen carefully. In the next chapter we\n",
    "examine a number of approaches for choosing the correct level of smooth-\n",
    "ness and, in general, for selecting the best overall method.\n",
    "\n",
    "Finally, recall from Chapter 3 that in the regression setting we can accom-\n",
    "modate a non-linear relationship between the predictors and the response\n",
    "by performing regression using transformations of the predictors. A similar\n",
    "approach could be taken in the classification setting. For instance, we could\n",
    "create a more flexible version of logistic regression by including X 2 , X 3 ,\n",
    "and even X 4 as predictors. This may or may not improve logistic regres-\n",
    "sion’s performance, depending on whether the increase in variance due to\n",
    "the added flexibility is offset by a sufficiently large reduction in bias. We\n",
    "could do the same for LDA. If we added all possible quadratic terms and\n",
    "cross-products to LDA, the form of the model would be the same as the\n",
    "QDA model, although the parameter estimates would be different. This\n",
    "device allows us to move somewhere between an LDA and a QDA model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
