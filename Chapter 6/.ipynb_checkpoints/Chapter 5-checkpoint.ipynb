{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Resampling methods are an `absolutely necessary tool` in `modern statistics`. \n",
    "- They `involve repeatedly drawing samples` from a `training set and refitting a model` of `interest` on `each sample` in `order` to `obtain additional information` about the `fitted model`. \n",
    ">__For example__, in order to `estimate the variability of a linear regression fit`, we can `repeatedly draw different samples` from the `training data`, `fit a linear regression` to `each new sample`, and then `examine` the extent to which the `resulting fits differ`. \n",
    "- Such an approach may allow us to `obtain information` that would not be `available from fitting the model only once using the original training sample`\n",
    "\n",
    "***\n",
    "\n",
    "- __Resampling approaches can be `computationally expensive`__, because they `involve fitting the same statistical method multiple times using different subsets of the training data`. \n",
    "\n",
    "- However, due to recent `advances` in __`computing power, the computational requirements` of resampling methods generally are not prohibitive__. \n",
    "\n",
    "- In this chapter, we discuss `two of the most commonly used resampling methods`, \n",
    "\n",
    "\n",
    "    1. cross-validation and \n",
    "    2. the bootstrap. \n",
    "\n",
    "\n",
    "- Both methods are important tools in the practical application of many statistical learning procedures. \n",
    ">__For example__, `cross-validation` can be used to `estimate` the __`test error`__ associated with a given `statistical learning method` in order to `evaluate its performance, or to select the appropriate level of flexibility`.\n",
    "\n",
    "- The `process of evaluating a model’s performance` is known as ___`model assessment`___, whereas the `process of selecting the proper level of flexibility for a model` is known as ___`model selection`___. \n",
    "\n",
    "\n",
    "- The ___`bootstrap`___ is used in `several contexts`, `most commonly` to `provide a measure of accuracy` of a `parameter estimate` or of a `given statistical learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Chapter%202.ipynb\">Chapter 2</a> \n",
    "\n",
    "we discuss the `distinction between` __the test error rate and the training error rate__. \n",
    "\n",
    "- The ___`test error`___ is the average error that results from using a statistical learning method to predict the response on a new observation— that is, a measurement that was not used in training the method. \n",
    "\n",
    "Given a `data set`, the use of a `particular statistical learning method` is warranted if it results in a `low test error`. \n",
    "\n",
    "- The `test error` can be `easily calculated` if a `designated test set is available`. \n",
    "\n",
    "    Unfortunately, this is usually not the case.\n",
    "\n",
    "In contrast, the ___`training error`___ can be `easily calculated` by applying the `statistical learning method` to the `observations used in its training`. \n",
    "\n",
    "- But as we saw in <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Chapter%202.ipynb\">Chapter 2</a> , the __training error rate__ often is `quite different` from the __test error rate__, and in `particular` the former can `dramatically underestimate the latter`.\n",
    "\n",
    "In the absence of a `very large designated test set` that can be used to `directly estimate the test error rate`, a `number of techniques can be used` to `estimate this quantity using the available training data`. \n",
    "\n",
    "Some methods make a `mathematical adjustment` to the `training error rate` in order to `estimate the test error rate`. \n",
    "\n",
    "    In this section, we instead consider a class of methods that estimate thetest error rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations.\n",
    "\n",
    "In <a href=\"#5.1.1-The-Validation-Set-Approach\">Sections 5.1.1–<a href=\"#5.1.4-Bias-Variance-Trade-Off-for-k-Fold-Cross-Validation\">5.1.4</a></a>, for simplicity we assume that we are `interested in performing regression with a quantitative response`. \n",
    "\n",
    "In <a href=\"#5.1.5-Cross-Validation-on-Classification-Problems\">Section 5.1.5</a> we consider the case of `classification` with a `qualitative response`. As we will see, the `key concepts remain the same regardless of whether the response` is ___`quantitative or qualitative`___."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.1 The Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we would like to estimate the `test error` associated with `fitting` a `particular statistical learning method` on a `set of observations`. \n",
    "\n",
    "- The ___`validation set approach`___, displayed in <a href=\"#Figure5.1\">Figure 5.1</a>, is a `very simple strategy` for `this task`. \n",
    "\n",
    "- It involves `randomly dividing` the `available set of observations into two parts`, a __training set__ and a __validation set or hold-out set__. \n",
    "\n",
    "The `model is fit on` the `training set`, and the `fitted model is used to predict` the `responses` for the `observations in the validation set`. \n",
    "\n",
    "The __`resulting validation set error rate`__—typically assessed using `MSE` in the `case of a quantitative\n",
    "response`—provides an estimate of the `test error rate`.\n",
    "\n",
    "We illustrate the `validation set approach` on the `Auto data set`. \n",
    "\n",
    "Recall from Chapter 3 that there appears to be a `non-linear relationship between` __`mpg`__ and __`horsepower`__ , and that a `model that predicts` __`mpg`__ using $horsepower$ and $horsepower^2$ gives `better results` than a `model that uses only a linear term`.\n",
    "\n",
    "<a id=\"Figure5.1\"></a>\n",
    "![image.png](Figures/Figure5.1.png)\n",
    ">__FIGURE 5.1.__ A `schematic display` of the `validation set approach`. \n",
    "- A set of $n$ observations are `randomly split into`\n",
    "- a __`training set`__ (shown in blue, containing observations 7, 22, and 13, among others) and \n",
    "- a __`validation set`__ (shown in beige, and containing observation 91, among others). \n",
    "- The statistical learning method is `fit on the training set`, and its __performance__ is `evaluated on the validation set`.\n",
    "\n",
    "It is natural to wonder whether a `cubic` or `higher-order fit` might `provide` even `better results`.\n",
    "\n",
    "- We `answer this question` in <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%203/Chapter%203.ipynb\">Chapter 3</a> by looking at the __`p-values`__ associated with a `cubic term` and `higher-order polynomial terms` in a `linear regression`. \n",
    "\n",
    "But we could `also answer` this `question` using the `validation method`. \n",
    "- We `randomly split` the 392 `observations` into __`two sets`__, a `training set` containing 196 of the `data points`, and a `validation set` containing the `remaining 196 observations`. \n",
    "\n",
    "\n",
    "- The `validation set error rates` that `result` from `fitting various regression models` on the `training sample`\n",
    "and `evaluating their performance` on the `validation sample`, using `MSE as a measure of validation set error`, are shown in the `left-hand panel` of<a href=\"#Figure5.2\">Figure 5.2</a>. \n",
    "\n",
    "---\n",
    "![image.png](Figures/FigureE1.png)\n",
    "\n",
    "The `validation set MSE` for the __`quadratic fit`__ is considerably `smaller than for the linear fit`. \n",
    "\n",
    "However, the `validation set MSE` for the __`cubic fit`__ is actually `slightly larger` than for the `quadratic fit`. \n",
    "\n",
    "--- \n",
    "\n",
    "This implies that including a `cubic term` in the `regression does not lead to better prediction`\n",
    "than simply using a `quadratic term`.\n",
    "\n",
    "\n",
    "Recall that `in order to create` the `left-hand panel` of <a href=\"#Figure5.2\">Figure 5.2</a>, we `randomly divided` the `data set into two parts`, a `training set` and a `validation\n",
    "set`. \n",
    "\n",
    "If we `repeat the process` of `randomly splitting the sample set into two parts`, we will get a `somewhat different estimate` for the `test MSE`. \n",
    "\n",
    "As an illustration, the `right-hand panel` of <a href=\"#Figure5.2\">Figure 5.2</a> displays `ten different validation set MSE curves` from the `Auto data set`, produced using `ten different random splits` of the `observations into training and validation sets`. \n",
    "\n",
    "All `ten curves indicate that the model` with a `quadratic term` has a `dramatically\n",
    "smaller validation set MSE` than the `model with only a linear term`. \n",
    "\n",
    "Furthermore, all `ten curves indicate` that `there is not much benefit` in `including\n",
    "cubic` or `higher-order polynomial terms` in the `model`. \n",
    "\n",
    "But it is worth `noting` that `each of the ten curves results` in a `different test MSE estimate` for `each\n",
    "of the ten regression models considered`. \n",
    "\n",
    "And there is no consensus among the curves as to which model results in the `smallest validation set MSE`. \n",
    "\n",
    "Based on the `variability among these curves`, all that we can conclude with `any confidence` is that the `linear fit is not adequate` for `this data`.\n",
    "\n",
    "The `validation set approach` is `conceptually simple` and is `easy to implement`. \n",
    "\n",
    "But it has two `potential drawbacks`:\n",
    "\n",
    "<a id=\"Figure5.2\"></a>\n",
    "![image.png](Figures/Figure5.2.png)\n",
    ">__FIGURE 5.2.__ The validation set approach was used on the Auto data set in\n",
    "order to estimate the test error that results from predicting mpg using polynomial\n",
    "functions of horsepower. \n",
    "- __Left:__ Validation error estimates for a single split into training and validation data sets. \n",
    "- __Right:__ The validation method was repeated te times, each time using a different random split of the observations into a training set and a validation set. \n",
    "    This illustrates the variability in the estimated test MSE that results from this approach.\n",
    "\n",
    "1. As is shown in the `right-hand panel` of <a href=\"#Figure5.2\">Figure 5.2</a>, the `validation estimate` of the `test error rate` can be `highly variable`, depending on precisely which `observations are included` in the `training set` and which `observations are included in the validation set`.\n",
    "\n",
    "\n",
    "2. In the `validation approach`, only a `subset of the observations`—those that are `included in the training set rather than in the validation set`—are used to `fit the model`. Since `statistical methods tend to perform worse` when `trained on fewer observations`, this suggests that the `validation set error rate` may tend to `overestimate the test error rate` for the `model fit on the entire data set`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___`Leave-one-out cross-validation (LOOCV)`___ is closely related to the `validation sset approach` of <a href=\"#5.1.1-The-Validation-Set-Approach\">Section 5.1.1</a>, but it `attempts to address` that `method’s drawbacks`.\n",
    "\n",
    "Like the `validation set approach`, __LOOCV__ involves `splitting the set of observations into two parts`. \n",
    "\n",
    "However, instead of `creating two subsets of\n",
    "comparable size`, a `single observation` $(x_{1} , y_{1} )$ is used for the `validation set`, and the `remaining observations` $(x_{2} , y_{2} ), \\dots , (x_{n} , y_{n} )$ make up the `training set`. \n",
    "\n",
    "The `statistical learning method is fit on` the $n − 1$ `training observations`, and a `prediction`$\\hat{y}_{1}$ is made for the `excluded observation`, using its value $x_{1}$ . \n",
    "\n",
    "Since $(x_{1} , y_{1} )$ was `not used` in the `fitting process`, $MSE_{1} =(y_{1} − \\hat{y}_{1} )^2$ provides an `approximately unbiased estimate for the test error`.\n",
    "\n",
    "But even though $MSE_{1}$ is `unbiased for the test error`, it is a `poor estimate` because it is `highly variable`, since it is based upon a `single observation` $(x_{1} , y_{1} )$.\n",
    "\n",
    "<a id=\"Figure5.3\"></a>\n",
    "![image.png](Figures/Figure5.3.png)\n",
    ">__FIGURE 5.3.__ A schematic display of LOOCV. \n",
    "- A set of n data points is repeatedly split into a training set (shown in blue) containing all but one observation, and a validation set that contains only that observation (shown in beige). \n",
    "- The test error is then estimated by averaging the n resulting MSE’s. The first training set contains all but observation 1, the second training set contains all but observation 2, and so forth.\n",
    "\n",
    "We can `repeat the procedure` by `selecting` $(x_{2} , y_{2} )$ for the `validation\n",
    "data`, `training the statistical learning procedure` on the $n − 1$ observations\n",
    "$\\big \\{ (x_{1} , y_{1} ), (x_{3} , y_{3} ), \\dots , (x_{n} , y_{n} ) \\big\\}$, and computing $MSE_{2} = (y_{2} −\\hat{y}_{2})^2$ . \n",
    "\n",
    "- __Repeating this approach $n$ times produces $n$ squared errors__, $MSE_{1} , \\dots , MSE_{n}$ .\n",
    "\n",
    "The `LOOCV estimate` for the `test MSE` is the `average` of these $n$ `test error\n",
    "estimates`:\n",
    "\n",
    "<a id=\"Formula5.1\"></a>\n",
    "<font size=5><center> $ CV_{n} = \\frac{1}{n} \\sum_{i=1}^{n} MSE_{i}$ </center></font>\n",
    "\n",
    "- A `schematic of the LOOCV approach` is illustrated in <a href=\"#Figure5.3\">Figure 5.3</a>.\n",
    "\n",
    "- `LOOCV has a couple of major advantages over the validation set approach`. \n",
    "\n",
    "\n",
    "1. First, it has far less bias. \n",
    "  - In LOOCV, we repeatedly fit the statistical learning method using training sets that contain $n − 1$ observations, almost as many as are in the entire data set. \n",
    "  - This is in contrast to the validation set approach, in which the training set is typically around half the size of the original data set. \n",
    "  - Consequently, the LOOCV approach tends not to overestimate the test error rate as much as the validation set approach does. \n",
    "  \n",
    "2.  Second, in contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times willalways yield the same results: there is no randomness in the training/validation set splits.\n",
    "\n",
    "<a id=\"Figure5.4\"></a>\n",
    "![image.png](Figures/Figure5.4.png)\n",
    ">__FIGURE 5.4__. `Cross-validation was used` on the `Auto data set` in order to `estimate the test error` that results from predicting mpg using `polynomial functions\n",
    "of horsepower`. \n",
    "- __Left:__ The LOOCV error curve. \n",
    "- __Right:__ 10-fold CV was run nine separate times, each with a different random split of the data into ten parts. \n",
    "    The figure shows the nine slightly different CV error curves.\n",
    "\n",
    "We used LOOCV on the `Auto data set` in order to obtain an `estimate of the test set MSE` that `results` from `fitting a linear regression model` to `predict mpg using polynomial functions of horsepower`. \n",
    "\n",
    "The results are shown in the __left-hand panel__ of <a href=\"#Figure5.4\">Figure 5.4</a>.\n",
    "\n",
    "`LOOCV` has the `potential to be expensive to implement`, since the `model\n",
    "has to be fit` $n$ `times`. \n",
    "\n",
    "This can be `very time consuming` if $n$ is `large`, and if\n",
    "`each individual model is slow to fit`. \n",
    "\n",
    "With `least squares linear or polynomial regression`, an `amazing shortcut makes the cost of LOOCV` the `same as that of a single model fit!` The following formula holds:\n",
    "\n",
    "<a id=\"Formula5.2\"></a>\n",
    "<font size=5><center> $ CV_{n} = \\frac{1}{n} \\sum_{i=1}^{n} \\Big( \\frac{y_{i} - \\hat{y}_{i}}{1 - h_{i}} \\Big)^2$ </center></font>\n",
    "\n",
    ">where \n",
    "- $\\hat{y}_{i}$ is the $ith$ `fitted value` from the `original least squares fit`, and $h_{i}$ is the `leverage defined` in (3.37). \n",
    "This is like the `ordinary MSE`, except the $ith$ `residual is divided` by $1 − h_{i}$ . \n",
    "\n",
    "The leverage __`lies between` $1/n$ and $1$__, and reflects the amount that an observation influences its own fit.\n",
    "\n",
    "Hence the `residuals for high-leverage points` are `inflated in this formula` by `exactly the right amount` for `this equality to hold`.\n",
    "\n",
    "`LOOCV is a very general method`, and can be used with any kind of `predictive modeling`. \n",
    "\n",
    "For example we could use it with `logistic regression` or `linear discriminant analysis`, or any of the `methods discussed` in later chapters. \n",
    "\n",
    "<a id=\"Figure5.5\"></a>\n",
    "![image.png](Figures/Figure5.5.png)\n",
    ">__FIGURE 5.5.__ A schematic display of `5-fold CV`. \n",
    "- A set of $n$ observations is randomly split into five non-overlapping groups. \n",
    "- Each of these fifths acts as a validation set (shown in beige), and the remainder as a training set (shown in\n",
    "blue). \n",
    "- The __`test error is estimated by averaging the five resulting MSE estimates`__.\n",
    "\n",
    "The magic formula <a href=\"#Formula5.2\">(5.2)</a> does `not hold in general`, in which case the `model has to be refit` $n$ `times`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.3 k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to `LOOCV` is ___`k-fold CV`___. \n",
    "\n",
    "- This approach involves `randomly dividing` the `set of observations into` $k$ __`groups`__, or __`folds`__, of `approximately equal size`. \n",
    "\n",
    "$ \\gt $ The `first fold` is `treated` as a `validation set`, and the `method is fit on the remaining` $k − 1$ `folds`. \n",
    "\n",
    "The `mean squared error`, $MSE_{1}$ , is then `computed on the observations` in the `held-out fold`. \n",
    "\n",
    "This __procedure is `repeated` $k$ times__; each time, a `different group of observations` is `treated as a validation set`. \n",
    "\n",
    "This `process results` in $k$ `estimates` of the `test error`, $MSE_{1} , MSE_{2} , \\dots , MSE_{k}$ . \n",
    "\n",
    "The __k-fold CV estimate__ is `computed by averaging these values`, <a href=\"#Figure5.5\">Figure 5.5</a> illustrates the `k-fold CV` approach.\n",
    "\n",
    "<a id=\"Formula5.3\"></a>\n",
    "<font size=5><center> $ CV_{k} = \\frac{1}{k} \\sum_{i=1}^{k} MSE_{i} $ </center></font>\n",
    "\n",
    "\n",
    "`LOOCV` is special case of `k-fold CV` in which $k = n$\n",
    "\n",
    "The difference or Advantage of k-fold is:\n",
    "- It is computationally good, compared to LOOCV because it performs these repetations for n times. while k fold does it only k times.\n",
    "\n",
    "__[Note: you should not use k = n otherwise there will be no difference between both the k fold and LOOCV.]__\n",
    "\n",
    "\n",
    "<a id=\"Figure5.6\"></a>\n",
    "![image.png](Figures/Figure5.6.png)\n",
    "\n",
    ">__FIGURE 5.6__. \n",
    "- `True` and `estimated test MSE` for the simulated `data sets` in <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Chapter%202.ipynb#Figure2.9\">Figures 2.9 ( left)</a>, <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Chapter%202.ipynb#Figure2.10\">2.10 ( center)</a>, and <a href=\"http://localhost:8888/notebooks/islr-book/Chapter%202/Chapter%202.ipynb#Figure2.11\">2.11 ( right)</a>. \n",
    "- The `true test MSE` is shown in `blue`, the `LOOCV estimate` is shown as a `black dashed line`, and the `10-fold CV` estimate is shown in `orange`. The `crosses indicate` the `minimum of each of the MSE curves`.\n",
    "\n",
    "__[Read more about this from book Page No 196, 197]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.4 Bias-Variance Trade-Off for k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that Advantages of K-fold includes:\n",
    "1. $ K < n $ has computational advantage.\n",
    "2. It gives more accurate test error rates compared to k-fold.\n",
    "    But it has to be done with Bias-Variance Trade Off.\n",
    "\n",
    "One more things to note is `Validation set approach` can lead to __Over-estimation__ of test-error rate.\n",
    "\n",
    "Now focusing on good things, but keeping the computational advantages aside we can say that `LOOCV` can not give a biased error rates.\n",
    "\n",
    "__SO, `LOOCV` will not give the Biased Test Error Rate, therefore `its preferable` to k-fold CV`.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.5 Cross-Validation on Classification Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross Validation in `Regression setting` where the outcome $Y$ is `Quantitative`, so MSE to quantify the Test Error. But\n",
    "- Cross Validation can also be very useful approach in the `Classification settings` when $Y$ is `Qualitative`.\n",
    "\n",
    "using no of misclassification observation for instance in the classification settings, the LOOCV error rates takes the form\n",
    "\n",
    "<a id=\"Formula5.4\"></a>\n",
    "<font size=5><center> $ CV_{(n)} =  \\frac{1}{n} \\sum_{i=1}^{n}Err_{i} $ </center></font>\n",
    ">Where $Err_{i} = I(y_{i} \\neq \\hat{y_{i}})$\n",
    "\n",
    "The CV error rate and validation set error rates are defined analogously.\n",
    "\n",
    "As an example, we `fit various logistic regression models` on the two dimensional classification data displayed in <a href=\"#Figure2.13\">Figure 2.13</a>. \n",
    "\n",
    "In the `top-left panel` of <a href=\"#Figure5.7\">Figure 5.7</a>, the __black solid line__ shows the `estimated decision boundary` resulting from fitting a `standard logistic regression model` to this `data set`. \n",
    "\n",
    "Since this is simulated data, we can compute the `true test error rate`, which takes a value of 0.201 and so is substantially `larger` than the `Bayes error rate` of 0.133. \n",
    "\n",
    "Clearly `logistic regression` does not have enough `flexibility` to model the `Bayes decision boundary` in this setting. \n",
    "\n",
    "We can easily `extend logistic regression` to obtain a `non-linear decision boundary` by using\n",
    "`polynomial functions of the predictors`, as we did in the `regression setting` in <a href=\"#\">Section 3.3.2</a>. For example, we can fit a `quadratic logistic regression model`, given by\n",
    "\n",
    "<a id=\"Formula5.5\"></a>\n",
    "<font size=5><center> $ log \\Big( \\frac{p}{1-p} \\Big) = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{1}^{2} + \\beta_{3} X_{2} + \\beta_{4} X_{2}^{2} $ </center></font>\n",
    "\n",
    "<a id=\"Figure5.7\"></a>\n",
    "![image.png](Figures/Figure5.7.png)\n",
    ">__FIGURE 5.7__. `Logistic regression` fits on the two-dimensional classification data displayed in <a href=\"#Figure2.13\">Figure 2.13</a>. \n",
    "<br>The `Bayes decision boundary` is represented using a `purple dashed line`. \n",
    "<br>Estimated decision boundaries from `linear, quadratic, cubic and quartic (degrees 1–4) logistic regressions` are displayed in `black`. \n",
    "<br>The `test error rates` for the `four logistic regression fits` are respectively 0.201, 0.197, 0.160, and 0.162, while the `Bayes error rate` is 0.133. \n",
    "\n",
    "- Top right panel of <a href=\"#Figure5.7\">Figure 5.7</a> displayes the decision boundary, which is now curved.\n",
    ">However the test error rate has improved slightly to 0.197.\n",
    "- Much higher improvement is apparent in __bottom-left panel__ of <a href=\"#Figure5.7\">Figure 5.7</a>,  in which we have fit logistic regression model involving `cubic polynomials` of the predictors. \n",
    ">Now the test error rate has decreased to 0.160. Going to the `quadratic polynomial` (bottom-right) slightly increases the test error rate.\n",
    "\n",
    "<a id=\"Figure5.8\"></a>\n",
    "![image.png](Figures/Figure5.8.png)\n",
    ">__FIGURE 5.8__. `Test error` (brown), `training error` (blue), and `10-fold CV error` (black) on the two dimensional `classification` data displayed in <a href=\"#Figure5.7\">Figure 5.7</a>. \n",
    "\n",
    ">__Left:__ Logistic regression using polynomial functions of the predictors. The order of the polynomials used is displayed on the x-axis. \n",
    "\n",
    ">__Right:__ The KNN classifier with\n",
    "different values of K, the number of neighbors used in the KNN classifier.\n",
    "\n",
    "__For more detailed example. Read Book Page No: 186__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ___`bootstrap`___ is a __widely applicable and extremely powerful statistical tool__ that can be `used` to `quantify` the `uncertainty associated` with a `given estimator` or `statistical learning method`.\n",
    "\n",
    ">The `bootstrap method` is a `resampling technique` used to estimate `statistics` on a population by `sampling a dataset with replacement`.\n",
    "<br>It can be used to `estimate summary statistics` such as the `mean or standard deviation`. It is used in `applied machine learning` to estimate the `skill of machine learning models` when `making predictions` on `data not included in the training data`.\n",
    "\n",
    "<a id=\"Figure5.11\"></a>\n",
    "![image.png](Figures/Figure5.11.png)\n",
    "\n",
    "__Explained with detailed example in book Page No (188-190)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Lab: Cross-Validation and the Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.1 The Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.3 k-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.4 The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
