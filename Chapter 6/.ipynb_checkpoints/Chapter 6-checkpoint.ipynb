{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Linear Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regression setting, the standard Linear Model\n",
    "\n",
    "<a id=\"Formula6.1\"></a>\n",
    "<font size=5><center> $ Y = \\beta_{0} + \\beta_{1}X_{1} + \\dots + \\beta_{p}X_{p} + \\epsilon $ </center></font>\n",
    "\n",
    "is commonly used to describe the relationship between a response $ Y $ and a set of Variables $ X_{1},X_{2},\\dots,X_{p} $. typically fits this model using least squares.\n",
    "\n",
    "- In Chapter 7 we generalize <a href=\"#Formula6.1\">(6.1)</a> in order to `accommodate non-linear`, but `still additive`, `relationships`, while \n",
    "\n",
    "- in Chapter 8 we consider even `more general non-linear models`. However, the linear model `has distinct advantages` in terms of `inference` and, on `real-world problems`, is `often surprisingly competitive` in relation to non-linear methods.\n",
    "\n",
    "Hence, before moving to the `non-linear world`, we discuss in this chapter some ways in which the `simple linear model` can be `improved`, by `replacing plain least squares fitting` with some `alternative fitting procedures`.\n",
    "\n",
    "__Why might we want to use another fitting procedure instead of `least squares`?__ \n",
    "\n",
    "    As we will see, alternative fitting procedures can yield better _prediction accuracy_ and _model interpretability_.\n",
    "    \n",
    "- ___Prediction Accuracy:___ Provided that the `true relationship` between the `response and the predictors` is approximately `linear`, the `least squares` estimates will have `low bias`.  <br>If $n \\gt\\gt p$—that is, if $n$, the `number of observations`, is `much larger` than $p$, the `number of variables`—then the `least squares estimates` tend to also have `low variance`, and hence will perform well on `test observations`. <br>However, if $n$ is not much larger than $p$, then there can be a lot of `variability` in the `least squares fit`, resulting in `overfitting` and `consequently poor predictions` on `future observations not used in model training`. And if $p \\gt n$, then there is __`no longer a unique least squares coefficient estimate`__: the `variance` is `infinite` so the method `cannot be used at all`. <br>By constraining or shrinking the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias. This can lead to substantial improvements in the accuracy with which we can predict the response for observations not used in model training.\n",
    "\n",
    "\n",
    "- ___Model Interpretability:___ It is often the case that some or many of the variables used in a multiple regression model are in fact not associated with the response. Including such irrelevant variables leads to unnecessary complexity in the resulting model. By removing these variables—that is, by setting the corresponding coefficient estimates to zero—we can obtain a model that is more easily interpreted. Now least squares is extremely unlikely to yield any coefficient estimates that are exactly zero. In this chapter, we see some approaches for automatically performing ___`feature selection`___ or ___`variable selection`___—that is, for excluding irrelevant variables from a multiple regression model.\n",
    "\n",
    "There are many `alternatives`, both `classical` and `modern`, to using `least squares` to fit <a href=\"#Formula6.1\">(6.1)</a>. \n",
    "\n",
    "In this chapter, we discuss `three important classes of methods`.\n",
    "\n",
    "1. ___Subset Selection___. This approach involves identifying a subset of the p predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.\n",
    "\n",
    "\n",
    "2. ___Shrinkage___. This approach involves fitting a model involving all p predictors. However, the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as regularization) has the effect of reducing variance. Depending on what type of shrinkage is performed, some of the coefficients may be estimated to be exactly zero. Hence, shrinkage methods can also perform variable selection.\n",
    "\n",
    "\n",
    "3. ___Dimension Reduction.___ This approach involves projecting the p predictors into a M -dimensional subspace, where M < p. This is achieved by computing M different linear combinations, or projections, of the variables. Then these M projections are used as predictors to fit a linear regression model by least squares.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Subset Selection\n",
    "### 6.1.1 Best Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform ___`Best Subset Selection`___, we fit a separate lease square regression for each possible combination of the $p$ predictors. That is, we fit all $p$ models that contain exactly one predictor, all $ \\binom{p}{2}  = p(p − 1)/2$ models that contain `exactly two predictors`, and so forth. \n",
    "\n",
    "We then look at all of the `resulting models`, with the `goal of identifying` the `one that is best`.\n",
    "\n",
    "\n",
    "The `problem of selecting the best model` from `among` the $2^p$ `possibilities` considered by `best subset selection` is `not trivial`. \n",
    "\n",
    "This is usually broken up `into two stages`, as described in <a href=\"#Algorithm-6.1-Best-Subset-Selection\">___`Algorithm 6.1`___</a>.\n",
    "\n",
    "***\n",
    "#### Algorithm 6.1 Best Subset Selection\n",
    "***\n",
    "<font size=3 face=\"Times New Roman\"><b>\n",
    "\n",
    "1.  Let $ M_{0} $ denote the null model , which contains no predictors. This model simply predicts the sample mean for each observation. \n",
    "\n",
    "\n",
    "2. For $ k = 1, 2, \\dots, p$:\n",
    "    \n",
    "    (a) Fit all k p models that contain exactly k predictors.\n",
    "    \n",
    "    (b) Pick the best among these $k_{p}$ models, and call it $ M_{k} $. Here best is defined as having the smallest RSS, or equivalently largest $R^2$ .\n",
    "\n",
    "\n",
    "3. Select a single best model from among $ M_{0} , \\dots , M_{p}$ using cross-validated prediction error, $C_{p}$ (AIC), BIC, or adjusted $R^2$ .\n",
    "</b></font>\n",
    "***\n",
    "\n",
    "In <a href=\"#Algorithm-6.1-Best-Subset-Selection\">__`Algorithm 6.1`__</a>, Step 2 identifies the best model (on the training data) for each subset size, in order to reduce the problem from one of $2^p$ possible models to one of $p + 1$ possible models. <br>In <a href=\"#Figure6.1\">Figure 6.1</a>, these models form the lower frontier depicted in red.\n",
    "\n",
    "Now in order to select a `single best model`, we must `simply choose among` these $p + 1$ `options`. \n",
    "\n",
    "This task must be performed with care, because the $RSS$ of these $p + 1$ models decreases monotonically, and the $R^2$ `increases monotonically`, as the `number of features` included in the `models increases`. \n",
    "\n",
    "Therefore, if we use these statistics to `select the best model`, then we will `always end up with a model involving all of the variables`. \n",
    "\n",
    "The `problem` is that a `low RSS` or a `high` $R^2$ indicates a model with a `low training error`, whereas we wish to choose a model that has a `low test error`. (As shown in Chapter 2 in Figures 2.9–2.11, `training error` tends to be quite a `bit smaller than test error`, and a `low training error by no means guarantees a low test error`.) Therefore, in `Step 3`, we use `cross-validated prediction error`, $C_{p}$, $BIC$, or `adjusted` $R^2$ in order to select among $M_{0} , M_{1} ,\\dots , M_{p}$ .\n",
    "These approaches are discussed in <a href=\"#6.1.3\">Section 6.1.3</a>.\n",
    "\n",
    "\n",
    "<a id=\"Figure6.1\"></a>\n",
    "![image.png](Figures/Figure6.1.png)\n",
    ">__FIGURE 6.1.__ For each possible model containing a subset of the ten predictors in the Credit data set, the RSS and $R^2$ are displayed. \n",
    "<br>The red frontier tracks the best model for a given number of predictors, according to RSS and $R^2$ . Though the data set contains only ten predictors, the x-axis ranges from 1 to 11, since one of the variables is categorical and takes on three values, leading to the creation of two dummy variables.\n",
    "\n",
    "- we have `presented best subset selection` here for `least squares regression`, the `same ideas apply` to other `types of models`, such as `logistic regression`. \n",
    "    \n",
    "    In the case of `logistic regression`, instead of `ordering models by` $RSS$ in `Step 2` of <a href=\"#Algorithm-6.1-Best-Subset-Selection\">Algorithm 6.1</a>, we instead use the `deviance`, a measure that plays the role of $RSS$ for a `broader class of models`. \n",
    "    \n",
    "    __The `deviance` is `negative two times the maximized log-likelihood`; the `smaller the deviance`, the `better the fit`__\n",
    "    \n",
    "While `best subset selection` is a `simple and conceptually appealing approach`, it __suffers from computational limitations__. \n",
    "\n",
    "The `number of possible models` that must be considered `grows rapidly` as $p$ `increases`. \n",
    "\n",
    "In general, there are $2^{p}$ models that involve subsets of $p$ predictors. \n",
    "\n",
    "So if $p = 10$, then there are approximately 1,000 possible models to be considered, and \n",
    "\n",
    "if $p = 20$, then there are `over one million possibilities!` Consequently, `best subset selection` becomes `computationally infeasible` for `values of` $p$ `greater` than around 40, even with `extremely fast modern computers`.\n",
    "\n",
    "There are `computational shortcuts`—so called __branch-and-bound techniques—for eliminating some choices__, but these have `their limitations` as $p$ gets `large`. They also only work for least squares linear regression. We present computationally efficient alternatives to best subset selection next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `computational reasons`, ___`best subset selection`___ `cannot be applied` with `very large` $p$. \n",
    "\n",
    "`Best subset selection` may also `suffer from statistical problems` when $p$ is `large`. \n",
    "\n",
    "The `larger the search space`, the `higher the chance of finding models` that `look good on the training data`, even `though they might not have\n",
    "Forward stepwise selection is a computationally efficient alternative to best\n",
    "subset selection. While the best subset selection procedure considers all\n",
    "2 p possible models containing subsets of the p predictors, forward step-\n",
    "wise considers a much smaller set of models. Forward stepwise selection\n",
    "begins with a model containing no predictors, and then adds predictors\n",
    "to the model, one-at-a-time, until all of the predictors are in the model.\n",
    "In particular, at each step the variable that gives the greatest additional\n",
    "improvement to the fit is added to the model. More formally, the forward\n",
    "stepwise selection procedure is given in Algorithm 6.2.any predictive power` on `future data`. \n",
    "\n",
    "Thus an `enormous search space` can lead to `overfitting and high variance` of the `coefficient estimates`.\n",
    "For both of `these reasons`, `stepwise methods`, which `explore a far more restricted set of models`, are `attractive alternatives to best subset selection`.\n",
    "\n",
    "#### Forward Stepwise Selection\n",
    "\n",
    "__Forward stepwise selection__ is a `computationally efficient alternative` to `best subset selection`. \n",
    "\n",
    "While the `best subset selection` procedure considers all $2^p$ `possible models` containing `subsets` of the $p$ `predictors`, `forward stepwise considers` a much `smaller set of models`. \n",
    "\n",
    "__Forward stepwise selection begins__ with a model `containing no predictors`, and then `adds predictors` to the `model`, __`one-at-a-time`__, until all of the predictors are in the model.\n",
    "In particular at each step the variable that gives the greatest additional improvement the fit is added to the model. More formallythe forward stepwise selection procedure is given in Algorithm 6.2.\n",
    "\n",
    "***\n",
    "#### Algorithm 6.2 Forward Stepwise Selection\n",
    "***\n",
    "<font size=3 face=\"Times New Roman\"><b>\n",
    "\n",
    "1.  Let $ M_{0} $ denote the null model , which contains no predictors.\n",
    "\n",
    "\n",
    "2. For $ k = 0, 1, 2, \\dots, p-1$:\n",
    "    \n",
    "    (a) Consider all $p − k$ models that augment the predictors in $M_{k}$ with one additional predictor.\n",
    "    \n",
    "    (b) Pick the best among these $p-k$ models, and call it $ M_{k+1} $. Here best is defined as having the smallest $RSS$, or largest $R^2$ .\n",
    "\n",
    "\n",
    "3. Select a single best model from among $ M_{0} , \\dots , M_{p}$ using cross-validated prediction error, $C_{p}$ (AIC), BIC, or adjusted $R^2$ .\n",
    "</b></font>\n",
    "***\n",
    "\n",
    "Unlike `best subset selection`, which involved `fitting` $2^p$ `models`, `forward stepwise selection` involves `fitting one null model`, `along with` $p − k$ `models` in the $kth$ iteration, for $k = 0, \\dots , p − 1$. This `amounts` to a total of $1 + \\sum_{k=0}^{p-1}(p − k) = 1 + p(p + 1)/2$ models. This is a `substantial difference`: when $p = 20$, best `subset selection requires` fitting 1,048,576 `models`, whereas `forward stepwise selection` requires `fitting only 211 models`.\n",
    "\n",
    "\n",
    "In `Step 2(b)` of __Algorithm 6.2__, we `must identify` the `best model` from `among those` $p− k$ that augment $M_{k}$ with one `additional predictor`. We can do this by `simply choosing` the `model with the lowest RSS` or the `highest` $R^2$ . \n",
    "\n",
    "However, in `Step 3`, we must `identify the best model among` a `set of models` with `different numbers of variables`. This is `more challenging`, and is `discussed` in Section 6.1.3.\n",
    "\n",
    "\n",
    "`Forward stepwise selection’s` __computational advantage over best subset selection is clear__. Though `forward stepwise` tends to `do well in practice`, it is `not guaranteed` to `find` the `best possible model` out of `all` $2^$p models containing `subsets` of the $p$ predictors. For instance, suppose that in a given `data set` with $p = 3$ predictors, the best possible one-variable model contains $X_1$ , and the `best possible two-variable` model instead contains $X_2$ and $X_3$ . Then `forward stepwise selection` will `fail` to select the `best possible` two-variable model, because $M_1$ will contain $X_1$ , so $M_2$ must also contain $X_1$ together with `one additional variable`.\n",
    "\n",
    "\n",
    "<a href=\"#Table6.1\">Table 6.1</a>, which `shows` the `first four selected models` for `best subset` and `forward stepwise selection` on the `Credit data set`, illustrates this `phenomenon`. Both `best subset selection` and `forward stepwise selection` choose rating for the best `one-variable model` and then `include income` and `student for the two- and three-variable models`. However, `best subset selection replaces rating by cards` in the `four-variable model`, while `forward stepwise selection` must `maintain rating` in its `four-variable model`. In this example, <a href=\"#Figure6.1\">Figure 6.1</a> indicates that there is `not much difference between` the `three- and four-variable models` in terms of $RSS$, so `either` of the `four-variable models` will likely be adequate.\n",
    "\n",
    "\n",
    "`Forward stepwise selection` can be `applied` even in the `high-dimensional setting` where $n \\lt p$; however, in this case, it is `possible to construct sub- models` $M_0 , \\dots , M_{n−1}$ only, since each `submodel` is `fit using least squares`, which will not `yield a unique solution` if $p \\ge n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `forward stepwise selection`, `backward stepwise selection` provides an `efficient alternative` to `best subset selection`. However, `unlike forward stepwise selection`, it begins with the `full least squares model` containing all $p$ `predictors`, and then `iteratively removes` the `least useful predictor`,\n",
    "`one-at-a-time`. Details are given in <a href=\"#Algorithm6.3\">Algorithm 6.3</a>.\n",
    "\n",
    "<a id=\"Table6.1\"></a>\n",
    "\n",
    "| # Variables | Best subset                   | Forward Stepwise            |\n",
    "| ----------- | ----------------------------- | --------------------------- |\n",
    "| One         | rating                        | rating                      |\n",
    "| Two         | rating, income                | rating,income               |\n",
    "| Three       | rating, income, student       | rating,income,student       |\n",
    "| Four        | cards,income, student, limit  | rating,student,income,limit |\n",
    "\n",
    ">__TABLE 6.1.__ The `first four selected models` for `best subset selection` and `forward\n",
    "stepwise selection` on the `Credit data set`. The `first three models are identical` but\n",
    "the `fourth models differ`.\n",
    "\n",
    "***\n",
    "#### Algorithm 6.3 Backward Stepwise Selection\n",
    "***\n",
    "<font size=3 face=\"Times New Roman\"><b>\n",
    "\n",
    "1.  Let $ M_{p} $ denote the full model , which contains all $p$ predictors.\n",
    "\n",
    "\n",
    "2. For $ k = p, p-1, \\dots, 1$:\n",
    "    \n",
    "    (a) Consider all $k$ models that all but one of the predictors in $M_{k}$ for a total of $k-1$ predictors.\n",
    "    \n",
    "    (b) Pick the best among these $k$ models, and call it $ M_{k-1} $. Here best is defined as having the smallest $RSS$, or largest $R^2$ .\n",
    "\n",
    "\n",
    "3. Select a single best model from among $ M_{0} , \\dots , M_{p}$ using cross-validated prediction error, $C_{p}$ (AIC), BIC, or adjusted $R^2$ .\n",
    "</b></font>\n",
    "***\n",
    "\n",
    "Like `forward stepwise selection`, the `backward selection approach` searches through only $1 + p(p + 1)/2$ models, and so can be `applied in settings` where $p$ is `too large to apply best subset selection`. Also like `forward stepwise selection`, `backward stepwise selection` is `not guaranteed` to `yield the best model containing a subset` of the $p$ predictors.\n",
    "\n",
    "\n",
    "`Backward selection requires` that the `number of samples` $n$ is `larger than` the `number of variables` $p$ (so that the full model can be fit). In contrast, `forward stepwise` can be `used even` when $n \\lt p$, and so is the `only viable subset method` when $p$ is very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __`best subset`, `forward stepwise`, and `backward stepwise selection`__ approaches `generally give similar` but `not identical models`. As another alternative, `hybrid versions` of `forward and backward stepwise selection` are `available`, in which `variables are added to the model sequentially`, in `analogy` to `forward selection`. However, after `adding each new variable`, the `method may also remove` any `variables` that `no longer provide` an `improvement in the model fit`. Such an approach attempts to more `closely mimic best subset selection while retaining` the `computational advantages` of `forward and backward stepwise selection`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Choosing the Optimal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best subset selection, forward selection, and backward selection result in the __`creation of a set of models, each of which contains a subset of the p predictors`__. \n",
    "\n",
    "In order to implement these methods, `we need a way to determine` \n",
    "\n",
    "___`which of these models is best`___. \n",
    "\n",
    "As we discussed in <a href=\"#6.1.1-Best-Subset-Selection\">Section 6.1.1</a>, the model containing all of the predictors will always have the __`smallest RSS and the largest` $R^2$__ , since these quantities are related to the `training error`. Instead, we wish to `choose a model` with a `low test error`. As is `evident` here, and as we show in `Chapter 2`, the `training error` can be a `poor estimate` of the `test error`. Therefore, RSS and $R^2$ are `not suitable` for `selecting the best model` among a `collection of models` with `different numbers of predictors`.\n",
    "\n",
    "In order to select the best model with respect to test error, we need to estimate this test error. \n",
    "\n",
    "__There are two common approaches:__\n",
    "\n",
    "1. We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.\n",
    "\n",
    "\n",
    "2. We can directly estimate the test error, using either a validation set approach or a cross-validation approach, as discussed in Chapter 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $C_{p}$ , AIC, BIC, and Adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `training error` will `decrease as more variables` are `included in the model`, but the `test error may not`. Therefore, `training set` $RSS$ and `training set` $R^2$ `cannot be used` to `select from among a set of models` with `different numbers of variables`.\n",
    "\n",
    "However, `a number of techniques` for __`adjusting the training error for the model size are available`__. \n",
    "\n",
    "These approaches can be used to `select among a set of models` with `different numbers of variables`. We now consider four such approaches:\n",
    "\n",
    "$C_p$ , __Akaike information criterion ($AIC$)__, __Bayesian information\n",
    "criterion ($BIC$)__, and __adjusted $R^2$__ . \n",
    "\n",
    "Figure 6.2 displays $C_p$ , $BIC$, and adjusted $R^2$ for the `best model` of `each size` produced by best `subset selection` on the `Credit data set`.\n",
    "\n",
    "<a id=\"Figure6.2\"></a>\n",
    "![image.png](Figures/Figure6.2.png)\n",
    ">__FIGURE 6.2.__ $C_{p}$ , $BIC$, and adjusted $R^2$ are `shown for the best models of each size` for the `Credit data set` (the lower frontier in <a href=\"#Figure6.1\">Figure 6.1</a>). $C_{p}$ and $BIC$ are estimates of test $MSE$. In the middle plot we see that the $BIC$ estimate of test error shows an increase after four variables are selected. The other two plots are rather flat after four variables are included.\n",
    "\n",
    "For a `fitted least squares model` containing $d$ `predictors`, the $C_p$ estimate of `test MSE` is `computed using the equation`\n",
    "\n",
    "<a id=\"Formula6.2\"></a>\n",
    "<font size=5><center> $ C_{p} = \\frac{1}{n}(RSS + 2d \\hat{\\sigma}^2) $ </center></font>\n",
    ">where $\\hat{\\sigma}^2$ is an estimate of variance of the error $\\epsilon$  associated with each response measurement in <a href=\"#Formula6.1\">(6.1)</a>\n",
    "\n",
    "<font size=5><center> $ AIC = \\frac{1}{n\\hat{\\sigma}^2}(RSS +  2d\\hat{\\sigma}^2)$ </center></font>\n",
    "\n",
    "<a id=\"Formula6.3\"></a>\n",
    "<font size=5><center> $ BIC = \\frac{1}{n\\hat{\\sigma}^2}(RSS + log(n)d \\hat{\\sigma}^2) $ </center></font>\n",
    "\n",
    "<a id=\"Formula6.4\"></a>\n",
    "<font size=5><center> $ Adjusted R^2 =  1 -  \\frac{RSS/(n-d-1}{TSS/(n-1)}$ </center></font>\n",
    "\n",
    "These three things are not that important, but you might need them to understand $R^2$.\n",
    "\n",
    "__[Read more from Book: Page No 211 & 212]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation and Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly estimate the `test error rate` using validation set and cross validation methods(As discussed in Chapter 5).\n",
    "\n",
    "This procedure has an advantage relative to $AIC,BIC,C_p $ and $Adjusted R^2$, in that it provides direct estimation of test error, and makes fewer assumptions about the true underlaying model. \n",
    "\n",
    "\n",
    "<a id=\"Figure6.3\"></a>\n",
    "![image.png](Figures/Figure6.3.png)\n",
    ">__Figure 6.3.__ For the `Credit data set`, three quantities are displayed for the `best model` containing $d$ predictors, for $d$ ranging from 1 to 11.\n",
    "<br>The overall `best model`, based on each of these quantities, is shown as a `blue cross`. \n",
    "\n",
    ">__Left:__ Square root of BIC. \n",
    "\n",
    ">__Center:__ Validation set errors. \n",
    "\n",
    ">__Right:__ Cross-validation errors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Shrinkage Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `subset selection methods` described in <a href=\"#6.1-Subset-Selection\">Section 6.1</a> involve using `least squares` to `fit a linear model` that contains a `subset of the predictors`. \n",
    "\n",
    "As an __alternative__, we can `fit a model containing` all $p$ `predictors` using a `technique` that __constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero__.\n",
    "\n",
    "It may not be immediately obvious why such a constraint should improve the fit, but it turns out that `shrinking the coefficient estimates` can `significantly reduce their variance`. \n",
    "\n",
    "The `two best-known techniques` for `shrinking` the `regression coefficients towards zero` are __ridge regression and the lasso.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Ridge Rrgression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from Chapter 3 that the `least squares fitting procedure estimates` $\\beta_{0} , \\beta_{1} , \\dots , \\beta_{p}$ using the `values that minimize`\n",
    "\n",
    "<font size=5><center> $ RSS = \\sum_{i=1}^{n} \\Bigg( y_{i} - \\beta_{0} - \\sum_{j=1}^{p} \\beta_{j}X_{ij}\\Bigg)^2 $ </center></font>\n",
    "\n",
    "___`Ridge regression`___ is `very similar to least squares`, except that the `coefficients are estimated` by `minimizing` a slightly different `quantity`. \n",
    "\n",
    "In particular, the `ridge regression coefficient` estimates $\\hat{\\beta}^R$ are the `values that minimize`\n",
    "\n",
    "<a id=\"Formula6.5\"></a>\n",
    "<font size=5><center> $ \\sum_{i=1}^{n} \\Bigg( y_{i} - \\beta_{0} - \\sum_{j=1}^{p} \\beta_{j}X_{ij}\\Bigg)^2 + \\lambda \\sum_{j=1}^{p} \\beta^2_{j} = RSS + \\lambda \\sum_{j=1}^{p} \\beta^2_{j} $ </center></font>\n",
    "\n",
    ">where $\\lambda \\ge 0 $ is a `Turing Parameter` to be determined separately.\n",
    "\n",
    "<a href=\"#Formula6.5\">Equation 6.5</a> trades off two different criteria. As with `least squares`, `ridge regression seeks coefficient estimates` that `fit the data well`, by `making` the $RSS$ `small`. \n",
    "\n",
    "However, the `second term`, $ \\lambda \\sum_{j} \\beta_{j}^2 $, called a ___`shrinkage penalty`___, is `small` when $\\beta_{1} , \\dots , \\beta_{p}$ are `close to zero`, and so it has the effect of `shrinking` the estimates of $\\beta_{j}$ towards `zero`.\n",
    "\n",
    "<a id=\"Figure6.4\"></a>\n",
    "![image.png](Figures/Figure6.4.png)\n",
    ">__FIGURE 6.4__. The `standardized ridge regression coefficients` are displayed for the `Credit data set`, as a `function` of $\\lambda$ and $|| \\hat{\\beta}_{\\lambda}^R ||2/ ||\\hat{\\beta} ||2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Application to the Credit Data\n",
    "\n",
    "__Read on Book Page No 216 & 217__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Does Ridge Regression Improve Over Least Squares?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantage is the `Bias Variance Trade Off` -> $ \\lambda $ \n",
    "\n",
    "__As $ \\lambda $ increases Variance Decreases but again Bias Increases.__\n",
    "\n",
    "__AS $ \\lambda $ decreases Variance Increases but the Bias Decreases.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 The Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression have one obvious disadvnatages.\n",
    "\n",
    "Unlike Best, Forward & Backward Stepwise Selection, which will generally select models that involve just a subset of variables, Ridge Regression will include all $p$ predictors in the Final Model.\n",
    "\n",
    "The Penalty $\\lambda \\sum \\beta^2_{j}$ in <a href=\"#Formula6.5\">(6.5)</a> will shrink all coefficient towards zero but not set any of them exactly to zero (unless $\\lambda = \\infty$).\n",
    "\n",
    "This may not be a problem for `prediction accuracy`, but it can create a challenge in model `interpretation` in `settings` in which the `number of variables` $p$ is `quite large`. \n",
    "\n",
    ">__For example__, in the `Credit data set`, it appears that the `most important variables` are __income , limit , rating , and student.__ So we `might wish` to `build a model including` just these predictors. \n",
    "<br>However, `ridge regression` __will always generate a model involving all ten predictors__. \n",
    "<br>Increasing the value of$\\lambda$ will tend to `reduce the magnitudes of the coefficients`, but will not result in `exclusion` of any of the `variables`.\n",
    "\n",
    "The ___`lasso`___ is a `relatively recent alternative` to `ridge regression` that `overcomes this disadvantage`. \n",
    "\n",
    "The `lasso coefficients`, $\\hat{\\beta}^L_{\\lambda}$ , `minimize the quantity`\n",
    "\n",
    "<a id=\"Formula6.7\"></a>\n",
    "<font size=5><center> $ \\sum_{i=1}^{n} \\Bigg( y_{i} - \\beta_{0} - \\sum_{j=1}^{p} \\beta_{j}X_{ij}\\Bigg)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_{j}| = RSS + \\lambda \\sum_{j=1}^{p} |\\beta_{j}| $ </center></font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Variable Selection Property Of the Lasso"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Why is it that the lasso, unlike ridge regression, results in coefficient estimates that are exactly equal to zero?__\n",
    "\n",
    "    The `formulations` <a href=\"#Formula6.8\">(6.8)</a> and <a href=\"#Formula6.9\">(6.9)</a> can be used to shed light on the issue. <a href=\"#Figure6.7\">Figure 6.7</a> illustrates the situation. The least squares solution is marked as $\\hat{\\beta}$, while the `blue diamond` and circle represent the lasso and ridge regression constraints in (6.8) and (6.9), respectively. If s is sufficiently large, then the constraint regions will contain β̂, and so the ridge regression and lasso estimates will be the same as the least squares estimates. (Such a large value of s corresponds to λ = 0 in (6.5) and (6.7).) However, in Figure 6.7 the least squares estimates lie outside of the diamond and the circle, and so the least squares estimates are not the same as the lasso and ridge regression estimates. \n",
    "    \n",
    "<a id=\"Figure6.7\"></a>    \n",
    "![image.png](Figures/Figure6.7.png)\n",
    ">__FIGURE 6.7__. Contours of the `error` and `constraint functions` for the __`lasso (left)`__ and __`ridge regression (right)`__. \n",
    "<br>The `solid blue areas` are the `constraint regions`, $|\\beta_{1} | + |\\beta_{2}| \\le s$ and $ \\beta^2_{1} + \\beta^2_{2} \\le s$, while the red ellipses are the contours of the RSS.\n",
    "\n",
    "- `intersection` will `not` generally occur on `an axis`, and so the `ridge regression coefficient` estimates will be `exclusively non-zero`. However, the `lasso constraint` has `corners` at each of `the axes`, and so the `ellipse` will often `intersect` the `constraint region` at `an axis`. When this `occurs`, one of the `coefficients` will `equal zero`. In `higher dimensions`, `many of the coefficient estimates may equal zero simultaneously`. In <a href=\"#Figure6.7\">Figure 6.7</a>, the `intersection` occurs at $\\beta_{1} = 0$, and so the `resulting model will only include` $\\beta_{2}$ .\n",
    "\n",
    "<a id=\"Figure6.8\"></a>\n",
    "![image.png](Figures/Figure6.8.png)\n",
    ">__FIGURE 6.8__. __`Left:`__ Plots of `squared bias` (black), `variance` (green), and `test MSE`\n",
    "(purple) for the `lasso` on a simulated data set. \n",
    "__`Right:`__ Comparison of `squared bias`, `variance` and `test MSE` between `lasso` (solid) and `ridge` (dotted). Both are plotted `against their` $R2$ on the `training data`, as a `common form of indexing`.\n",
    "\n",
    "In <a href=\"#Figure6.7\">Figure 6.7</a>, we considered the `simple case` of $p = 2$. When $p = 3$,\n",
    "then the `constraint region` for `ridge regression` becomes a `sphere`, and the\n",
    "`constraint region` for the `lasso` becomes a `polyhedron`. When $p \\gt 3$, the constraint for `ridge regression` becomes a `hypersphere`, and the constraint for the `lasso` becomes a `polytope`. However, the` key ideas` depicted in <a href=\"#Figure6.7\">Figure 6.7</a> still hold. In particular, the `lasso` leads to feature selection when $p \\gt 2$ due to the `sharp corners` of the `polyhedron` or `polytope`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the Lasso and Ridge Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Figure6.9\"></a>\n",
    "![image.png](Figures/Figure6.9.png)\n",
    ">__FIGURE 6.9.__ __`Left:`__ Plots of `squared bias` (black), `variance` (green), and `test MSE`\n",
    "(purple) for the `lasso`. The simulated data is similar to that in <a href=\"#Figure6.8\">Figure 6.8</a>, except\n",
    "that `now` only `two predictors` are related to the `response`. __`Right:`__ Comparison of\n",
    "`squared bias`, `variance` and `test MSE` between `lasso` (solid) and `ridge` (dotted). Both\n",
    "are plotted against their $R^2$ on the `training data`, as a `common form of indexing`.\n",
    "The crosses in both plots indicate the `lasso model` for which the `MSE` is smallest.\n",
    "\n",
    "\n",
    "__When Lasso?__\n",
    "- Relatively small no of predictors, have substantial coefficients and \n",
    "- other remaining predictors have coefficients that are very small or equal to zero.\n",
    "\n",
    "__When Ridge?__\n",
    "- if Response is funciton of many/all predictors, \n",
    "- all with coefficient of roughly equal size.\n",
    "\n",
    "\n",
    "__CROSS VALIDATION:__\n",
    "- __can be used to determine the approach on perticular data-set.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Simple Special Case for Ridge Regression and the Lasso"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain a better intuition about the behaviour of Ridge Regression and the lasso, Consider a simple __Special Case__ with\n",
    "\n",
    "$ n = p $\n",
    "\n",
    "$X$ a `diagonal matrix`\n",
    "\n",
    "$\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "To simplify the problem further.\n",
    "- Assume that we are performing regression without an `intercept`.\n",
    "\n",
    "With these assumption:\n",
    "\n",
    "- the `usual least squares problem simplifies` to `finding` $\\beta_{1}, \\dots , \\beta_{p}$ that minimize\n",
    "\n",
    "<a id=\"Formula6.11\"></a>\n",
    "<font size=5><center> $\\sum_{j=1}^{p} ( y_{j} - \\beta_{j})^2 $</center></font>\n",
    "\n",
    "In this case, the least squares solution is given by <font size=5><center> $\\hat{\\beta}_{j} = y_{j}  $</center></font>.\n",
    "\n",
    "And in this setting, ___`ridge regression`___ amounts to finding $\\beta_{1}, \\dots , \\beta_{p}$ such that\n",
    "\n",
    "<a id=\"Formula6.12\"></a>\n",
    "<font size=5><center> $\\sum_{j=1}^{p} ( y_{j} - \\beta_{j})^2 + \\lambda \\sum_{j=1}^{p} \\beta^2_{j} $</center></font>\n",
    "\n",
    "is minimized, and the `lasso amounts` to finding the `coefficients` such that\n",
    "\n",
    "<a id=\"Formula6.13\"></a>\n",
    "<font size=5><center> $\\sum_{j=1}^{p} ( y_{j} - \\beta_{j})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_{j}| $</center></font>\n",
    "\n",
    "is minimized. One can show that in this `setting`, the `ridge regression estimates` take the form\n",
    "\n",
    "<a id=\"Formula6.14\"></a>\n",
    "<font size=5><center> $ \\hat{\\beta}^R_{j} = y_{j}(1 + \\lambda) $</center></font>\n",
    "\n",
    "and the `lasso estimates take the form`\n",
    "\n",
    "<a id=\"Formula6.15\"></a>\n",
    "<font size=5><center> $ \\hat{\\beta}^L_{j} = \\Bigg \\{ \\begin{matrix} y_{j} - \\lambda/2 & if y_{j} > \\lambda/2; \\\\ y_{j} + \\lambda/2 & if y_{j} < -\\lambda/2; \\\\ 0 & if y_{j} \\le \\lambda/2. \\end{matrix} $</center></font>\n",
    "\n",
    "\n",
    "<a href=\"#Figure6.10\">Figure 6.10</a> displays the situation. We can see that `ridge regression` and the `lasso perform` __two very different types of shrinkage__. In `ridge regression`, each least squares coefficient estimate is `shrunken by the same proportion`. In contrast, the `lasso shrinks` each `least squares coefficient towards zero by a constant amount`, $\\lambda/2$; the `least squares coefficients that are less than` $\\lambda/2$ in `absolute value are shrunken entirely to zero`. The type of `shrinkage performed` by the `lasso` in this `simple setting` <a href=\"#Formula6.15\">(6.15)</a> is known as ___`soft-thresholding`___. \n",
    "\n",
    "__The fact that some `lasso coefficients are shrunken entirely to zero` explains why the `lasso performs feature selection`.__\n",
    "\n",
    "In the case of a `more general data matrix` $X$, the `story is a little more complicated` than __what is depicted in <a href=\"#Figure6.10\">Figure 6.10</a>,__ but the `main ideas stillhold approximately`: __ridge regression more or less shrinks every dimension__\n",
    "of the data by the `same proportion`, whereas the __`lasso`__ `more or less shrinks all coefficients toward zero by a similar amount`, and `sufficiently small coefficients are shrunken all the way to zero`.\n",
    "\n",
    "<a href=\"#Figure6.10\"></a>\n",
    "![image.png](Figures/Figure6.10.png)\n",
    ">__FIGURE 6.10.__ The `ridge regression` and `lasso coefficient estimates` for a `simple setting` with $n = p$ and $X$ a diagonal matrix with 1’s on the diagonal. \n",
    "<br>__Left:__ The ridge regression coefficient estimates are shrunken proportionally towards zero,\n",
    "relative to the least squares estimates. \n",
    "<br>__Right:__ The lasso coefficient estimates are\n",
    "soft-thresholded towards zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Interpretation for Ridge Regression and the Lasso\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Figure6.11\"></a>\n",
    "![image.png](Figures/Figure6.11.png)\n",
    ">__FIGURE 6.11.__\n",
    "<br>__Left:__ Ridge regression is the posterior mode for $\\beta$ under a Gaussian prior. \n",
    "<br>__Right:__ The lasso is the posterior mode for β under a double-exponential prior.\n",
    "\n",
    "__Read it from book Page No: 226 & 227__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
